{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T11:53:32.875748Z",
     "start_time": "2024-07-19T11:53:31.864439Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Implements a simple n-gram language model in PyTorch.\n",
    "Acts as the correctness reference for all the other versions.\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:53:32.880685Z",
     "start_time": "2024-07-19T11:53:32.876752Z"
    }
   },
   "cell_type": "code",
   "source": "from common import RNG",
   "id": "1a3a93b4262a11b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter ",
   "id": "a9e74038e4d240e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:20:51.839912Z",
     "start_time": "2024-07-19T12:20:51.836763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = 3 # if 3 tokens predict the 4th, this is a 4-gram model\n",
    "embedding_size = 64\n",
    "hidden_size = 512\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_steps = 50000"
   ],
   "id": "5454ffdaaf4fd139",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (optional) Optimize Hyperparameter",
   "id": "a9902e5c38720ae0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pip install optunahub",
   "id": "d254ec85c9377eeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:23:43.118795Z",
     "start_time": "2024-07-19T12:23:43.099501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "\n",
    "def t(train_model, evaluate_model, X_val, y_val):\n",
    "    def objective(trial):\n",
    "        embedding_size = trial.suggest_int('embedding_size', 50, 300)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "        hidden_size = trial.suggest_int('hidden_size', 128, 512)\n",
    "        \n",
    "        model = train_model(embedding_size, learning_rate, hidden_size)\n",
    "        val_loss = evaluate_model(model, X_val, y_val)\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    return objective\n",
    "\n"
   ],
   "id": "51d60a73ebbcae60",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MLP",
   "id": "859bd5ad605936b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:53:32.888884Z",
     "start_time": "2024-07-19T11:53:32.884843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the previous n tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, context_length, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, embedding_size) # token embedding table\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(context_length * embedding_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx are the input tokens, (B, T) tensor of integers\n",
    "        # targets are the target tokens, (B, ) tensor of integers\n",
    "        B, T = idx.size()\n",
    "        # encode all the tokens using the embedding table\n",
    "        emb = self.wte(idx) # (B, T, embedding_size)\n",
    "        # concat all of the embeddings together\n",
    "        emb = emb.view(B, -1) # (B, T * embedding_size)\n",
    "        # forward through the MLP\n",
    "        logits = self.mlp(emb)\n",
    "        # if we are given desired targets, also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss"
   ],
   "id": "cf26ea2286eb8b20",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:53:32.892968Z",
     "start_time": "2024-07-19T11:53:32.889888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# simple DataLoader that iterates over all the n-grams\n",
    "\n",
    "def dataloader(tokens, context_length, batch_size):\n",
    "    # returns inputs, targets as torch Tensors of shape (B, T), (B, )\n",
    "    n = len(tokens)\n",
    "    inputs, targets = [], []\n",
    "    pos = 0\n",
    "    while True:\n",
    "        # simple sliding window over the tokens, of size context_length + 1\n",
    "        window = tokens[pos:pos + context_length + 1]\n",
    "        inputs.append(window[:-1])\n",
    "        targets.append(window[-1])\n",
    "        # once we've collected a batch, emit it\n",
    "        if len(inputs) == batch_size:\n",
    "            yield (torch.tensor(inputs), torch.tensor(targets))\n",
    "            inputs, targets = [], []\n",
    "        # advance the position and wrap around if we reach the end\n",
    "        pos += 1\n",
    "        if pos + context_length >= n:\n",
    "            pos = 0"
   ],
   "id": "ab461da83e9fbc28",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:53:32.897313Z",
     "start_time": "2024-07-19T11:53:32.893972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# evaluation function\n",
    "\n",
    "def eval_split(model, tokens, max_batches=None):\n",
    "    # calculate the loss on the given tokens\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = len(tokens) // batch_size\n",
    "    if max_batches is not None:\n",
    "        num_batches = min(num_batches, max_batches)\n",
    "    data_iter = dataloader(tokens, context_length, batch_size)\n",
    "    for _ in range(num_batches):\n",
    "        inputs, targets = next(data_iter)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    mean_loss = total_loss / num_batches\n",
    "    return mean_loss"
   ],
   "id": "9ed89c2e8a0fcb6a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "8213011a2d8fc534"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:53:35.537557Z",
     "start_time": "2024-07-19T11:53:35.520970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random = RNG(1337)\n",
    "# TODO: actually use this rng for the model initialization\n",
    "\n",
    "# \"train\" the Tokenizer, so we're able to map between characters and tokens\n",
    "train_text = open('data/train.txt', 'r').read()\n",
    "assert all(c == '\\n' or ('a' <= c <= 'z') for c in train_text)"
   ],
   "id": "c06b73caddad99e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:53:42.036890Z",
     "start_time": "2024-07-19T11:53:42.028885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uchars = sorted(list(set(train_text))) # unique characters we see in the input\n",
    "uchars"
   ],
   "id": "14a39114abf1d3da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:54:07.668745Z",
     "start_time": "2024-07-19T11:54:07.664956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = len(uchars)\n",
    "vocab_size"
   ],
   "id": "f16a279f66bc6970",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:55:06.221281Z",
     "start_time": "2024-07-19T11:55:06.218310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# let's train!\n",
    "\n",
    "char_to_token = {c: i for i, c in enumerate(uchars)}\n",
    "token_to_char = {i: c for i, c in enumerate(uchars)}\n",
    "print(char_to_token)\n",
    "print(token_to_char)"
   ],
   "id": "7a84849cb46d68b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:55:23.270555Z",
     "start_time": "2024-07-19T11:55:23.258400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EOT_TOKEN = char_to_token['\\n'] # designate \\n as the delimiting <|endoftext|> token\n",
    "# pre-tokenize all the splits one time up here\n",
    "test_tokens = [char_to_token[c] for c in open('data/test.txt', 'r').read()]\n",
    "val_tokens = [char_to_token[c] for c in open('data/val.txt', 'r').read()]\n",
    "train_tokens = [char_to_token[c] for c in open('data/train.txt', 'r').read()]"
   ],
   "id": "76f43ed74f450713",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:55:32.777911Z",
     "start_time": "2024-07-19T11:55:32.016467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# create the model\n",
    "\n",
    "model = MLP(vocab_size, context_length, embedding_size, hidden_size)\n",
    "\n",
    "# create the optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ],
   "id": "db4d4dfcd433871d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:22:57.550553Z",
     "start_time": "2024-07-19T12:21:03.716589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# training loop\n",
    "print(f'num_steps {num_steps}, num_epochs {num_steps * batch_size / len(train_tokens):.2f}')\n",
    "train_data_iter = dataloader(train_tokens, context_length, batch_size)\n",
    "for step in range(num_steps):\n",
    "    # cosine learning rate schedule, from max lr to 0\n",
    "    lr = learning_rate * 0.5 * (1 + math.cos(math.pi * step / num_steps))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    # every now and then evaluate the validation loss\n",
    "    last_step = step == num_steps - 1\n",
    "    if step % 200 == 0 or last_step:\n",
    "        train_loss = eval_split(model, train_tokens, max_batches=20)\n",
    "        val_loss = eval_split(model, val_tokens)\n",
    "        print(f'step {step} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | lr {lr:e}')\n",
    "    # ensure the model is in training mode\n",
    "    model.train()\n",
    "    # get the next batch of training data\n",
    "    inputs, targets = next(train_data_iter)\n",
    "    # forward through the model\n",
    "    logits, loss = model(inputs, targets)\n",
    "    # backpropagate and update the weights\n",
    "    loss.backward()\n",
    "    # step the optimizer\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ],
   "id": "c7c2bdef29f84636",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps 50000, num_epochs 14.97\n",
      "step 0 | train_loss 3.3127 | val_loss 3.3217 | lr 1.000000e-03\n",
      "step 200 | train_loss 2.4165 | val_loss 2.4280 | lr 9.999605e-04\n",
      "step 400 | train_loss 2.3531 | val_loss 2.3637 | lr 9.998421e-04\n",
      "step 600 | train_loss 2.3161 | val_loss 2.3258 | lr 9.996447e-04\n",
      "step 800 | train_loss 2.3040 | val_loss 2.3013 | lr 9.993685e-04\n",
      "step 1000 | train_loss 2.2985 | val_loss 2.2921 | lr 9.990134e-04\n",
      "step 1200 | train_loss 2.2708 | val_loss 2.2793 | lr 9.985795e-04\n",
      "step 1400 | train_loss 2.2513 | val_loss 2.2580 | lr 9.980668e-04\n",
      "step 1600 | train_loss 2.2518 | val_loss 2.2558 | lr 9.974755e-04\n",
      "step 1800 | train_loss 2.2489 | val_loss 2.2439 | lr 9.968057e-04\n",
      "step 2000 | train_loss 2.2324 | val_loss 2.2297 | lr 9.960574e-04\n",
      "step 2200 | train_loss 2.2277 | val_loss 2.2293 | lr 9.952307e-04\n",
      "step 2400 | train_loss 2.2246 | val_loss 2.2174 | lr 9.943259e-04\n",
      "step 2600 | train_loss 2.2141 | val_loss 2.2127 | lr 9.933430e-04\n",
      "step 2800 | train_loss 2.2119 | val_loss 2.2143 | lr 9.922822e-04\n",
      "step 3000 | train_loss 2.2090 | val_loss 2.2003 | lr 9.911436e-04\n",
      "step 3200 | train_loss 2.2117 | val_loss 2.1908 | lr 9.899275e-04\n",
      "step 3400 | train_loss 2.1028 | val_loss 2.1981 | lr 9.886341e-04\n",
      "step 3600 | train_loss 2.1565 | val_loss 2.1924 | lr 9.872634e-04\n",
      "step 3800 | train_loss 2.1600 | val_loss 2.1810 | lr 9.858159e-04\n",
      "step 4000 | train_loss 2.1595 | val_loss 2.1820 | lr 9.842916e-04\n",
      "step 4200 | train_loss 2.1610 | val_loss 2.1874 | lr 9.826908e-04\n",
      "step 4400 | train_loss 2.1575 | val_loss 2.1711 | lr 9.810138e-04\n",
      "step 4600 | train_loss 2.1481 | val_loss 2.1809 | lr 9.792609e-04\n",
      "step 4800 | train_loss 2.1516 | val_loss 2.1749 | lr 9.774323e-04\n",
      "step 5000 | train_loss 2.1575 | val_loss 2.1759 | lr 9.755283e-04\n",
      "step 5200 | train_loss 2.1704 | val_loss 2.1647 | lr 9.735492e-04\n",
      "step 5400 | train_loss 2.1690 | val_loss 2.1639 | lr 9.714953e-04\n",
      "step 5600 | train_loss 2.1521 | val_loss 2.1642 | lr 9.693669e-04\n",
      "step 5800 | train_loss 2.1519 | val_loss 2.1651 | lr 9.671645e-04\n",
      "step 6000 | train_loss 2.1649 | val_loss 2.1636 | lr 9.648882e-04\n",
      "step 6200 | train_loss 2.1493 | val_loss 2.1594 | lr 9.625386e-04\n",
      "step 6400 | train_loss 2.1699 | val_loss 2.1587 | lr 9.601159e-04\n",
      "step 6600 | train_loss 2.1475 | val_loss 2.1492 | lr 9.576206e-04\n",
      "step 6800 | train_loss 2.0543 | val_loss 2.1519 | lr 9.550530e-04\n",
      "step 7000 | train_loss 2.0933 | val_loss 2.1427 | lr 9.524135e-04\n",
      "step 7200 | train_loss 2.1002 | val_loss 2.1435 | lr 9.497026e-04\n",
      "step 7400 | train_loss 2.1068 | val_loss 2.1414 | lr 9.469207e-04\n",
      "step 7600 | train_loss 2.1162 | val_loss 2.1478 | lr 9.440682e-04\n",
      "step 7800 | train_loss 2.1081 | val_loss 2.1416 | lr 9.411456e-04\n",
      "step 8000 | train_loss 2.1125 | val_loss 2.1422 | lr 9.381533e-04\n",
      "step 8200 | train_loss 2.1177 | val_loss 2.1403 | lr 9.350919e-04\n",
      "step 8400 | train_loss 2.1197 | val_loss 2.1390 | lr 9.319617e-04\n",
      "step 8600 | train_loss 2.1086 | val_loss 2.1388 | lr 9.287633e-04\n",
      "step 8800 | train_loss 2.1194 | val_loss 2.1334 | lr 9.254972e-04\n",
      "step 9000 | train_loss 2.1147 | val_loss 2.1362 | lr 9.221640e-04\n",
      "step 9200 | train_loss 2.1260 | val_loss 2.1360 | lr 9.187640e-04\n",
      "step 9400 | train_loss 2.1190 | val_loss 2.1363 | lr 9.152979e-04\n",
      "step 9600 | train_loss 2.1218 | val_loss 2.1386 | lr 9.117663e-04\n",
      "step 9800 | train_loss 2.1355 | val_loss 2.1300 | lr 9.081696e-04\n",
      "step 10000 | train_loss 2.1204 | val_loss 2.1235 | lr 9.045085e-04\n",
      "step 10200 | train_loss 2.0423 | val_loss 2.1301 | lr 9.007835e-04\n",
      "step 10400 | train_loss 2.0616 | val_loss 2.1305 | lr 8.969952e-04\n",
      "step 10600 | train_loss 2.0623 | val_loss 2.1222 | lr 8.931442e-04\n",
      "step 10800 | train_loss 2.0728 | val_loss 2.1236 | lr 8.892312e-04\n",
      "step 11000 | train_loss 2.0775 | val_loss 2.1180 | lr 8.852566e-04\n",
      "step 11200 | train_loss 2.0727 | val_loss 2.1256 | lr 8.812213e-04\n",
      "step 11400 | train_loss 2.0650 | val_loss 2.1209 | lr 8.771257e-04\n",
      "step 11600 | train_loss 2.0938 | val_loss 2.1228 | lr 8.729706e-04\n",
      "step 11800 | train_loss 2.0979 | val_loss 2.1214 | lr 8.687566e-04\n",
      "step 12000 | train_loss 2.0926 | val_loss 2.1158 | lr 8.644843e-04\n",
      "step 12200 | train_loss 2.0936 | val_loss 2.1203 | lr 8.601545e-04\n",
      "step 12400 | train_loss 2.0894 | val_loss 2.1185 | lr 8.557678e-04\n",
      "step 12600 | train_loss 2.0993 | val_loss 2.1214 | lr 8.513250e-04\n",
      "step 12800 | train_loss 2.1007 | val_loss 2.1210 | lr 8.468267e-04\n",
      "step 13000 | train_loss 2.0977 | val_loss 2.1167 | lr 8.422736e-04\n",
      "step 13200 | train_loss 2.1044 | val_loss 2.1147 | lr 8.376664e-04\n",
      "step 13400 | train_loss 1.9622 | val_loss 2.1123 | lr 8.330059e-04\n",
      "step 13600 | train_loss 2.0337 | val_loss 2.1138 | lr 8.282929e-04\n",
      "step 13800 | train_loss 2.0474 | val_loss 2.1139 | lr 8.235280e-04\n",
      "step 14000 | train_loss 2.0463 | val_loss 2.1111 | lr 8.187120e-04\n",
      "step 14200 | train_loss 2.0571 | val_loss 2.1132 | lr 8.138457e-04\n",
      "step 14400 | train_loss 2.0593 | val_loss 2.1025 | lr 8.089298e-04\n",
      "step 14600 | train_loss 2.0519 | val_loss 2.1056 | lr 8.039651e-04\n",
      "step 14800 | train_loss 2.0605 | val_loss 2.1076 | lr 7.989525e-04\n",
      "step 15000 | train_loss 2.0629 | val_loss 2.1096 | lr 7.938926e-04\n",
      "step 15200 | train_loss 2.0784 | val_loss 2.1038 | lr 7.887864e-04\n",
      "step 15400 | train_loss 2.0768 | val_loss 2.1030 | lr 7.836345e-04\n",
      "step 15600 | train_loss 2.0703 | val_loss 2.1118 | lr 7.784378e-04\n",
      "step 15800 | train_loss 2.0714 | val_loss 2.1076 | lr 7.731972e-04\n",
      "step 16000 | train_loss 2.0834 | val_loss 2.1086 | lr 7.679134e-04\n",
      "step 16200 | train_loss 2.0749 | val_loss 2.1097 | lr 7.625873e-04\n",
      "step 16400 | train_loss 2.0799 | val_loss 2.1013 | lr 7.572198e-04\n",
      "step 16600 | train_loss 2.0957 | val_loss 2.1036 | lr 7.518116e-04\n",
      "step 16800 | train_loss 1.9761 | val_loss 2.1025 | lr 7.463637e-04\n",
      "step 17000 | train_loss 2.0226 | val_loss 2.1014 | lr 7.408768e-04\n",
      "step 17200 | train_loss 2.0267 | val_loss 2.0955 | lr 7.353520e-04\n",
      "step 17400 | train_loss 2.0294 | val_loss 2.1004 | lr 7.297899e-04\n",
      "step 17600 | train_loss 2.0494 | val_loss 2.1020 | lr 7.241916e-04\n",
      "step 17800 | train_loss 2.0431 | val_loss 2.0949 | lr 7.185579e-04\n",
      "step 18000 | train_loss 2.0441 | val_loss 2.0919 | lr 7.128896e-04\n",
      "step 18200 | train_loss 2.0483 | val_loss 2.0983 | lr 7.071878e-04\n",
      "step 18400 | train_loss 2.0560 | val_loss 2.0977 | lr 7.014532e-04\n",
      "step 18600 | train_loss 2.0570 | val_loss 2.0978 | lr 6.956868e-04\n",
      "step 18800 | train_loss 2.0557 | val_loss 2.0943 | lr 6.898895e-04\n",
      "step 19000 | train_loss 2.0636 | val_loss 2.0985 | lr 6.840623e-04\n",
      "step 19200 | train_loss 2.0636 | val_loss 2.0953 | lr 6.782059e-04\n",
      "step 19400 | train_loss 2.0658 | val_loss 2.0971 | lr 6.723215e-04\n",
      "step 19600 | train_loss 2.0602 | val_loss 2.0978 | lr 6.664098e-04\n",
      "step 19800 | train_loss 2.0796 | val_loss 2.0948 | lr 6.604718e-04\n",
      "step 20000 | train_loss 2.0626 | val_loss 2.0876 | lr 6.545085e-04\n",
      "step 20200 | train_loss 1.9838 | val_loss 2.0938 | lr 6.485208e-04\n",
      "step 20400 | train_loss 2.0103 | val_loss 2.0901 | lr 6.425096e-04\n",
      "step 20600 | train_loss 2.0142 | val_loss 2.0887 | lr 6.364760e-04\n",
      "step 20800 | train_loss 2.0204 | val_loss 2.0882 | lr 6.304208e-04\n",
      "step 21000 | train_loss 2.0320 | val_loss 2.0898 | lr 6.243449e-04\n",
      "step 21200 | train_loss 2.0293 | val_loss 2.0871 | lr 6.182495e-04\n",
      "step 21400 | train_loss 2.0252 | val_loss 2.0874 | lr 6.121354e-04\n",
      "step 21600 | train_loss 2.0423 | val_loss 2.0879 | lr 6.060036e-04\n",
      "step 21800 | train_loss 2.0456 | val_loss 2.0870 | lr 5.998550e-04\n",
      "step 22000 | train_loss 2.0357 | val_loss 2.0852 | lr 5.936907e-04\n",
      "step 22200 | train_loss 2.0456 | val_loss 2.0855 | lr 5.875115e-04\n",
      "step 22400 | train_loss 2.0407 | val_loss 2.0908 | lr 5.813186e-04\n",
      "step 22600 | train_loss 2.0592 | val_loss 2.0902 | lr 5.751128e-04\n",
      "step 22800 | train_loss 2.0516 | val_loss 2.0909 | lr 5.688951e-04\n",
      "step 23000 | train_loss 2.0568 | val_loss 2.0894 | lr 5.626666e-04\n",
      "step 23200 | train_loss 2.0638 | val_loss 2.0870 | lr 5.564282e-04\n",
      "step 23400 | train_loss 1.9988 | val_loss 2.0822 | lr 5.501809e-04\n",
      "step 23600 | train_loss 1.9914 | val_loss 2.0858 | lr 5.439256e-04\n",
      "step 23800 | train_loss 2.0046 | val_loss 2.0856 | lr 5.376634e-04\n",
      "step 24000 | train_loss 2.0056 | val_loss 2.0855 | lr 5.313953e-04\n",
      "step 24200 | train_loss 2.0144 | val_loss 2.0827 | lr 5.251222e-04\n",
      "step 24400 | train_loss 2.0256 | val_loss 2.0794 | lr 5.188451e-04\n",
      "step 24600 | train_loss 2.0175 | val_loss 2.0836 | lr 5.125650e-04\n",
      "step 24800 | train_loss 2.0174 | val_loss 2.0780 | lr 5.062830e-04\n",
      "step 25000 | train_loss 2.0218 | val_loss 2.0824 | lr 5.000000e-04\n",
      "step 25200 | train_loss 2.0298 | val_loss 2.0803 | lr 4.937170e-04\n",
      "step 25400 | train_loss 2.0313 | val_loss 2.0787 | lr 4.874350e-04\n",
      "step 25600 | train_loss 2.0294 | val_loss 2.0814 | lr 4.811549e-04\n",
      "step 25800 | train_loss 2.0342 | val_loss 2.0819 | lr 4.748778e-04\n",
      "step 26000 | train_loss 2.0372 | val_loss 2.0813 | lr 4.686047e-04\n",
      "step 26200 | train_loss 2.0352 | val_loss 2.0827 | lr 4.623366e-04\n",
      "step 26400 | train_loss 2.0400 | val_loss 2.0789 | lr 4.560744e-04\n",
      "step 26600 | train_loss 2.0495 | val_loss 2.0774 | lr 4.498191e-04\n",
      "step 26800 | train_loss 1.9594 | val_loss 2.0794 | lr 4.435718e-04\n",
      "step 27000 | train_loss 1.9920 | val_loss 2.0773 | lr 4.373334e-04\n",
      "step 27200 | train_loss 1.9978 | val_loss 2.0749 | lr 4.311049e-04\n",
      "step 27400 | train_loss 2.0010 | val_loss 2.0770 | lr 4.248872e-04\n",
      "step 27600 | train_loss 2.0098 | val_loss 2.0794 | lr 4.186814e-04\n",
      "step 27800 | train_loss 2.0094 | val_loss 2.0719 | lr 4.124885e-04\n",
      "step 28000 | train_loss 2.0056 | val_loss 2.0732 | lr 4.063093e-04\n",
      "step 28200 | train_loss 2.0101 | val_loss 2.0738 | lr 4.001450e-04\n",
      "step 28400 | train_loss 2.0177 | val_loss 2.0750 | lr 3.939964e-04\n",
      "step 28600 | train_loss 2.0233 | val_loss 2.0722 | lr 3.878646e-04\n",
      "step 28800 | train_loss 2.0243 | val_loss 2.0715 | lr 3.817505e-04\n",
      "step 29000 | train_loss 2.0206 | val_loss 2.0732 | lr 3.756551e-04\n",
      "step 29200 | train_loss 2.0200 | val_loss 2.0732 | lr 3.695792e-04\n",
      "step 29400 | train_loss 2.0255 | val_loss 2.0756 | lr 3.635240e-04\n",
      "step 29600 | train_loss 2.0225 | val_loss 2.0764 | lr 3.574904e-04\n",
      "step 29800 | train_loss 2.0342 | val_loss 2.0743 | lr 3.514792e-04\n",
      "step 30000 | train_loss 2.0279 | val_loss 2.0714 | lr 3.454915e-04\n",
      "step 30200 | train_loss 1.9679 | val_loss 2.0725 | lr 3.395282e-04\n",
      "step 30400 | train_loss 1.9891 | val_loss 2.0694 | lr 3.335902e-04\n",
      "step 30600 | train_loss 1.9928 | val_loss 2.0700 | lr 3.276785e-04\n",
      "step 30800 | train_loss 1.9950 | val_loss 2.0700 | lr 3.217941e-04\n",
      "step 31000 | train_loss 2.0033 | val_loss 2.0722 | lr 3.159377e-04\n",
      "step 31200 | train_loss 2.0036 | val_loss 2.0699 | lr 3.101105e-04\n",
      "step 31400 | train_loss 2.0032 | val_loss 2.0689 | lr 3.043132e-04\n",
      "step 31600 | train_loss 2.0041 | val_loss 2.0685 | lr 2.985468e-04\n",
      "step 31800 | train_loss 2.0089 | val_loss 2.0678 | lr 2.928122e-04\n",
      "step 32000 | train_loss 2.0086 | val_loss 2.0680 | lr 2.871104e-04\n",
      "step 32200 | train_loss 2.0142 | val_loss 2.0655 | lr 2.814421e-04\n",
      "step 32400 | train_loss 2.0094 | val_loss 2.0687 | lr 2.758084e-04\n",
      "step 32600 | train_loss 2.0137 | val_loss 2.0682 | lr 2.702101e-04\n",
      "step 32800 | train_loss 2.0145 | val_loss 2.0697 | lr 2.646480e-04\n",
      "step 33000 | train_loss 2.0186 | val_loss 2.0720 | lr 2.591232e-04\n",
      "step 33200 | train_loss 2.0244 | val_loss 2.0680 | lr 2.536363e-04\n",
      "step 33400 | train_loss 2.0199 | val_loss 2.0655 | lr 2.481884e-04\n",
      "step 33600 | train_loss 1.9797 | val_loss 2.0678 | lr 2.427802e-04\n",
      "step 33800 | train_loss 1.9881 | val_loss 2.0675 | lr 2.374127e-04\n",
      "step 34000 | train_loss 1.9899 | val_loss 2.0669 | lr 2.320866e-04\n",
      "step 34200 | train_loss 1.9915 | val_loss 2.0662 | lr 2.268028e-04\n",
      "step 34400 | train_loss 1.9981 | val_loss 2.0661 | lr 2.215622e-04\n",
      "step 34600 | train_loss 1.9962 | val_loss 2.0669 | lr 2.163655e-04\n",
      "step 34800 | train_loss 1.9937 | val_loss 2.0642 | lr 2.112136e-04\n",
      "step 35000 | train_loss 1.9995 | val_loss 2.0648 | lr 2.061074e-04\n",
      "step 35200 | train_loss 2.0008 | val_loss 2.0640 | lr 2.010475e-04\n",
      "step 35400 | train_loss 2.0023 | val_loss 2.0625 | lr 1.960349e-04\n",
      "step 35600 | train_loss 2.0028 | val_loss 2.0633 | lr 1.910702e-04\n",
      "step 35800 | train_loss 2.0035 | val_loss 2.0647 | lr 1.861543e-04\n",
      "step 36000 | train_loss 2.0033 | val_loss 2.0646 | lr 1.812880e-04\n",
      "step 36200 | train_loss 2.0046 | val_loss 2.0651 | lr 1.764720e-04\n",
      "step 36400 | train_loss 2.0076 | val_loss 2.0650 | lr 1.717071e-04\n",
      "step 36600 | train_loss 2.0119 | val_loss 2.0633 | lr 1.669941e-04\n",
      "step 36800 | train_loss 1.9739 | val_loss 2.0636 | lr 1.623336e-04\n",
      "step 37000 | train_loss 1.9830 | val_loss 2.0639 | lr 1.577264e-04\n",
      "step 37200 | train_loss 1.9876 | val_loss 2.0636 | lr 1.531733e-04\n",
      "step 37400 | train_loss 1.9883 | val_loss 2.0637 | lr 1.486750e-04\n",
      "step 37600 | train_loss 1.9895 | val_loss 2.0641 | lr 1.442322e-04\n",
      "step 37800 | train_loss 1.9950 | val_loss 2.0626 | lr 1.398455e-04\n",
      "step 38000 | train_loss 1.9901 | val_loss 2.0628 | lr 1.355157e-04\n",
      "step 38200 | train_loss 1.9907 | val_loss 2.0622 | lr 1.312434e-04\n",
      "step 38400 | train_loss 1.9923 | val_loss 2.0621 | lr 1.270294e-04\n",
      "step 38600 | train_loss 1.9970 | val_loss 2.0596 | lr 1.228743e-04\n",
      "step 38800 | train_loss 1.9977 | val_loss 2.0595 | lr 1.187787e-04\n",
      "step 39000 | train_loss 1.9965 | val_loss 2.0610 | lr 1.147434e-04\n",
      "step 39200 | train_loss 1.9965 | val_loss 2.0607 | lr 1.107688e-04\n",
      "step 39400 | train_loss 1.9982 | val_loss 2.0615 | lr 1.068558e-04\n",
      "step 39600 | train_loss 1.9977 | val_loss 2.0620 | lr 1.030048e-04\n",
      "step 39800 | train_loss 2.0006 | val_loss 2.0610 | lr 9.921651e-05\n",
      "step 40000 | train_loss 2.0056 | val_loss 2.0608 | lr 9.549150e-05\n",
      "step 40200 | train_loss 1.9809 | val_loss 2.0607 | lr 9.183037e-05\n",
      "step 40400 | train_loss 1.9851 | val_loss 2.0601 | lr 8.823370e-05\n",
      "step 40600 | train_loss 1.9875 | val_loss 2.0603 | lr 8.470205e-05\n",
      "step 40800 | train_loss 1.9874 | val_loss 2.0607 | lr 8.123598e-05\n",
      "step 41000 | train_loss 1.9885 | val_loss 2.0610 | lr 7.783604e-05\n",
      "step 41200 | train_loss 1.9903 | val_loss 2.0603 | lr 7.450276e-05\n",
      "step 41400 | train_loss 1.9906 | val_loss 2.0605 | lr 7.123667e-05\n",
      "step 41600 | train_loss 1.9892 | val_loss 2.0600 | lr 6.803829e-05\n",
      "step 41800 | train_loss 1.9903 | val_loss 2.0596 | lr 6.490812e-05\n",
      "step 42000 | train_loss 1.9917 | val_loss 2.0591 | lr 6.184666e-05\n",
      "step 42200 | train_loss 1.9930 | val_loss 2.0583 | lr 5.885439e-05\n",
      "step 42400 | train_loss 1.9929 | val_loss 2.0589 | lr 5.593178e-05\n",
      "step 42600 | train_loss 1.9923 | val_loss 2.0588 | lr 5.307929e-05\n",
      "step 42800 | train_loss 1.9926 | val_loss 2.0594 | lr 5.029737e-05\n",
      "step 43000 | train_loss 1.9932 | val_loss 2.0600 | lr 4.758647e-05\n",
      "step 43200 | train_loss 1.9960 | val_loss 2.0591 | lr 4.494701e-05\n",
      "step 43400 | train_loss 1.9968 | val_loss 2.0589 | lr 4.237941e-05\n",
      "step 43600 | train_loss 1.9863 | val_loss 2.0591 | lr 3.988408e-05\n",
      "step 43800 | train_loss 1.9874 | val_loss 2.0590 | lr 3.746140e-05\n",
      "step 44000 | train_loss 1.9881 | val_loss 2.0590 | lr 3.511176e-05\n",
      "step 44200 | train_loss 1.9878 | val_loss 2.0590 | lr 3.283553e-05\n",
      "step 44400 | train_loss 1.9885 | val_loss 2.0591 | lr 3.063307e-05\n",
      "step 44600 | train_loss 1.9891 | val_loss 2.0591 | lr 2.850473e-05\n",
      "step 44800 | train_loss 1.9890 | val_loss 2.0592 | lr 2.645085e-05\n",
      "step 45000 | train_loss 1.9889 | val_loss 2.0588 | lr 2.447174e-05\n",
      "step 45200 | train_loss 1.9890 | val_loss 2.0586 | lr 2.256773e-05\n",
      "step 45400 | train_loss 1.9892 | val_loss 2.0583 | lr 2.073911e-05\n",
      "step 45600 | train_loss 1.9898 | val_loss 2.0581 | lr 1.898616e-05\n",
      "step 45800 | train_loss 1.9896 | val_loss 2.0584 | lr 1.730918e-05\n",
      "step 46000 | train_loss 1.9898 | val_loss 2.0583 | lr 1.570842e-05\n",
      "step 46200 | train_loss 1.9898 | val_loss 2.0585 | lr 1.418413e-05\n",
      "step 46400 | train_loss 1.9903 | val_loss 2.0586 | lr 1.273656e-05\n",
      "step 46600 | train_loss 1.9910 | val_loss 2.0584 | lr 1.136594e-05\n",
      "step 46800 | train_loss 1.9891 | val_loss 2.0584 | lr 1.007247e-05\n",
      "step 47000 | train_loss 1.9888 | val_loss 2.0584 | lr 8.856375e-06\n",
      "step 47200 | train_loss 1.9889 | val_loss 2.0584 | lr 7.717833e-06\n",
      "step 47400 | train_loss 1.9890 | val_loss 2.0584 | lr 6.657028e-06\n",
      "step 47600 | train_loss 1.9890 | val_loss 2.0584 | lr 5.674128e-06\n",
      "step 47800 | train_loss 1.9892 | val_loss 2.0584 | lr 4.769287e-06\n",
      "step 48000 | train_loss 1.9893 | val_loss 2.0585 | lr 3.942649e-06\n",
      "step 48200 | train_loss 1.9892 | val_loss 2.0585 | lr 3.194345e-06\n",
      "step 48400 | train_loss 1.9892 | val_loss 2.0584 | lr 2.524492e-06\n",
      "step 48600 | train_loss 1.9892 | val_loss 2.0584 | lr 1.933195e-06\n",
      "step 48800 | train_loss 1.9892 | val_loss 2.0584 | lr 1.420550e-06\n",
      "step 49000 | train_loss 1.9892 | val_loss 2.0584 | lr 9.866358e-07\n",
      "step 49200 | train_loss 1.9892 | val_loss 2.0584 | lr 6.315217e-07\n",
      "step 49400 | train_loss 1.9892 | val_loss 2.0584 | lr 3.552637e-07\n",
      "step 49600 | train_loss 1.9892 | val_loss 2.0584 | lr 1.579054e-07\n",
      "step 49800 | train_loss 1.9892 | val_loss 2.0584 | lr 3.947790e-08\n",
      "step 49999 | train_loss 1.9892 | val_loss 2.0584 | lr 9.869605e-13\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
