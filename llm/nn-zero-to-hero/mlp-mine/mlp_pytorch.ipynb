{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.232944Z",
     "start_time": "2024-07-19T14:00:10.183324Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Implements a simple n-gram language model in PyTorch.\n",
    "Acts as the correctness reference for all the other versions.\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import optuna\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\50196\\.conda\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.236459Z",
     "start_time": "2024-07-19T14:00:11.233947Z"
    }
   },
   "cell_type": "code",
   "source": "from common import RNG",
   "id": "1a3a93b4262a11b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter ",
   "id": "a9e74038e4d240e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.239396Z",
     "start_time": "2024-07-19T14:00:11.237462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = 3 # if 3 tokens predict the 4th, this is a 4-gram model\n",
    "embedding_size = 64\n",
    "hidden_size = 512\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_steps = 50000"
   ],
   "id": "5454ffdaaf4fd139",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MLP",
   "id": "859bd5ad605936b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.243006Z",
     "start_time": "2024-07-19T14:00:11.239396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, context_length, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(context_length * embedding_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        emb = self.wte(idx)\n",
    "        emb = emb.view(B, -1)\n",
    "        logits = self.mlp(emb)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss"
   ],
   "id": "cf26ea2286eb8b20",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.247010Z",
     "start_time": "2024-07-19T14:00:11.244010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dataloader(tokens, context_length, batch_size, device):\n",
    "    n = len(tokens)\n",
    "    inputs, targets = [], []\n",
    "    pos = 0\n",
    "    while True:\n",
    "        window = tokens[pos:pos + context_length + 1]\n",
    "        inputs.append(window[:-1])\n",
    "        targets.append(window[-1])\n",
    "        if len(inputs) == batch_size:\n",
    "            yield (torch.tensor(inputs, device=device), torch.tensor(targets, device=device))\n",
    "            inputs, targets = [], []\n",
    "        pos += 1\n",
    "        if pos + context_length >= n:\n",
    "            pos = 0"
   ],
   "id": "ab461da83e9fbc28",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.250588Z",
     "start_time": "2024-07-19T14:00:11.248140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_split(model, tokens, context_length, batch_size, device, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = len(tokens) // batch_size\n",
    "    if max_batches is not None:\n",
    "        num_batches = min(num_batches, max_batches)\n",
    "    data_iter = dataloader(tokens, context_length, batch_size, device)\n",
    "    for _ in range(num_batches):\n",
    "        inputs, targets = next(data_iter)\n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    mean_loss = total_loss / num_batches\n",
    "    return mean_loss"
   ],
   "id": "9ed89c2e8a0fcb6a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "8213011a2d8fc534"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.254533Z",
     "start_time": "2024-07-19T14:00:11.250588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, train_tokens, val_tokens, context_length, batch_size, num_steps, learning_rate, device):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    train_data_iter = dataloader(train_tokens, context_length, batch_size, device)\n",
    "    for step in range(num_steps):\n",
    "        lr = learning_rate * 0.5 * (1 + math.cos(math.pi * step / num_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        if step % 200 == 0 or step == num_steps - 1:\n",
    "            train_loss = eval_split(model, train_tokens, context_length, batch_size, device, max_batches=20)\n",
    "            val_loss = eval_split(model, val_tokens, context_length, batch_size, device)\n",
    "            print(f'step {step} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | lr {lr:e}')\n",
    "        model.train()\n",
    "        inputs, targets = next(train_data_iter)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return val_loss"
   ],
   "id": "b66e7d6de0f84ce1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.256935Z",
     "start_time": "2024-07-19T14:00:11.254533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random = RNG(1337)\n",
    "# TODO: actually use this rng for the model initialization\n"
   ],
   "id": "c06b73caddad99e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## (optional) Optimize Hyperparameter",
   "id": "d42178e568730a0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:00:11.260444Z",
     "start_time": "2024-07-19T14:00:11.256935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    embedding_size = trial.suggest_int('embedding_size', 16, 64)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 256, 1024)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    context_length = trial.suggest_int('context_length', 2, 5)\n",
    "\n",
    "    # Load data and preprocess\n",
    "    train_text = open('data/train.txt', 'r').read()\n",
    "    uchars = sorted(list(set(train_text)))\n",
    "    vocab_size = len(uchars)\n",
    "    char_to_token = {c: i for i, c in enumerate(uchars)}\n",
    "    train_tokens = [char_to_token[c] for c in train_text]\n",
    "    val_tokens = [char_to_token[c] for c in open('data/val.txt', 'r').read()]\n",
    "\n",
    "    # Create the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MLP(vocab_size, context_length, embedding_size, hidden_size).to(device)\n",
    "\n",
    "    # Train the model\n",
    "    num_steps = 50000\n",
    "    val_loss = train(model, train_tokens, val_tokens, context_length, batch_size, num_steps, learning_rate, device)\n",
    "    \n",
    "    return val_loss"
   ],
   "id": "589325cd0dc68d35",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:23:21.730648Z",
     "start_time": "2024-07-19T14:00:11.260444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: {}'.format(trial.value))\n",
    "\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))"
   ],
   "id": "5900141b06d7b465",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:00:11,261] A new study created in memory with name: no-name-f27561d9-03d1-4777-a7c7-83960f9f0297\n",
      "C:\\Users\\50196\\AppData\\Local\\Temp\\ipykernel_46372\\779706708.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | train_loss 3.3023 | val_loss 3.3001 | lr 3.140955e-04\n",
      "step 200 | train_loss 2.3936 | val_loss 2.4220 | lr 3.140831e-04\n",
      "step 400 | train_loss 2.3212 | val_loss 2.3475 | lr 3.140459e-04\n",
      "step 600 | train_loss 2.2930 | val_loss 2.3173 | lr 3.139839e-04\n",
      "step 800 | train_loss 2.2743 | val_loss 2.2917 | lr 3.138971e-04\n",
      "step 1000 | train_loss 2.2544 | val_loss 2.2694 | lr 3.137856e-04\n",
      "step 1200 | train_loss 2.2414 | val_loss 2.2576 | lr 3.136493e-04\n",
      "step 1400 | train_loss 2.2335 | val_loss 2.2465 | lr 3.134883e-04\n",
      "step 1600 | train_loss 2.2218 | val_loss 2.2352 | lr 3.133026e-04\n",
      "step 1800 | train_loss 2.1903 | val_loss 2.2264 | lr 3.130922e-04\n",
      "step 2000 | train_loss 2.1905 | val_loss 2.2186 | lr 3.128571e-04\n",
      "step 2200 | train_loss 2.1830 | val_loss 2.2111 | lr 3.125975e-04\n",
      "step 2400 | train_loss 2.1769 | val_loss 2.2046 | lr 3.123133e-04\n",
      "step 2600 | train_loss 2.1821 | val_loss 2.1989 | lr 3.120046e-04\n",
      "step 2800 | train_loss 2.1704 | val_loss 2.1931 | lr 3.116714e-04\n",
      "step 3000 | train_loss 2.1700 | val_loss 2.1901 | lr 3.113137e-04\n",
      "step 3200 | train_loss 2.1646 | val_loss 2.1829 | lr 3.109318e-04\n",
      "step 3400 | train_loss 2.1145 | val_loss 2.1786 | lr 3.105255e-04\n",
      "step 3600 | train_loss 2.1406 | val_loss 2.1738 | lr 3.100950e-04\n",
      "step 3800 | train_loss 2.1400 | val_loss 2.1751 | lr 3.096403e-04\n",
      "step 4000 | train_loss 2.1367 | val_loss 2.1674 | lr 3.091616e-04\n",
      "step 4200 | train_loss 2.1372 | val_loss 2.1627 | lr 3.086588e-04\n",
      "step 4400 | train_loss 2.1317 | val_loss 2.1567 | lr 3.081320e-04\n",
      "step 4600 | train_loss 2.1298 | val_loss 2.1569 | lr 3.075814e-04\n",
      "step 4800 | train_loss 2.1290 | val_loss 2.1558 | lr 3.070071e-04\n",
      "step 5000 | train_loss 2.1248 | val_loss 2.1487 | lr 3.064090e-04\n",
      "step 5200 | train_loss 2.1029 | val_loss 2.1523 | lr 3.057874e-04\n",
      "step 5400 | train_loss 2.1031 | val_loss 2.1459 | lr 3.051423e-04\n",
      "step 5600 | train_loss 2.1022 | val_loss 2.1458 | lr 3.044738e-04\n",
      "step 5800 | train_loss 2.1098 | val_loss 2.1395 | lr 3.037820e-04\n",
      "step 6000 | train_loss 2.1059 | val_loss 2.1348 | lr 3.030671e-04\n",
      "step 6200 | train_loss 2.1038 | val_loss 2.1354 | lr 3.023290e-04\n",
      "step 6400 | train_loss 2.1090 | val_loss 2.1359 | lr 3.015681e-04\n",
      "step 6600 | train_loss 2.1064 | val_loss 2.1349 | lr 3.007843e-04\n",
      "step 6800 | train_loss 2.0730 | val_loss 2.1315 | lr 2.999778e-04\n",
      "step 7000 | train_loss 2.0797 | val_loss 2.1283 | lr 2.991488e-04\n",
      "step 7200 | train_loss 2.0819 | val_loss 2.1253 | lr 2.982973e-04\n",
      "step 7400 | train_loss 2.0792 | val_loss 2.1227 | lr 2.974235e-04\n",
      "step 7600 | train_loss 2.0867 | val_loss 2.1216 | lr 2.965276e-04\n",
      "step 7800 | train_loss 2.0841 | val_loss 2.1229 | lr 2.956096e-04\n",
      "step 8000 | train_loss 2.0863 | val_loss 2.1226 | lr 2.946697e-04\n",
      "step 8200 | train_loss 2.0826 | val_loss 2.1173 | lr 2.937081e-04\n",
      "step 8400 | train_loss 2.0295 | val_loss 2.1184 | lr 2.927250e-04\n",
      "step 8600 | train_loss 2.0588 | val_loss 2.1129 | lr 2.917204e-04\n",
      "step 8800 | train_loss 2.0641 | val_loss 2.1190 | lr 2.906945e-04\n",
      "step 9000 | train_loss 2.0610 | val_loss 2.1108 | lr 2.896475e-04\n",
      "step 9200 | train_loss 2.0709 | val_loss 2.1114 | lr 2.885796e-04\n",
      "step 9400 | train_loss 2.0669 | val_loss 2.1065 | lr 2.874910e-04\n",
      "step 9600 | train_loss 2.0650 | val_loss 2.1080 | lr 2.863817e-04\n",
      "step 9800 | train_loss 2.0686 | val_loss 2.1093 | lr 2.852520e-04\n",
      "step 10000 | train_loss 2.0673 | val_loss 2.1052 | lr 2.841020e-04\n",
      "step 10200 | train_loss 2.0397 | val_loss 2.1064 | lr 2.829320e-04\n",
      "step 10400 | train_loss 2.0471 | val_loss 2.1040 | lr 2.817422e-04\n",
      "step 10600 | train_loss 2.0483 | val_loss 2.1046 | lr 2.805326e-04\n",
      "step 10800 | train_loss 2.0527 | val_loss 2.1011 | lr 2.793035e-04\n",
      "step 11000 | train_loss 2.0539 | val_loss 2.1017 | lr 2.780551e-04\n",
      "step 11200 | train_loss 2.0507 | val_loss 2.1006 | lr 2.767876e-04\n",
      "step 11400 | train_loss 2.0575 | val_loss 2.1020 | lr 2.755012e-04\n",
      "step 11600 | train_loss 2.0606 | val_loss 2.1014 | lr 2.741961e-04\n",
      "step 11800 | train_loss 2.0222 | val_loss 2.0984 | lr 2.728725e-04\n",
      "step 12000 | train_loss 2.0325 | val_loss 2.0966 | lr 2.715306e-04\n",
      "step 12200 | train_loss 2.0378 | val_loss 2.0964 | lr 2.701707e-04\n",
      "step 12400 | train_loss 2.0332 | val_loss 2.0927 | lr 2.687928e-04\n",
      "step 12600 | train_loss 2.0417 | val_loss 2.0943 | lr 2.673973e-04\n",
      "step 12800 | train_loss 2.0434 | val_loss 2.0936 | lr 2.659844e-04\n",
      "step 13000 | train_loss 2.0435 | val_loss 2.0954 | lr 2.645543e-04\n",
      "step 13200 | train_loss 2.0440 | val_loss 2.0923 | lr 2.631072e-04\n",
      "step 13400 | train_loss 1.9890 | val_loss 2.0934 | lr 2.616434e-04\n",
      "step 13600 | train_loss 2.0180 | val_loss 2.0894 | lr 2.601631e-04\n",
      "step 13800 | train_loss 2.0239 | val_loss 2.0945 | lr 2.586664e-04\n",
      "step 14000 | train_loss 2.0228 | val_loss 2.0888 | lr 2.571538e-04\n",
      "step 14200 | train_loss 2.0321 | val_loss 2.0887 | lr 2.556253e-04\n",
      "step 14400 | train_loss 2.0324 | val_loss 2.0845 | lr 2.540812e-04\n",
      "step 14600 | train_loss 2.0307 | val_loss 2.0871 | lr 2.525218e-04\n",
      "step 14800 | train_loss 2.0335 | val_loss 2.0878 | lr 2.509474e-04\n",
      "step 15000 | train_loss 2.0352 | val_loss 2.0860 | lr 2.493581e-04\n",
      "step 15200 | train_loss 2.0060 | val_loss 2.0845 | lr 2.477542e-04\n",
      "step 15400 | train_loss 2.0143 | val_loss 2.0849 | lr 2.461361e-04\n",
      "step 15600 | train_loss 2.0169 | val_loss 2.0855 | lr 2.445038e-04\n",
      "step 15800 | train_loss 2.0199 | val_loss 2.0843 | lr 2.428577e-04\n",
      "step 16000 | train_loss 2.0231 | val_loss 2.0844 | lr 2.411981e-04\n",
      "step 16200 | train_loss 2.0215 | val_loss 2.0820 | lr 2.395252e-04\n",
      "step 16400 | train_loss 2.0279 | val_loss 2.0841 | lr 2.378393e-04\n",
      "step 16600 | train_loss 2.0314 | val_loss 2.0825 | lr 2.361406e-04\n",
      "step 16800 | train_loss 1.9921 | val_loss 2.0812 | lr 2.344295e-04\n",
      "step 17000 | train_loss 2.0065 | val_loss 2.0800 | lr 2.327061e-04\n",
      "step 17200 | train_loss 2.0085 | val_loss 2.0801 | lr 2.309707e-04\n",
      "step 17400 | train_loss 2.0058 | val_loss 2.0792 | lr 2.292237e-04\n",
      "step 17600 | train_loss 2.0157 | val_loss 2.0797 | lr 2.274653e-04\n",
      "step 17800 | train_loss 2.0172 | val_loss 2.0785 | lr 2.256958e-04\n",
      "step 18000 | train_loss 2.0181 | val_loss 2.0819 | lr 2.239154e-04\n",
      "step 18200 | train_loss 2.0186 | val_loss 2.0795 | lr 2.221245e-04\n",
      "step 18400 | train_loss 1.9723 | val_loss 2.0795 | lr 2.203233e-04\n",
      "step 18600 | train_loss 1.9933 | val_loss 2.0785 | lr 2.185121e-04\n",
      "step 18800 | train_loss 1.9995 | val_loss 2.0804 | lr 2.166912e-04\n",
      "step 19000 | train_loss 2.0002 | val_loss 2.0775 | lr 2.148609e-04\n",
      "step 19200 | train_loss 2.0084 | val_loss 2.0778 | lr 2.130214e-04\n",
      "step 19400 | train_loss 2.0076 | val_loss 2.0727 | lr 2.111731e-04\n",
      "step 19600 | train_loss 2.0097 | val_loss 2.0754 | lr 2.093163e-04\n",
      "step 19800 | train_loss 2.0111 | val_loss 2.0765 | lr 2.074512e-04\n",
      "step 20000 | train_loss 2.0188 | val_loss 2.0771 | lr 2.055782e-04\n",
      "step 20200 | train_loss 1.9860 | val_loss 2.0751 | lr 2.036975e-04\n",
      "step 20400 | train_loss 1.9933 | val_loss 2.0754 | lr 2.018094e-04\n",
      "step 20600 | train_loss 1.9952 | val_loss 2.0739 | lr 1.999142e-04\n",
      "step 20800 | train_loss 1.9980 | val_loss 2.0742 | lr 1.980123e-04\n",
      "step 21000 | train_loss 2.0034 | val_loss 2.0733 | lr 1.961039e-04\n",
      "step 21200 | train_loss 2.0040 | val_loss 2.0715 | lr 1.941894e-04\n",
      "step 21400 | train_loss 2.0080 | val_loss 2.0744 | lr 1.922690e-04\n",
      "step 21600 | train_loss 2.0120 | val_loss 2.0731 | lr 1.903430e-04\n",
      "step 21800 | train_loss 1.9717 | val_loss 2.0726 | lr 1.884118e-04\n",
      "step 22000 | train_loss 1.9885 | val_loss 2.0708 | lr 1.864756e-04\n",
      "step 22200 | train_loss 1.9907 | val_loss 2.0718 | lr 1.845347e-04\n",
      "step 22400 | train_loss 1.9890 | val_loss 2.0718 | lr 1.825895e-04\n",
      "step 22600 | train_loss 1.9991 | val_loss 2.0704 | lr 1.806403e-04\n",
      "step 22800 | train_loss 1.9989 | val_loss 2.0688 | lr 1.786874e-04\n",
      "step 23000 | train_loss 2.0004 | val_loss 2.0722 | lr 1.767311e-04\n",
      "step 23200 | train_loss 2.0030 | val_loss 2.0715 | lr 1.747716e-04\n",
      "step 23400 | train_loss 1.9797 | val_loss 2.0700 | lr 1.728093e-04\n",
      "step 23600 | train_loss 1.9779 | val_loss 2.0722 | lr 1.708446e-04\n",
      "step 23800 | train_loss 1.9840 | val_loss 2.0709 | lr 1.688777e-04\n",
      "step 24000 | train_loss 1.9869 | val_loss 2.0710 | lr 1.669089e-04\n",
      "step 24200 | train_loss 1.9915 | val_loss 2.0696 | lr 1.649385e-04\n",
      "step 24400 | train_loss 1.9909 | val_loss 2.0667 | lr 1.629669e-04\n",
      "step 24600 | train_loss 1.9938 | val_loss 2.0683 | lr 1.609944e-04\n",
      "step 24800 | train_loss 1.9961 | val_loss 2.0695 | lr 1.590212e-04\n",
      "step 25000 | train_loss 2.0015 | val_loss 2.0690 | lr 1.570477e-04\n",
      "step 25200 | train_loss 1.9730 | val_loss 2.0677 | lr 1.550743e-04\n",
      "step 25400 | train_loss 1.9790 | val_loss 2.0682 | lr 1.531011e-04\n",
      "step 25600 | train_loss 1.9814 | val_loss 2.0666 | lr 1.511286e-04\n",
      "step 25800 | train_loss 1.9816 | val_loss 2.0674 | lr 1.491570e-04\n",
      "step 26000 | train_loss 1.9910 | val_loss 2.0657 | lr 1.471866e-04\n",
      "step 26200 | train_loss 1.9889 | val_loss 2.0649 | lr 1.452178e-04\n",
      "step 26400 | train_loss 1.9923 | val_loss 2.0679 | lr 1.432509e-04\n",
      "step 26600 | train_loss 1.9953 | val_loss 2.0663 | lr 1.412862e-04\n",
      "step 26800 | train_loss 1.9604 | val_loss 2.0666 | lr 1.393239e-04\n",
      "step 27000 | train_loss 1.9768 | val_loss 2.0655 | lr 1.373644e-04\n",
      "step 27200 | train_loss 1.9785 | val_loss 2.0673 | lr 1.354081e-04\n",
      "step 27400 | train_loss 1.9794 | val_loss 2.0659 | lr 1.334552e-04\n",
      "step 27600 | train_loss 1.9851 | val_loss 2.0645 | lr 1.315059e-04\n",
      "step 27800 | train_loss 1.9862 | val_loss 2.0630 | lr 1.295608e-04\n",
      "step 28000 | train_loss 1.9860 | val_loss 2.0650 | lr 1.276199e-04\n",
      "step 28200 | train_loss 1.9907 | val_loss 2.0666 | lr 1.256837e-04\n",
      "step 28400 | train_loss 1.9853 | val_loss 2.0632 | lr 1.237525e-04\n",
      "step 28600 | train_loss 1.9693 | val_loss 2.0664 | lr 1.218265e-04\n",
      "step 28800 | train_loss 1.9731 | val_loss 2.0645 | lr 1.199061e-04\n",
      "step 29000 | train_loss 1.9758 | val_loss 2.0657 | lr 1.179916e-04\n",
      "step 29200 | train_loss 1.9793 | val_loss 2.0635 | lr 1.160832e-04\n",
      "step 29400 | train_loss 1.9803 | val_loss 2.0621 | lr 1.141813e-04\n",
      "step 29600 | train_loss 1.9808 | val_loss 2.0629 | lr 1.122861e-04\n",
      "step 29800 | train_loss 1.9847 | val_loss 2.0643 | lr 1.103980e-04\n",
      "step 30000 | train_loss 1.9869 | val_loss 2.0630 | lr 1.085173e-04\n",
      "step 30200 | train_loss 1.9657 | val_loss 2.0633 | lr 1.066443e-04\n",
      "step 30400 | train_loss 1.9717 | val_loss 2.0630 | lr 1.047792e-04\n",
      "step 30600 | train_loss 1.9734 | val_loss 2.0624 | lr 1.029224e-04\n",
      "step 30800 | train_loss 1.9723 | val_loss 2.0630 | lr 1.010741e-04\n",
      "step 31000 | train_loss 1.9796 | val_loss 2.0608 | lr 9.923462e-05\n",
      "step 31200 | train_loss 1.9777 | val_loss 2.0613 | lr 9.740430e-05\n",
      "step 31400 | train_loss 1.9798 | val_loss 2.0633 | lr 9.558340e-05\n",
      "step 31600 | train_loss 1.9828 | val_loss 2.0618 | lr 9.377220e-05\n",
      "step 31800 | train_loss 1.9562 | val_loss 2.0623 | lr 9.197100e-05\n",
      "step 32000 | train_loss 1.9691 | val_loss 2.0617 | lr 9.018007e-05\n",
      "step 32200 | train_loss 1.9704 | val_loss 2.0629 | lr 8.839970e-05\n",
      "step 32400 | train_loss 1.9723 | val_loss 2.0620 | lr 8.663017e-05\n",
      "step 32600 | train_loss 1.9749 | val_loss 2.0607 | lr 8.487177e-05\n",
      "step 32800 | train_loss 1.9762 | val_loss 2.0594 | lr 8.312476e-05\n",
      "step 33000 | train_loss 1.9752 | val_loss 2.0603 | lr 8.138942e-05\n",
      "step 33200 | train_loss 1.9791 | val_loss 2.0623 | lr 7.966603e-05\n",
      "step 33400 | train_loss 1.9793 | val_loss 2.0602 | lr 7.795486e-05\n",
      "step 33600 | train_loss 1.9647 | val_loss 2.0616 | lr 7.625618e-05\n",
      "step 33800 | train_loss 1.9674 | val_loss 2.0611 | lr 7.457026e-05\n",
      "step 34000 | train_loss 1.9687 | val_loss 2.0615 | lr 7.289736e-05\n",
      "step 34200 | train_loss 1.9708 | val_loss 2.0603 | lr 7.123775e-05\n",
      "step 34400 | train_loss 1.9733 | val_loss 2.0590 | lr 6.959169e-05\n",
      "step 34600 | train_loss 1.9722 | val_loss 2.0594 | lr 6.795944e-05\n",
      "step 34800 | train_loss 1.9752 | val_loss 2.0609 | lr 6.634126e-05\n",
      "step 35000 | train_loss 1.9772 | val_loss 2.0602 | lr 6.473740e-05\n",
      "step 35200 | train_loss 1.9624 | val_loss 2.0601 | lr 6.314812e-05\n",
      "step 35400 | train_loss 1.9669 | val_loss 2.0600 | lr 6.157366e-05\n",
      "step 35600 | train_loss 1.9681 | val_loss 2.0599 | lr 6.001429e-05\n",
      "step 35800 | train_loss 1.9673 | val_loss 2.0600 | lr 5.847023e-05\n",
      "step 36000 | train_loss 1.9708 | val_loss 2.0580 | lr 5.694175e-05\n",
      "step 36200 | train_loss 1.9709 | val_loss 2.0588 | lr 5.542907e-05\n",
      "step 36400 | train_loss 1.9712 | val_loss 2.0595 | lr 5.393243e-05\n",
      "step 36600 | train_loss 1.9733 | val_loss 2.0591 | lr 5.245208e-05\n",
      "step 36800 | train_loss 1.9583 | val_loss 2.0593 | lr 5.098825e-05\n",
      "step 37000 | train_loss 1.9644 | val_loss 2.0591 | lr 4.954117e-05\n",
      "step 37200 | train_loss 1.9656 | val_loss 2.0598 | lr 4.811106e-05\n",
      "step 37400 | train_loss 1.9667 | val_loss 2.0595 | lr 4.669815e-05\n",
      "step 37600 | train_loss 1.9678 | val_loss 2.0586 | lr 4.530267e-05\n",
      "step 37800 | train_loss 1.9695 | val_loss 2.0575 | lr 4.392484e-05\n",
      "step 38000 | train_loss 1.9685 | val_loss 2.0577 | lr 4.256487e-05\n",
      "step 38200 | train_loss 1.9706 | val_loss 2.0591 | lr 4.122297e-05\n",
      "step 38400 | train_loss 1.9726 | val_loss 2.0584 | lr 3.989937e-05\n",
      "step 38600 | train_loss 1.9630 | val_loss 2.0585 | lr 3.859427e-05\n",
      "step 38800 | train_loss 1.9649 | val_loss 2.0586 | lr 3.730787e-05\n",
      "step 39000 | train_loss 1.9655 | val_loss 2.0588 | lr 3.604038e-05\n",
      "step 39200 | train_loss 1.9657 | val_loss 2.0584 | lr 3.479200e-05\n",
      "step 39400 | train_loss 1.9677 | val_loss 2.0574 | lr 3.356292e-05\n",
      "step 39600 | train_loss 1.9675 | val_loss 2.0575 | lr 3.235334e-05\n",
      "step 39800 | train_loss 1.9683 | val_loss 2.0583 | lr 3.116346e-05\n",
      "step 40000 | train_loss 1.9705 | val_loss 2.0580 | lr 2.999345e-05\n",
      "step 40200 | train_loss 1.9622 | val_loss 2.0579 | lr 2.884351e-05\n",
      "step 40400 | train_loss 1.9646 | val_loss 2.0580 | lr 2.771381e-05\n",
      "step 40600 | train_loss 1.9650 | val_loss 2.0580 | lr 2.660453e-05\n",
      "step 40800 | train_loss 1.9650 | val_loss 2.0583 | lr 2.551586e-05\n",
      "step 41000 | train_loss 1.9657 | val_loss 2.0573 | lr 2.444795e-05\n",
      "step 41200 | train_loss 1.9668 | val_loss 2.0570 | lr 2.340098e-05\n",
      "step 41400 | train_loss 1.9664 | val_loss 2.0573 | lr 2.237512e-05\n",
      "step 41600 | train_loss 1.9678 | val_loss 2.0575 | lr 2.137052e-05\n",
      "step 41800 | train_loss 1.9621 | val_loss 2.0575 | lr 2.038735e-05\n",
      "step 42000 | train_loss 1.9634 | val_loss 2.0576 | lr 1.942576e-05\n",
      "step 42200 | train_loss 1.9642 | val_loss 2.0579 | lr 1.848590e-05\n",
      "step 42400 | train_loss 1.9646 | val_loss 2.0579 | lr 1.756792e-05\n",
      "step 42600 | train_loss 1.9647 | val_loss 2.0575 | lr 1.667197e-05\n",
      "step 42800 | train_loss 1.9656 | val_loss 2.0568 | lr 1.579818e-05\n",
      "step 43000 | train_loss 1.9654 | val_loss 2.0568 | lr 1.494670e-05\n",
      "step 43200 | train_loss 1.9660 | val_loss 2.0573 | lr 1.411765e-05\n",
      "step 43400 | train_loss 1.9673 | val_loss 2.0571 | lr 1.331118e-05\n",
      "step 43600 | train_loss 1.9636 | val_loss 2.0571 | lr 1.252741e-05\n",
      "step 43800 | train_loss 1.9644 | val_loss 2.0573 | lr 1.176646e-05\n",
      "step 44000 | train_loss 1.9645 | val_loss 2.0574 | lr 1.102844e-05\n",
      "step 44200 | train_loss 1.9643 | val_loss 2.0575 | lr 1.031349e-05\n",
      "step 44400 | train_loss 1.9648 | val_loss 2.0570 | lr 9.621710e-06\n",
      "step 44600 | train_loss 1.9650 | val_loss 2.0569 | lr 8.953208e-06\n",
      "step 44800 | train_loss 1.9651 | val_loss 2.0571 | lr 8.308092e-06\n",
      "step 45000 | train_loss 1.9658 | val_loss 2.0570 | lr 7.686464e-06\n",
      "step 45200 | train_loss 1.9637 | val_loss 2.0570 | lr 7.088422e-06\n",
      "step 45400 | train_loss 1.9643 | val_loss 2.0571 | lr 6.514060e-06\n",
      "step 45600 | train_loss 1.9644 | val_loss 2.0572 | lr 5.963469e-06\n",
      "step 45800 | train_loss 1.9645 | val_loss 2.0573 | lr 5.436736e-06\n",
      "step 46000 | train_loss 1.9644 | val_loss 2.0570 | lr 4.933944e-06\n",
      "step 46200 | train_loss 1.9647 | val_loss 2.0569 | lr 4.455172e-06\n",
      "step 46400 | train_loss 1.9646 | val_loss 2.0569 | lr 4.000497e-06\n",
      "step 46600 | train_loss 1.9648 | val_loss 2.0570 | lr 3.569990e-06\n",
      "step 46800 | train_loss 1.9641 | val_loss 2.0570 | lr 3.163719e-06\n",
      "step 47000 | train_loss 1.9642 | val_loss 2.0570 | lr 2.781747e-06\n",
      "step 47200 | train_loss 1.9643 | val_loss 2.0571 | lr 2.424137e-06\n",
      "step 47400 | train_loss 1.9644 | val_loss 2.0571 | lr 2.090942e-06\n",
      "step 47600 | train_loss 1.9644 | val_loss 2.0571 | lr 1.782218e-06\n",
      "step 47800 | train_loss 1.9645 | val_loss 2.0570 | lr 1.498012e-06\n",
      "step 48000 | train_loss 1.9645 | val_loss 2.0570 | lr 1.238368e-06\n",
      "step 48200 | train_loss 1.9645 | val_loss 2.0570 | lr 1.003329e-06\n",
      "step 48400 | train_loss 1.9646 | val_loss 2.0570 | lr 7.929314e-07\n",
      "step 48600 | train_loss 1.9644 | val_loss 2.0570 | lr 6.072080e-07\n",
      "step 48800 | train_loss 1.9644 | val_loss 2.0570 | lr 4.461883e-07\n",
      "step 49000 | train_loss 1.9644 | val_loss 2.0570 | lr 3.098979e-07\n",
      "step 49200 | train_loss 1.9644 | val_loss 2.0570 | lr 1.983581e-07\n",
      "step 49400 | train_loss 1.9644 | val_loss 2.0570 | lr 1.115867e-07\n",
      "step 49600 | train_loss 1.9644 | val_loss 2.0570 | lr 4.959736e-08\n",
      "step 49800 | train_loss 1.9644 | val_loss 2.0570 | lr 1.239983e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:01:11,896] Trial 0 finished with value: 2.056985463414873 and parameters: {'embedding_size': 47, 'hidden_size': 738, 'learning_rate': 0.0003140954960818081, 'batch_size': 128, 'context_length': 3}. Best is trial 0 with value: 2.056985463414873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.9644 | val_loss 2.0570 | lr 3.099998e-13\n",
      "step 0 | train_loss 3.3026 | val_loss 3.3027 | lr 1.288762e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\50196\\AppData\\Local\\Temp\\ipykernel_46372\\779706708.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200 | train_loss 2.6088 | val_loss 2.6082 | lr 1.288711e-04\n",
      "step 400 | train_loss 2.5009 | val_loss 2.5000 | lr 1.288559e-04\n",
      "step 600 | train_loss 2.4483 | val_loss 2.4552 | lr 1.288304e-04\n",
      "step 800 | train_loss 2.4207 | val_loss 2.4291 | lr 1.287948e-04\n",
      "step 1000 | train_loss 2.4082 | val_loss 2.4127 | lr 1.287490e-04\n",
      "step 1200 | train_loss 2.3910 | val_loss 2.4007 | lr 1.286931e-04\n",
      "step 1400 | train_loss 2.3781 | val_loss 2.3891 | lr 1.286271e-04\n",
      "step 1600 | train_loss 2.3751 | val_loss 2.3806 | lr 1.285509e-04\n",
      "step 1800 | train_loss 2.3689 | val_loss 2.3719 | lr 1.284645e-04\n",
      "step 2000 | train_loss 2.3611 | val_loss 2.3642 | lr 1.283681e-04\n",
      "step 2200 | train_loss 2.3590 | val_loss 2.3601 | lr 1.282616e-04\n",
      "step 2400 | train_loss 2.3554 | val_loss 2.3543 | lr 1.281449e-04\n",
      "step 2600 | train_loss 2.3486 | val_loss 2.3522 | lr 1.280183e-04\n",
      "step 2800 | train_loss 2.3492 | val_loss 2.3452 | lr 1.278816e-04\n",
      "step 3000 | train_loss 2.3451 | val_loss 2.3402 | lr 1.277348e-04\n",
      "step 3200 | train_loss 2.3451 | val_loss 2.3355 | lr 1.275781e-04\n",
      "step 3400 | train_loss 2.3169 | val_loss 2.3345 | lr 1.274114e-04\n",
      "step 3600 | train_loss 2.3273 | val_loss 2.3304 | lr 1.272348e-04\n",
      "step 3800 | train_loss 2.3273 | val_loss 2.3268 | lr 1.270482e-04\n",
      "step 4000 | train_loss 2.3208 | val_loss 2.3236 | lr 1.268518e-04\n",
      "step 4200 | train_loss 2.3236 | val_loss 2.3235 | lr 1.266455e-04\n",
      "step 4400 | train_loss 2.3230 | val_loss 2.3197 | lr 1.264293e-04\n",
      "step 4600 | train_loss 2.3174 | val_loss 2.3184 | lr 1.262034e-04\n",
      "step 4800 | train_loss 2.3143 | val_loss 2.3149 | lr 1.259678e-04\n",
      "step 5000 | train_loss 2.3180 | val_loss 2.3148 | lr 1.257224e-04\n",
      "step 5200 | train_loss 2.3220 | val_loss 2.3108 | lr 1.254673e-04\n",
      "step 5400 | train_loss 2.3187 | val_loss 2.3071 | lr 1.252026e-04\n",
      "step 5600 | train_loss 2.3151 | val_loss 2.3081 | lr 1.249283e-04\n",
      "step 5800 | train_loss 2.3143 | val_loss 2.3065 | lr 1.246445e-04\n",
      "step 6000 | train_loss 2.3150 | val_loss 2.3060 | lr 1.243511e-04\n",
      "step 6200 | train_loss 2.3116 | val_loss 2.3032 | lr 1.240483e-04\n",
      "step 6400 | train_loss 2.3127 | val_loss 2.3003 | lr 1.237361e-04\n",
      "step 6600 | train_loss 2.3152 | val_loss 2.3000 | lr 1.234145e-04\n",
      "step 6800 | train_loss 2.2873 | val_loss 2.2989 | lr 1.230836e-04\n",
      "step 7000 | train_loss 2.3018 | val_loss 2.2963 | lr 1.227434e-04\n",
      "step 7200 | train_loss 2.2992 | val_loss 2.2948 | lr 1.223941e-04\n",
      "step 7400 | train_loss 2.2947 | val_loss 2.2943 | lr 1.220355e-04\n",
      "step 7600 | train_loss 2.2998 | val_loss 2.2945 | lr 1.216679e-04\n",
      "step 7800 | train_loss 2.2972 | val_loss 2.2923 | lr 1.212913e-04\n",
      "step 8000 | train_loss 2.3001 | val_loss 2.2933 | lr 1.209056e-04\n",
      "step 8200 | train_loss 2.2962 | val_loss 2.2898 | lr 1.205111e-04\n",
      "step 8400 | train_loss 2.2989 | val_loss 2.2886 | lr 1.201077e-04\n",
      "step 8600 | train_loss 2.2956 | val_loss 2.2882 | lr 1.196955e-04\n",
      "step 8800 | train_loss 2.2988 | val_loss 2.2851 | lr 1.192746e-04\n",
      "step 9000 | train_loss 2.2963 | val_loss 2.2862 | lr 1.188450e-04\n",
      "step 9200 | train_loss 2.2947 | val_loss 2.2874 | lr 1.184068e-04\n",
      "step 9400 | train_loss 2.2958 | val_loss 2.2849 | lr 1.179601e-04\n",
      "step 9600 | train_loss 2.2994 | val_loss 2.2868 | lr 1.175050e-04\n",
      "step 9800 | train_loss 2.3031 | val_loss 2.2832 | lr 1.170415e-04\n",
      "step 10000 | train_loss 2.2974 | val_loss 2.2811 | lr 1.165696e-04\n",
      "step 10200 | train_loss 2.2800 | val_loss 2.2816 | lr 1.160896e-04\n",
      "step 10400 | train_loss 2.2865 | val_loss 2.2826 | lr 1.156013e-04\n",
      "step 10600 | train_loss 2.2815 | val_loss 2.2789 | lr 1.151050e-04\n",
      "step 10800 | train_loss 2.2803 | val_loss 2.2788 | lr 1.146007e-04\n",
      "step 11000 | train_loss 2.2850 | val_loss 2.2776 | lr 1.140885e-04\n",
      "step 11200 | train_loss 2.2817 | val_loss 2.2786 | lr 1.135685e-04\n",
      "step 11400 | train_loss 2.2779 | val_loss 2.2775 | lr 1.130406e-04\n",
      "step 11600 | train_loss 2.2859 | val_loss 2.2765 | lr 1.125051e-04\n",
      "step 11800 | train_loss 2.2857 | val_loss 2.2753 | lr 1.119620e-04\n",
      "step 12000 | train_loss 2.2839 | val_loss 2.2744 | lr 1.114115e-04\n",
      "step 12200 | train_loss 2.2861 | val_loss 2.2744 | lr 1.108534e-04\n",
      "step 12400 | train_loss 2.2830 | val_loss 2.2747 | lr 1.102881e-04\n",
      "step 12600 | train_loss 2.2842 | val_loss 2.2768 | lr 1.097155e-04\n",
      "step 12800 | train_loss 2.2877 | val_loss 2.2744 | lr 1.091358e-04\n",
      "step 13000 | train_loss 2.2872 | val_loss 2.2730 | lr 1.085490e-04\n",
      "step 13200 | train_loss 2.2891 | val_loss 2.2723 | lr 1.079553e-04\n",
      "step 13400 | train_loss 2.2619 | val_loss 2.2716 | lr 1.073546e-04\n",
      "step 13600 | train_loss 2.2739 | val_loss 2.2716 | lr 1.067472e-04\n",
      "step 13800 | train_loss 2.2756 | val_loss 2.2712 | lr 1.061332e-04\n",
      "step 14000 | train_loss 2.2714 | val_loss 2.2707 | lr 1.055125e-04\n",
      "step 14200 | train_loss 2.2755 | val_loss 2.2709 | lr 1.048853e-04\n",
      "step 14400 | train_loss 2.2781 | val_loss 2.2693 | lr 1.042518e-04\n",
      "step 14600 | train_loss 2.2757 | val_loss 2.2702 | lr 1.036120e-04\n",
      "step 14800 | train_loss 2.2725 | val_loss 2.2682 | lr 1.029660e-04\n",
      "step 15000 | train_loss 2.2747 | val_loss 2.2694 | lr 1.023139e-04\n",
      "step 15200 | train_loss 2.2788 | val_loss 2.2665 | lr 1.016558e-04\n",
      "step 15400 | train_loss 2.2767 | val_loss 2.2664 | lr 1.009918e-04\n",
      "step 15600 | train_loss 2.2751 | val_loss 2.2690 | lr 1.003221e-04\n",
      "step 15800 | train_loss 2.2766 | val_loss 2.2670 | lr 9.964672e-05\n",
      "step 16000 | train_loss 2.2778 | val_loss 2.2683 | lr 9.896576e-05\n",
      "step 16200 | train_loss 2.2769 | val_loss 2.2668 | lr 9.827936e-05\n",
      "step 16400 | train_loss 2.2770 | val_loss 2.2654 | lr 9.758761e-05\n",
      "step 16600 | train_loss 2.2856 | val_loss 2.2677 | lr 9.689063e-05\n",
      "step 16800 | train_loss 2.2569 | val_loss 2.2659 | lr 9.618852e-05\n",
      "step 17000 | train_loss 2.2692 | val_loss 2.2650 | lr 9.548139e-05\n",
      "step 17200 | train_loss 2.2672 | val_loss 2.2628 | lr 9.476937e-05\n",
      "step 17400 | train_loss 2.2644 | val_loss 2.2643 | lr 9.405256e-05\n",
      "step 17600 | train_loss 2.2702 | val_loss 2.2654 | lr 9.333107e-05\n",
      "step 17800 | train_loss 2.2707 | val_loss 2.2646 | lr 9.260501e-05\n",
      "step 18000 | train_loss 2.2702 | val_loss 2.2635 | lr 9.187451e-05\n",
      "step 18200 | train_loss 2.2659 | val_loss 2.2624 | lr 9.113968e-05\n",
      "step 18400 | train_loss 2.2708 | val_loss 2.2623 | lr 9.040063e-05\n",
      "step 18600 | train_loss 2.2722 | val_loss 2.2621 | lr 8.965748e-05\n",
      "step 18800 | train_loss 2.2711 | val_loss 2.2604 | lr 8.891035e-05\n",
      "step 19000 | train_loss 2.2714 | val_loss 2.2613 | lr 8.815935e-05\n",
      "step 19200 | train_loss 2.2692 | val_loss 2.2622 | lr 8.740461e-05\n",
      "step 19400 | train_loss 2.2728 | val_loss 2.2618 | lr 8.664624e-05\n",
      "step 19600 | train_loss 2.2730 | val_loss 2.2630 | lr 8.588436e-05\n",
      "step 19800 | train_loss 2.2770 | val_loss 2.2608 | lr 8.511910e-05\n",
      "step 20000 | train_loss 2.2748 | val_loss 2.2594 | lr 8.435057e-05\n",
      "step 20200 | train_loss 2.2563 | val_loss 2.2603 | lr 8.357890e-05\n",
      "step 20400 | train_loss 2.2637 | val_loss 2.2596 | lr 8.280420e-05\n",
      "step 20600 | train_loss 2.2622 | val_loss 2.2592 | lr 8.202661e-05\n",
      "step 20800 | train_loss 2.2599 | val_loss 2.2588 | lr 8.124623e-05\n",
      "step 21000 | train_loss 2.2638 | val_loss 2.2591 | lr 8.046321e-05\n",
      "step 21200 | train_loss 2.2644 | val_loss 2.2590 | lr 7.967765e-05\n",
      "step 21400 | train_loss 2.2636 | val_loss 2.2598 | lr 7.888968e-05\n",
      "step 21600 | train_loss 2.2638 | val_loss 2.2576 | lr 7.809944e-05\n",
      "step 21800 | train_loss 2.2661 | val_loss 2.2570 | lr 7.730703e-05\n",
      "step 22000 | train_loss 2.2637 | val_loss 2.2570 | lr 7.651260e-05\n",
      "step 22200 | train_loss 2.2672 | val_loss 2.2559 | lr 7.571626e-05\n",
      "step 22400 | train_loss 2.2630 | val_loss 2.2574 | lr 7.491813e-05\n",
      "step 22600 | train_loss 2.2658 | val_loss 2.2585 | lr 7.411835e-05\n",
      "step 22800 | train_loss 2.2676 | val_loss 2.2578 | lr 7.331705e-05\n",
      "step 23000 | train_loss 2.2715 | val_loss 2.2583 | lr 7.251434e-05\n",
      "step 23200 | train_loss 2.2733 | val_loss 2.2575 | lr 7.171035e-05\n",
      "step 23400 | train_loss 2.2603 | val_loss 2.2557 | lr 7.090522e-05\n",
      "step 23600 | train_loss 2.2575 | val_loss 2.2562 | lr 7.009907e-05\n",
      "step 23800 | train_loss 2.2599 | val_loss 2.2560 | lr 6.929202e-05\n",
      "step 24000 | train_loss 2.2564 | val_loss 2.2559 | lr 6.848420e-05\n",
      "step 24200 | train_loss 2.2583 | val_loss 2.2550 | lr 6.767575e-05\n",
      "step 24400 | train_loss 2.2632 | val_loss 2.2555 | lr 6.686679e-05\n",
      "step 24600 | train_loss 2.2604 | val_loss 2.2566 | lr 6.605744e-05\n",
      "step 24800 | train_loss 2.2580 | val_loss 2.2548 | lr 6.524783e-05\n",
      "step 25000 | train_loss 2.2592 | val_loss 2.2546 | lr 6.443810e-05\n",
      "step 25200 | train_loss 2.2613 | val_loss 2.2538 | lr 6.362837e-05\n",
      "step 25400 | train_loss 2.2606 | val_loss 2.2536 | lr 6.281877e-05\n",
      "step 25600 | train_loss 2.2613 | val_loss 2.2546 | lr 6.200942e-05\n",
      "step 25800 | train_loss 2.2621 | val_loss 2.2546 | lr 6.120045e-05\n",
      "step 26000 | train_loss 2.2611 | val_loss 2.2554 | lr 6.039200e-05\n",
      "step 26200 | train_loss 2.2637 | val_loss 2.2546 | lr 5.958419e-05\n",
      "step 26400 | train_loss 2.2644 | val_loss 2.2542 | lr 5.877714e-05\n",
      "step 26600 | train_loss 2.2675 | val_loss 2.2536 | lr 5.797098e-05\n",
      "step 26800 | train_loss 2.2499 | val_loss 2.2536 | lr 5.716585e-05\n",
      "step 27000 | train_loss 2.2558 | val_loss 2.2534 | lr 5.636187e-05\n",
      "step 27200 | train_loss 2.2564 | val_loss 2.2524 | lr 5.555916e-05\n",
      "step 27400 | train_loss 2.2547 | val_loss 2.2528 | lr 5.475785e-05\n",
      "step 27600 | train_loss 2.2563 | val_loss 2.2535 | lr 5.395807e-05\n",
      "step 27800 | train_loss 2.2582 | val_loss 2.2530 | lr 5.315995e-05\n",
      "step 28000 | train_loss 2.2571 | val_loss 2.2531 | lr 5.236361e-05\n",
      "step 28200 | train_loss 2.2555 | val_loss 2.2521 | lr 5.156917e-05\n",
      "step 28400 | train_loss 2.2578 | val_loss 2.2522 | lr 5.077677e-05\n",
      "step 28600 | train_loss 2.2594 | val_loss 2.2510 | lr 4.998652e-05\n",
      "step 28800 | train_loss 2.2598 | val_loss 2.2509 | lr 4.919856e-05\n",
      "step 29000 | train_loss 2.2585 | val_loss 2.2514 | lr 4.841300e-05\n",
      "step 29200 | train_loss 2.2583 | val_loss 2.2519 | lr 4.762997e-05\n",
      "step 29400 | train_loss 2.2596 | val_loss 2.2528 | lr 4.684960e-05\n",
      "step 29600 | train_loss 2.2594 | val_loss 2.2526 | lr 4.607200e-05\n",
      "step 29800 | train_loss 2.2618 | val_loss 2.2517 | lr 4.529731e-05\n",
      "step 30000 | train_loss 2.2634 | val_loss 2.2516 | lr 4.452563e-05\n",
      "step 30200 | train_loss 2.2493 | val_loss 2.2514 | lr 4.375710e-05\n",
      "step 30400 | train_loss 2.2545 | val_loss 2.2505 | lr 4.299184e-05\n",
      "step 30600 | train_loss 2.2544 | val_loss 2.2505 | lr 4.222997e-05\n",
      "step 30800 | train_loss 2.2523 | val_loss 2.2508 | lr 4.147160e-05\n",
      "step 31000 | train_loss 2.2541 | val_loss 2.2509 | lr 4.071685e-05\n",
      "step 31200 | train_loss 2.2557 | val_loss 2.2512 | lr 3.996586e-05\n",
      "step 31400 | train_loss 2.2565 | val_loss 2.2517 | lr 3.921873e-05\n",
      "step 31600 | train_loss 2.2543 | val_loss 2.2501 | lr 3.847558e-05\n",
      "step 31800 | train_loss 2.2557 | val_loss 2.2496 | lr 3.773653e-05\n",
      "step 32000 | train_loss 2.2555 | val_loss 2.2495 | lr 3.700169e-05\n",
      "step 32200 | train_loss 2.2577 | val_loss 2.2489 | lr 3.627119e-05\n",
      "step 32400 | train_loss 2.2559 | val_loss 2.2499 | lr 3.554514e-05\n",
      "step 32600 | train_loss 2.2556 | val_loss 2.2501 | lr 3.482365e-05\n",
      "step 32800 | train_loss 2.2566 | val_loss 2.2502 | lr 3.410683e-05\n",
      "step 33000 | train_loss 2.2593 | val_loss 2.2510 | lr 3.339481e-05\n",
      "step 33200 | train_loss 2.2609 | val_loss 2.2501 | lr 3.268769e-05\n",
      "step 33400 | train_loss 2.2603 | val_loss 2.2494 | lr 3.198558e-05\n",
      "step 33600 | train_loss 2.2513 | val_loss 2.2493 | lr 3.128859e-05\n",
      "step 33800 | train_loss 2.2535 | val_loss 2.2496 | lr 3.059685e-05\n",
      "step 34000 | train_loss 2.2525 | val_loss 2.2492 | lr 2.991044e-05\n",
      "step 34200 | train_loss 2.2515 | val_loss 2.2490 | lr 2.922949e-05\n",
      "step 34400 | train_loss 2.2534 | val_loss 2.2491 | lr 2.855409e-05\n",
      "step 34600 | train_loss 2.2530 | val_loss 2.2494 | lr 2.788437e-05\n",
      "step 34800 | train_loss 2.2527 | val_loss 2.2494 | lr 2.722041e-05\n",
      "step 35000 | train_loss 2.2531 | val_loss 2.2487 | lr 2.656234e-05\n",
      "step 35200 | train_loss 2.2533 | val_loss 2.2484 | lr 2.591024e-05\n",
      "step 35400 | train_loss 2.2541 | val_loss 2.2481 | lr 2.526423e-05\n",
      "step 35600 | train_loss 2.2545 | val_loss 2.2480 | lr 2.462440e-05\n",
      "step 35800 | train_loss 2.2541 | val_loss 2.2485 | lr 2.399086e-05\n",
      "step 36000 | train_loss 2.2535 | val_loss 2.2489 | lr 2.336371e-05\n",
      "step 36200 | train_loss 2.2550 | val_loss 2.2489 | lr 2.274304e-05\n",
      "step 36400 | train_loss 2.2562 | val_loss 2.2490 | lr 2.212896e-05\n",
      "step 36600 | train_loss 2.2575 | val_loss 2.2485 | lr 2.152156e-05\n",
      "step 36800 | train_loss 2.2513 | val_loss 2.2485 | lr 2.092094e-05\n",
      "step 37000 | train_loss 2.2517 | val_loss 2.2484 | lr 2.032719e-05\n",
      "step 37200 | train_loss 2.2526 | val_loss 2.2483 | lr 1.974040e-05\n",
      "step 37400 | train_loss 2.2520 | val_loss 2.2483 | lr 1.916067e-05\n",
      "step 37600 | train_loss 2.2516 | val_loss 2.2482 | lr 1.858809e-05\n",
      "step 37800 | train_loss 2.2528 | val_loss 2.2482 | lr 1.802276e-05\n",
      "step 38000 | train_loss 2.2522 | val_loss 2.2483 | lr 1.746475e-05\n",
      "step 38200 | train_loss 2.2517 | val_loss 2.2481 | lr 1.691416e-05\n",
      "step 38400 | train_loss 2.2517 | val_loss 2.2480 | lr 1.637107e-05\n",
      "step 38600 | train_loss 2.2527 | val_loss 2.2471 | lr 1.583557e-05\n",
      "step 38800 | train_loss 2.2531 | val_loss 2.2473 | lr 1.530775e-05\n",
      "step 39000 | train_loss 2.2529 | val_loss 2.2476 | lr 1.478769e-05\n",
      "step 39200 | train_loss 2.2529 | val_loss 2.2475 | lr 1.427547e-05\n",
      "step 39400 | train_loss 2.2531 | val_loss 2.2480 | lr 1.377117e-05\n",
      "step 39600 | train_loss 2.2533 | val_loss 2.2479 | lr 1.327487e-05\n",
      "step 39800 | train_loss 2.2541 | val_loss 2.2478 | lr 1.278665e-05\n",
      "step 40000 | train_loss 2.2559 | val_loss 2.2477 | lr 1.230658e-05\n",
      "step 40200 | train_loss 2.2512 | val_loss 2.2477 | lr 1.183475e-05\n",
      "step 40400 | train_loss 2.2520 | val_loss 2.2475 | lr 1.137122e-05\n",
      "step 40600 | train_loss 2.2523 | val_loss 2.2475 | lr 1.091608e-05\n",
      "step 40800 | train_loss 2.2517 | val_loss 2.2477 | lr 1.046938e-05\n",
      "step 41000 | train_loss 2.2515 | val_loss 2.2475 | lr 1.003121e-05\n",
      "step 41200 | train_loss 2.2520 | val_loss 2.2475 | lr 9.601633e-06\n",
      "step 41400 | train_loss 2.2524 | val_loss 2.2477 | lr 9.180712e-06\n",
      "step 41600 | train_loss 2.2517 | val_loss 2.2474 | lr 8.768517e-06\n",
      "step 41800 | train_loss 2.2517 | val_loss 2.2473 | lr 8.365112e-06\n",
      "step 42000 | train_loss 2.2517 | val_loss 2.2470 | lr 7.970563e-06\n",
      "step 42200 | train_loss 2.2522 | val_loss 2.2470 | lr 7.584930e-06\n",
      "step 42400 | train_loss 2.2522 | val_loss 2.2471 | lr 7.208275e-06\n",
      "step 42600 | train_loss 2.2519 | val_loss 2.2471 | lr 6.840657e-06\n",
      "step 42800 | train_loss 2.2522 | val_loss 2.2473 | lr 6.482135e-06\n",
      "step 43000 | train_loss 2.2525 | val_loss 2.2474 | lr 6.132764e-06\n",
      "step 43200 | train_loss 2.2531 | val_loss 2.2472 | lr 5.792601e-06\n",
      "step 43400 | train_loss 2.2536 | val_loss 2.2472 | lr 5.461698e-06\n",
      "step 43600 | train_loss 2.2516 | val_loss 2.2472 | lr 5.140108e-06\n",
      "step 43800 | train_loss 2.2520 | val_loss 2.2472 | lr 4.827883e-06\n",
      "step 44000 | train_loss 2.2520 | val_loss 2.2472 | lr 4.525070e-06\n",
      "step 44200 | train_loss 2.2517 | val_loss 2.2472 | lr 4.231718e-06\n",
      "step 44400 | train_loss 2.2518 | val_loss 2.2472 | lr 3.947874e-06\n",
      "step 44600 | train_loss 2.2519 | val_loss 2.2472 | lr 3.673582e-06\n",
      "step 44800 | train_loss 2.2519 | val_loss 2.2473 | lr 3.408885e-06\n",
      "step 45000 | train_loss 2.2518 | val_loss 2.2471 | lr 3.153825e-06\n",
      "step 45200 | train_loss 2.2517 | val_loss 2.2470 | lr 2.908443e-06\n",
      "step 45400 | train_loss 2.2517 | val_loss 2.2470 | lr 2.672777e-06\n",
      "step 45600 | train_loss 2.2519 | val_loss 2.2469 | lr 2.446865e-06\n",
      "step 45800 | train_loss 2.2518 | val_loss 2.2470 | lr 2.230741e-06\n",
      "step 46000 | train_loss 2.2518 | val_loss 2.2470 | lr 2.024441e-06\n",
      "step 46200 | train_loss 2.2519 | val_loss 2.2471 | lr 1.827997e-06\n",
      "step 46400 | train_loss 2.2520 | val_loss 2.2471 | lr 1.641440e-06\n",
      "step 46600 | train_loss 2.2521 | val_loss 2.2470 | lr 1.464799e-06\n",
      "step 46800 | train_loss 2.2519 | val_loss 2.2470 | lr 1.298102e-06\n",
      "step 47000 | train_loss 2.2518 | val_loss 2.2470 | lr 1.141376e-06\n",
      "step 47200 | train_loss 2.2518 | val_loss 2.2470 | lr 9.946450e-07\n",
      "step 47400 | train_loss 2.2518 | val_loss 2.2470 | lr 8.579325e-07\n",
      "step 47600 | train_loss 2.2518 | val_loss 2.2470 | lr 7.312600e-07\n",
      "step 47800 | train_loss 2.2518 | val_loss 2.2470 | lr 6.146476e-07\n",
      "step 48000 | train_loss 2.2518 | val_loss 2.2470 | lr 5.081137e-07\n",
      "step 48200 | train_loss 2.2518 | val_loss 2.2470 | lr 4.116750e-07\n",
      "step 48400 | train_loss 2.2518 | val_loss 2.2470 | lr 3.253469e-07\n",
      "step 48600 | train_loss 2.2518 | val_loss 2.2470 | lr 2.491429e-07\n",
      "step 48800 | train_loss 2.2518 | val_loss 2.2470 | lr 1.830751e-07\n",
      "step 49000 | train_loss 2.2518 | val_loss 2.2470 | lr 1.271539e-07\n",
      "step 49200 | train_loss 2.2518 | val_loss 2.2470 | lr 8.138812e-08\n",
      "step 49400 | train_loss 2.2518 | val_loss 2.2470 | lr 4.578503e-08\n",
      "step 49600 | train_loss 2.2518 | val_loss 2.2470 | lr 2.035024e-08\n",
      "step 49800 | train_loss 2.2518 | val_loss 2.2470 | lr 5.087762e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:02:10,859] Trial 1 finished with value: 2.247021347284317 and parameters: {'embedding_size': 37, 'hidden_size': 913, 'learning_rate': 0.00012887620370903477, 'batch_size': 64, 'context_length': 2}. Best is trial 0 with value: 2.056985463414873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.2518 | val_loss 2.2470 | lr 1.271957e-13\n",
      "step 0 | train_loss 3.3209 | val_loss 3.3157 | lr 5.838780e-03\n",
      "step 200 | train_loss 2.3884 | val_loss 2.3656 | lr 5.838549e-03\n",
      "step 400 | train_loss 2.2926 | val_loss 2.2995 | lr 5.837858e-03\n",
      "step 600 | train_loss 2.2929 | val_loss 2.2892 | lr 5.836705e-03\n",
      "step 800 | train_loss 2.2926 | val_loss 2.2532 | lr 5.835092e-03\n",
      "step 1000 | train_loss 2.2866 | val_loss 2.2650 | lr 5.833019e-03\n",
      "step 1200 | train_loss 2.2516 | val_loss 2.2607 | lr 5.830485e-03\n",
      "step 1400 | train_loss 2.2764 | val_loss 2.2481 | lr 5.827492e-03\n",
      "step 1600 | train_loss 2.2445 | val_loss 2.2244 | lr 5.824040e-03\n",
      "step 1800 | train_loss 2.2439 | val_loss 2.2298 | lr 5.820129e-03\n",
      "step 2000 | train_loss 2.2449 | val_loss 2.2285 | lr 5.815759e-03\n",
      "step 2200 | train_loss 2.2232 | val_loss 2.2228 | lr 5.810933e-03\n",
      "step 2400 | train_loss 2.2354 | val_loss 2.2142 | lr 5.805650e-03\n",
      "step 2600 | train_loss 2.2188 | val_loss 2.2134 | lr 5.799911e-03\n",
      "step 2800 | train_loss 2.2072 | val_loss 2.2113 | lr 5.793717e-03\n",
      "step 3000 | train_loss 2.2330 | val_loss 2.1990 | lr 5.787069e-03\n",
      "step 3200 | train_loss 2.2517 | val_loss 2.2073 | lr 5.779969e-03\n",
      "step 3400 | train_loss 1.9309 | val_loss 2.1973 | lr 5.772416e-03\n",
      "step 3600 | train_loss 2.1261 | val_loss 2.1920 | lr 5.764414e-03\n",
      "step 3800 | train_loss 2.1378 | val_loss 2.1808 | lr 5.755962e-03\n",
      "step 4000 | train_loss 2.1547 | val_loss 2.1777 | lr 5.747062e-03\n",
      "step 4200 | train_loss 2.1829 | val_loss 2.2063 | lr 5.737715e-03\n",
      "step 4400 | train_loss 2.1652 | val_loss 2.1864 | lr 5.727924e-03\n",
      "step 4600 | train_loss 2.1796 | val_loss 2.1983 | lr 5.717689e-03\n",
      "step 4800 | train_loss 2.1824 | val_loss 2.1897 | lr 5.707012e-03\n",
      "step 5000 | train_loss 2.1927 | val_loss 2.1905 | lr 5.695895e-03\n",
      "step 5200 | train_loss 2.1847 | val_loss 2.1819 | lr 5.684339e-03\n",
      "step 5400 | train_loss 2.1934 | val_loss 2.1719 | lr 5.672347e-03\n",
      "step 5600 | train_loss 2.2055 | val_loss 2.1785 | lr 5.659920e-03\n",
      "step 5800 | train_loss 2.1928 | val_loss 2.1816 | lr 5.647060e-03\n",
      "step 6000 | train_loss 2.1999 | val_loss 2.1934 | lr 5.633770e-03\n",
      "step 6200 | train_loss 2.1995 | val_loss 2.1667 | lr 5.620051e-03\n",
      "step 6400 | train_loss 2.2013 | val_loss 2.1729 | lr 5.605905e-03\n",
      "step 6600 | train_loss 2.1700 | val_loss 2.1617 | lr 5.591336e-03\n",
      "step 6800 | train_loss 1.9663 | val_loss 2.1737 | lr 5.576344e-03\n",
      "step 7000 | train_loss 2.0850 | val_loss 2.1453 | lr 5.560933e-03\n",
      "step 7200 | train_loss 2.1366 | val_loss 2.1643 | lr 5.545104e-03\n",
      "step 7400 | train_loss 2.1480 | val_loss 2.1620 | lr 5.528861e-03\n",
      "step 7600 | train_loss 2.1509 | val_loss 2.1526 | lr 5.512206e-03\n",
      "step 7800 | train_loss 2.1565 | val_loss 2.1604 | lr 5.495142e-03\n",
      "step 8000 | train_loss 2.1355 | val_loss 2.1578 | lr 5.477671e-03\n",
      "step 8200 | train_loss 2.1581 | val_loss 2.1509 | lr 5.459795e-03\n",
      "step 8400 | train_loss 2.1732 | val_loss 2.1438 | lr 5.441519e-03\n",
      "step 8600 | train_loss 2.1637 | val_loss 2.1533 | lr 5.422844e-03\n",
      "step 8800 | train_loss 2.1552 | val_loss 2.1407 | lr 5.403774e-03\n",
      "step 9000 | train_loss 2.1693 | val_loss 2.1475 | lr 5.384312e-03\n",
      "step 9200 | train_loss 2.1724 | val_loss 2.1501 | lr 5.364461e-03\n",
      "step 9400 | train_loss 2.1603 | val_loss 2.1583 | lr 5.344223e-03\n",
      "step 9600 | train_loss 2.1842 | val_loss 2.1561 | lr 5.323603e-03\n",
      "step 9800 | train_loss 2.1671 | val_loss 2.1392 | lr 5.302602e-03\n",
      "step 10000 | train_loss 2.1677 | val_loss 2.1463 | lr 5.281226e-03\n",
      "step 10200 | train_loss 2.0178 | val_loss 2.1487 | lr 5.259476e-03\n",
      "step 10400 | train_loss 2.0698 | val_loss 2.1422 | lr 5.237357e-03\n",
      "step 10600 | train_loss 2.0940 | val_loss 2.1379 | lr 5.214872e-03\n",
      "step 10800 | train_loss 2.1014 | val_loss 2.1340 | lr 5.192025e-03\n",
      "step 11000 | train_loss 2.1146 | val_loss 2.1297 | lr 5.168818e-03\n",
      "step 11200 | train_loss 2.1041 | val_loss 2.1403 | lr 5.145257e-03\n",
      "step 11400 | train_loss 2.1103 | val_loss 2.1370 | lr 5.121344e-03\n",
      "step 11600 | train_loss 2.1403 | val_loss 2.1481 | lr 5.097083e-03\n",
      "step 11800 | train_loss 2.1332 | val_loss 2.1503 | lr 5.072478e-03\n",
      "step 12000 | train_loss 2.1325 | val_loss 2.1342 | lr 5.047533e-03\n",
      "step 12200 | train_loss 2.1355 | val_loss 2.1411 | lr 5.022253e-03\n",
      "step 12400 | train_loss 2.1589 | val_loss 2.1463 | lr 4.996640e-03\n",
      "step 12600 | train_loss 2.1315 | val_loss 2.1430 | lr 4.970699e-03\n",
      "step 12800 | train_loss 2.1473 | val_loss 2.1303 | lr 4.944434e-03\n",
      "step 13000 | train_loss 2.1379 | val_loss 2.1410 | lr 4.917850e-03\n",
      "step 13200 | train_loss 2.1292 | val_loss 2.1278 | lr 4.890950e-03\n",
      "step 13400 | train_loss 1.8528 | val_loss 2.1253 | lr 4.863738e-03\n",
      "step 13600 | train_loss 2.0166 | val_loss 2.1240 | lr 4.836220e-03\n",
      "step 13800 | train_loss 2.0523 | val_loss 2.1230 | lr 4.808398e-03\n",
      "step 14000 | train_loss 2.0584 | val_loss 2.1108 | lr 4.780279e-03\n",
      "step 14200 | train_loss 2.0773 | val_loss 2.1195 | lr 4.751866e-03\n",
      "step 14400 | train_loss 2.1095 | val_loss 2.1205 | lr 4.723163e-03\n",
      "step 14600 | train_loss 2.0764 | val_loss 2.1175 | lr 4.694175e-03\n",
      "step 14800 | train_loss 2.0726 | val_loss 2.1102 | lr 4.664908e-03\n",
      "step 15000 | train_loss 2.1017 | val_loss 2.1164 | lr 4.635364e-03\n",
      "step 15200 | train_loss 2.1178 | val_loss 2.1212 | lr 4.605550e-03\n",
      "step 15400 | train_loss 2.1036 | val_loss 2.1184 | lr 4.575469e-03\n",
      "step 15600 | train_loss 2.0865 | val_loss 2.1293 | lr 4.545127e-03\n",
      "step 15800 | train_loss 2.0944 | val_loss 2.1134 | lr 4.514528e-03\n",
      "step 16000 | train_loss 2.1122 | val_loss 2.1249 | lr 4.483677e-03\n",
      "step 16200 | train_loss 2.0789 | val_loss 2.1174 | lr 4.452579e-03\n",
      "step 16400 | train_loss 2.0908 | val_loss 2.1100 | lr 4.421239e-03\n",
      "step 16600 | train_loss 2.1126 | val_loss 2.1070 | lr 4.389662e-03\n",
      "step 16800 | train_loss 1.9364 | val_loss 2.1186 | lr 4.357853e-03\n",
      "step 17000 | train_loss 2.0145 | val_loss 2.1131 | lr 4.325817e-03\n",
      "step 17200 | train_loss 2.0355 | val_loss 2.1065 | lr 4.293558e-03\n",
      "step 17400 | train_loss 2.0240 | val_loss 2.1110 | lr 4.261083e-03\n",
      "step 17600 | train_loss 2.0563 | val_loss 2.1132 | lr 4.228395e-03\n",
      "step 17800 | train_loss 2.0405 | val_loss 2.1002 | lr 4.195501e-03\n",
      "step 18000 | train_loss 2.0576 | val_loss 2.0970 | lr 4.162406e-03\n",
      "step 18200 | train_loss 2.0697 | val_loss 2.1040 | lr 4.129114e-03\n",
      "step 18400 | train_loss 2.0879 | val_loss 2.0955 | lr 4.095631e-03\n",
      "step 18600 | train_loss 2.0925 | val_loss 2.1016 | lr 4.061962e-03\n",
      "step 18800 | train_loss 2.1054 | val_loss 2.0940 | lr 4.028113e-03\n",
      "step 19000 | train_loss 2.0869 | val_loss 2.0926 | lr 3.994089e-03\n",
      "step 19200 | train_loss 2.0928 | val_loss 2.1031 | lr 3.959895e-03\n",
      "step 19400 | train_loss 2.0866 | val_loss 2.1033 | lr 3.925537e-03\n",
      "step 19600 | train_loss 2.0942 | val_loss 2.1127 | lr 3.891020e-03\n",
      "step 19800 | train_loss 2.0991 | val_loss 2.0948 | lr 3.856349e-03\n",
      "step 20000 | train_loss 2.0832 | val_loss 2.0858 | lr 3.821531e-03\n",
      "step 20200 | train_loss 1.9313 | val_loss 2.0977 | lr 3.786570e-03\n",
      "step 20400 | train_loss 1.9831 | val_loss 2.0866 | lr 3.751472e-03\n",
      "step 20600 | train_loss 2.0045 | val_loss 2.0884 | lr 3.716243e-03\n",
      "step 20800 | train_loss 2.0110 | val_loss 2.0811 | lr 3.680888e-03\n",
      "step 21000 | train_loss 2.0316 | val_loss 2.0905 | lr 3.645413e-03\n",
      "step 21200 | train_loss 2.0131 | val_loss 2.0790 | lr 3.609823e-03\n",
      "step 21400 | train_loss 2.0190 | val_loss 2.0851 | lr 3.574124e-03\n",
      "step 21600 | train_loss 2.0381 | val_loss 2.0802 | lr 3.538321e-03\n",
      "step 21800 | train_loss 2.0332 | val_loss 2.0841 | lr 3.502421e-03\n",
      "step 22000 | train_loss 2.0363 | val_loss 2.0830 | lr 3.466429e-03\n",
      "step 22200 | train_loss 2.0480 | val_loss 2.0852 | lr 3.430350e-03\n",
      "step 22400 | train_loss 2.0488 | val_loss 2.0885 | lr 3.394191e-03\n",
      "step 22600 | train_loss 2.0619 | val_loss 2.0918 | lr 3.357957e-03\n",
      "step 22800 | train_loss 2.0510 | val_loss 2.0908 | lr 3.321653e-03\n",
      "step 23000 | train_loss 2.0588 | val_loss 2.0890 | lr 3.285286e-03\n",
      "step 23200 | train_loss 2.0579 | val_loss 2.0793 | lr 3.248862e-03\n",
      "step 23400 | train_loss 1.9185 | val_loss 2.0738 | lr 3.212385e-03\n",
      "step 23600 | train_loss 1.9414 | val_loss 2.0825 | lr 3.175862e-03\n",
      "step 23800 | train_loss 1.9651 | val_loss 2.0738 | lr 3.139298e-03\n",
      "step 24000 | train_loss 1.9583 | val_loss 2.0690 | lr 3.102700e-03\n",
      "step 24200 | train_loss 1.9926 | val_loss 2.0684 | lr 3.066073e-03\n",
      "step 24400 | train_loss 2.0089 | val_loss 2.0688 | lr 3.029422e-03\n",
      "step 24600 | train_loss 2.0067 | val_loss 2.0679 | lr 2.992754e-03\n",
      "step 24800 | train_loss 2.0033 | val_loss 2.0645 | lr 2.956075e-03\n",
      "step 25000 | train_loss 2.0164 | val_loss 2.0736 | lr 2.919390e-03\n",
      "step 25200 | train_loss 2.0198 | val_loss 2.0675 | lr 2.882705e-03\n",
      "step 25400 | train_loss 2.0248 | val_loss 2.0727 | lr 2.846025e-03\n",
      "step 25600 | train_loss 2.0344 | val_loss 2.0805 | lr 2.809357e-03\n",
      "step 25800 | train_loss 2.0330 | val_loss 2.0746 | lr 2.772707e-03\n",
      "step 26000 | train_loss 2.0231 | val_loss 2.0741 | lr 2.736080e-03\n",
      "step 26200 | train_loss 2.0338 | val_loss 2.0698 | lr 2.699482e-03\n",
      "step 26400 | train_loss 2.0307 | val_loss 2.0705 | lr 2.662918e-03\n",
      "step 26600 | train_loss 2.0398 | val_loss 2.0768 | lr 2.626395e-03\n",
      "step 26800 | train_loss 1.8621 | val_loss 2.0686 | lr 2.589918e-03\n",
      "step 27000 | train_loss 1.9268 | val_loss 2.0601 | lr 2.553493e-03\n",
      "step 27200 | train_loss 1.9360 | val_loss 2.0554 | lr 2.517126e-03\n",
      "step 27400 | train_loss 1.9559 | val_loss 2.0629 | lr 2.480823e-03\n",
      "step 27600 | train_loss 1.9602 | val_loss 2.0566 | lr 2.444589e-03\n",
      "step 27800 | train_loss 1.9723 | val_loss 2.0498 | lr 2.408429e-03\n",
      "step 28000 | train_loss 1.9744 | val_loss 2.0511 | lr 2.372351e-03\n",
      "step 28200 | train_loss 1.9804 | val_loss 2.0575 | lr 2.336359e-03\n",
      "step 28400 | train_loss 1.9870 | val_loss 2.0599 | lr 2.300458e-03\n",
      "step 28600 | train_loss 1.9870 | val_loss 2.0592 | lr 2.264656e-03\n",
      "step 28800 | train_loss 1.9853 | val_loss 2.0543 | lr 2.228957e-03\n",
      "step 29000 | train_loss 1.9963 | val_loss 2.0623 | lr 2.193367e-03\n",
      "step 29200 | train_loss 1.9976 | val_loss 2.0599 | lr 2.157892e-03\n",
      "step 29400 | train_loss 2.0085 | val_loss 2.0607 | lr 2.122537e-03\n",
      "step 29600 | train_loss 1.9964 | val_loss 2.0597 | lr 2.087307e-03\n",
      "step 29800 | train_loss 2.0072 | val_loss 2.0573 | lr 2.052210e-03\n",
      "step 30000 | train_loss 1.9824 | val_loss 2.0508 | lr 2.017249e-03\n",
      "step 30200 | train_loss 1.8641 | val_loss 2.0468 | lr 1.982430e-03\n",
      "step 30400 | train_loss 1.9186 | val_loss 2.0459 | lr 1.947760e-03\n",
      "step 30600 | train_loss 1.9259 | val_loss 2.0443 | lr 1.913243e-03\n",
      "step 30800 | train_loss 1.9288 | val_loss 2.0360 | lr 1.878885e-03\n",
      "step 31000 | train_loss 1.9404 | val_loss 2.0367 | lr 1.844691e-03\n",
      "step 31200 | train_loss 1.9449 | val_loss 2.0372 | lr 1.810667e-03\n",
      "step 31400 | train_loss 1.9550 | val_loss 2.0367 | lr 1.776818e-03\n",
      "step 31600 | train_loss 1.9557 | val_loss 2.0383 | lr 1.743149e-03\n",
      "step 31800 | train_loss 1.9661 | val_loss 2.0376 | lr 1.709666e-03\n",
      "step 32000 | train_loss 1.9677 | val_loss 2.0419 | lr 1.676374e-03\n",
      "step 32200 | train_loss 1.9717 | val_loss 2.0374 | lr 1.643279e-03\n",
      "step 32400 | train_loss 1.9594 | val_loss 2.0385 | lr 1.610384e-03\n",
      "step 32600 | train_loss 1.9673 | val_loss 2.0459 | lr 1.577697e-03\n",
      "step 32800 | train_loss 1.9726 | val_loss 2.0397 | lr 1.545222e-03\n",
      "step 33000 | train_loss 1.9796 | val_loss 2.0455 | lr 1.512963e-03\n",
      "step 33200 | train_loss 1.9712 | val_loss 2.0419 | lr 1.480927e-03\n",
      "step 33400 | train_loss 1.9615 | val_loss 2.0392 | lr 1.449117e-03\n",
      "step 33600 | train_loss 1.8763 | val_loss 2.0382 | lr 1.417540e-03\n",
      "step 33800 | train_loss 1.8944 | val_loss 2.0363 | lr 1.386200e-03\n",
      "step 34000 | train_loss 1.9102 | val_loss 2.0362 | lr 1.355103e-03\n",
      "step 34200 | train_loss 1.9144 | val_loss 2.0331 | lr 1.324252e-03\n",
      "step 34400 | train_loss 1.9243 | val_loss 2.0309 | lr 1.293653e-03\n",
      "step 34600 | train_loss 1.9272 | val_loss 2.0294 | lr 1.263311e-03\n",
      "step 34800 | train_loss 1.9175 | val_loss 2.0289 | lr 1.233230e-03\n",
      "step 35000 | train_loss 1.9367 | val_loss 2.0323 | lr 1.203416e-03\n",
      "step 35200 | train_loss 1.9378 | val_loss 2.0341 | lr 1.173872e-03\n",
      "step 35400 | train_loss 1.9391 | val_loss 2.0323 | lr 1.144604e-03\n",
      "step 35600 | train_loss 1.9378 | val_loss 2.0372 | lr 1.115617e-03\n",
      "step 35800 | train_loss 1.9407 | val_loss 2.0347 | lr 1.086914e-03\n",
      "step 36000 | train_loss 1.9357 | val_loss 2.0356 | lr 1.058501e-03\n",
      "step 36200 | train_loss 1.9476 | val_loss 2.0295 | lr 1.030381e-03\n",
      "step 36400 | train_loss 1.9408 | val_loss 2.0340 | lr 1.002560e-03\n",
      "step 36600 | train_loss 1.9405 | val_loss 2.0301 | lr 9.750416e-04\n",
      "step 36800 | train_loss 1.8411 | val_loss 2.0295 | lr 9.478301e-04\n",
      "step 37000 | train_loss 1.8718 | val_loss 2.0294 | lr 9.209300e-04\n",
      "step 37200 | train_loss 1.8910 | val_loss 2.0281 | lr 8.943454e-04\n",
      "step 37400 | train_loss 1.8923 | val_loss 2.0264 | lr 8.680807e-04\n",
      "step 37600 | train_loss 1.9019 | val_loss 2.0288 | lr 8.421398e-04\n",
      "step 37800 | train_loss 1.9104 | val_loss 2.0234 | lr 8.165270e-04\n",
      "step 38000 | train_loss 1.9050 | val_loss 2.0230 | lr 7.912462e-04\n",
      "step 38200 | train_loss 1.9039 | val_loss 2.0224 | lr 7.663015e-04\n",
      "step 38400 | train_loss 1.9135 | val_loss 2.0227 | lr 7.416968e-04\n",
      "step 38600 | train_loss 1.9249 | val_loss 2.0232 | lr 7.174360e-04\n",
      "step 38800 | train_loss 1.9228 | val_loss 2.0226 | lr 6.935229e-04\n",
      "step 39000 | train_loss 1.9204 | val_loss 2.0288 | lr 6.699613e-04\n",
      "step 39200 | train_loss 1.9166 | val_loss 2.0253 | lr 6.467549e-04\n",
      "step 39400 | train_loss 1.9262 | val_loss 2.0280 | lr 6.239074e-04\n",
      "step 39600 | train_loss 1.9300 | val_loss 2.0255 | lr 6.014223e-04\n",
      "step 39800 | train_loss 1.9244 | val_loss 2.0246 | lr 5.793033e-04\n",
      "step 40000 | train_loss 1.9214 | val_loss 2.0250 | lr 5.575538e-04\n",
      "step 40200 | train_loss 1.8582 | val_loss 2.0230 | lr 5.361773e-04\n",
      "step 40400 | train_loss 1.8716 | val_loss 2.0221 | lr 5.151771e-04\n",
      "step 40600 | train_loss 1.8787 | val_loss 2.0245 | lr 4.945566e-04\n",
      "step 40800 | train_loss 1.8813 | val_loss 2.0238 | lr 4.743190e-04\n",
      "step 41000 | train_loss 1.8890 | val_loss 2.0229 | lr 4.544675e-04\n",
      "step 41200 | train_loss 1.8904 | val_loss 2.0198 | lr 4.350052e-04\n",
      "step 41400 | train_loss 1.8950 | val_loss 2.0183 | lr 4.159352e-04\n",
      "step 41600 | train_loss 1.8935 | val_loss 2.0184 | lr 3.972606e-04\n",
      "step 41800 | train_loss 1.9002 | val_loss 2.0166 | lr 3.789842e-04\n",
      "step 42000 | train_loss 1.9054 | val_loss 2.0181 | lr 3.611090e-04\n",
      "step 42200 | train_loss 1.9063 | val_loss 2.0171 | lr 3.436378e-04\n",
      "step 42400 | train_loss 1.9035 | val_loss 2.0187 | lr 3.265733e-04\n",
      "step 42600 | train_loss 1.9027 | val_loss 2.0196 | lr 3.099183e-04\n",
      "step 42800 | train_loss 1.9073 | val_loss 2.0202 | lr 2.936753e-04\n",
      "step 43000 | train_loss 1.9096 | val_loss 2.0202 | lr 2.778469e-04\n",
      "step 43200 | train_loss 1.9090 | val_loss 2.0182 | lr 2.624357e-04\n",
      "step 43400 | train_loss 1.9011 | val_loss 2.0183 | lr 2.474441e-04\n",
      "step 43600 | train_loss 1.8721 | val_loss 2.0173 | lr 2.328743e-04\n",
      "step 43800 | train_loss 1.8778 | val_loss 2.0185 | lr 2.187288e-04\n",
      "step 44000 | train_loss 1.8784 | val_loss 2.0175 | lr 2.050098e-04\n",
      "step 44200 | train_loss 1.8790 | val_loss 2.0176 | lr 1.917194e-04\n",
      "step 44400 | train_loss 1.8822 | val_loss 2.0164 | lr 1.788598e-04\n",
      "step 44600 | train_loss 1.8815 | val_loss 2.0163 | lr 1.664328e-04\n",
      "step 44800 | train_loss 1.8835 | val_loss 2.0153 | lr 1.544407e-04\n",
      "step 45000 | train_loss 1.8846 | val_loss 2.0144 | lr 1.428851e-04\n",
      "step 45200 | train_loss 1.8872 | val_loss 2.0131 | lr 1.317680e-04\n",
      "step 45400 | train_loss 1.8886 | val_loss 2.0137 | lr 1.210911e-04\n",
      "step 45600 | train_loss 1.8902 | val_loss 2.0135 | lr 1.108560e-04\n",
      "step 45800 | train_loss 1.8880 | val_loss 2.0145 | lr 1.010645e-04\n",
      "step 46000 | train_loss 1.8881 | val_loss 2.0147 | lr 9.171800e-05\n",
      "step 46200 | train_loss 1.8896 | val_loss 2.0150 | lr 8.281803e-05\n",
      "step 46400 | train_loss 1.8913 | val_loss 2.0148 | lr 7.436599e-05\n",
      "step 46600 | train_loss 1.8913 | val_loss 2.0146 | lr 6.636321e-05\n",
      "step 46800 | train_loss 1.8833 | val_loss 2.0144 | lr 5.881096e-05\n",
      "step 47000 | train_loss 1.8818 | val_loss 2.0142 | lr 5.171042e-05\n",
      "step 47200 | train_loss 1.8820 | val_loss 2.0144 | lr 4.506272e-05\n",
      "step 47400 | train_loss 1.8823 | val_loss 2.0145 | lr 3.886892e-05\n",
      "step 47600 | train_loss 1.8819 | val_loss 2.0145 | lr 3.312998e-05\n",
      "step 47800 | train_loss 1.8822 | val_loss 2.0143 | lr 2.784682e-05\n",
      "step 48000 | train_loss 1.8823 | val_loss 2.0144 | lr 2.302026e-05\n",
      "step 48200 | train_loss 1.8821 | val_loss 2.0143 | lr 1.865108e-05\n",
      "step 48400 | train_loss 1.8821 | val_loss 2.0141 | lr 1.473995e-05\n",
      "step 48600 | train_loss 1.8823 | val_loss 2.0139 | lr 1.128750e-05\n",
      "step 48800 | train_loss 1.8826 | val_loss 2.0138 | lr 8.294278e-06\n",
      "step 49000 | train_loss 1.8827 | val_loss 2.0138 | lr 5.760749e-06\n",
      "step 49200 | train_loss 1.8827 | val_loss 2.0138 | lr 3.687316e-06\n",
      "step 49400 | train_loss 1.8827 | val_loss 2.0138 | lr 2.074306e-06\n",
      "step 49600 | train_loss 1.8828 | val_loss 2.0138 | lr 9.219746e-07\n",
      "step 49800 | train_loss 1.8828 | val_loss 2.0138 | lr 2.305027e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:03:15,179] Trial 2 finished with value: 2.0138411607061113 and parameters: {'embedding_size': 38, 'hidden_size': 349, 'learning_rate': 0.005838779662173129, 'batch_size': 64, 'context_length': 4}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8828 | val_loss 2.0138 | lr 5.762645e-12\n",
      "step 0 | train_loss 3.2858 | val_loss 3.2868 | lr 1.265469e-04\n",
      "step 200 | train_loss 2.6541 | val_loss 2.6628 | lr 1.265419e-04\n",
      "step 400 | train_loss 2.5204 | val_loss 2.5331 | lr 1.265269e-04\n",
      "step 600 | train_loss 2.4597 | val_loss 2.4757 | lr 1.265020e-04\n",
      "step 800 | train_loss 2.4237 | val_loss 2.4398 | lr 1.264670e-04\n",
      "step 1000 | train_loss 2.3986 | val_loss 2.4140 | lr 1.264221e-04\n",
      "step 1200 | train_loss 2.3758 | val_loss 2.3943 | lr 1.263672e-04\n",
      "step 1400 | train_loss 2.3606 | val_loss 2.3793 | lr 1.263023e-04\n",
      "step 1600 | train_loss 2.3481 | val_loss 2.3653 | lr 1.262275e-04\n",
      "step 1800 | train_loss 2.3279 | val_loss 2.3541 | lr 1.261427e-04\n",
      "step 2000 | train_loss 2.3188 | val_loss 2.3445 | lr 1.260480e-04\n",
      "step 2200 | train_loss 2.3109 | val_loss 2.3346 | lr 1.259434e-04\n",
      "step 2400 | train_loss 2.3017 | val_loss 2.3266 | lr 1.258289e-04\n",
      "step 2600 | train_loss 2.2976 | val_loss 2.3183 | lr 1.257045e-04\n",
      "step 2800 | train_loss 2.2901 | val_loss 2.3125 | lr 1.255703e-04\n",
      "step 3000 | train_loss 2.2838 | val_loss 2.3072 | lr 1.254262e-04\n",
      "step 3200 | train_loss 2.2777 | val_loss 2.3006 | lr 1.252723e-04\n",
      "step 3400 | train_loss 2.2604 | val_loss 2.2961 | lr 1.251086e-04\n",
      "step 3600 | train_loss 2.2625 | val_loss 2.2912 | lr 1.249352e-04\n",
      "step 3800 | train_loss 2.2597 | val_loss 2.2864 | lr 1.247520e-04\n",
      "step 4000 | train_loss 2.2563 | val_loss 2.2818 | lr 1.245591e-04\n",
      "step 4200 | train_loss 2.2523 | val_loss 2.2761 | lr 1.243565e-04\n",
      "step 4400 | train_loss 2.2508 | val_loss 2.2714 | lr 1.241443e-04\n",
      "step 4600 | train_loss 2.2432 | val_loss 2.2689 | lr 1.239225e-04\n",
      "step 4800 | train_loss 2.2420 | val_loss 2.2671 | lr 1.236911e-04\n",
      "step 5000 | train_loss 2.2408 | val_loss 2.2621 | lr 1.234501e-04\n",
      "step 5200 | train_loss 2.2296 | val_loss 2.2598 | lr 1.231997e-04\n",
      "step 5400 | train_loss 2.2257 | val_loss 2.2564 | lr 1.229397e-04\n",
      "step 5600 | train_loss 2.2237 | val_loss 2.2531 | lr 1.226704e-04\n",
      "step 5800 | train_loss 2.2222 | val_loss 2.2488 | lr 1.223917e-04\n",
      "step 6000 | train_loss 2.2219 | val_loss 2.2458 | lr 1.221036e-04\n",
      "step 6200 | train_loss 2.2175 | val_loss 2.2432 | lr 1.218063e-04\n",
      "step 6400 | train_loss 2.2167 | val_loss 2.2422 | lr 1.214997e-04\n",
      "step 6600 | train_loss 2.2160 | val_loss 2.2392 | lr 1.211839e-04\n",
      "step 6800 | train_loss 2.2038 | val_loss 2.2366 | lr 1.208590e-04\n",
      "step 7000 | train_loss 2.2030 | val_loss 2.2348 | lr 1.205250e-04\n",
      "step 7200 | train_loss 2.2028 | val_loss 2.2314 | lr 1.201820e-04\n",
      "step 7400 | train_loss 2.1996 | val_loss 2.2291 | lr 1.198299e-04\n",
      "step 7600 | train_loss 2.1995 | val_loss 2.2258 | lr 1.194689e-04\n",
      "step 7800 | train_loss 2.1986 | val_loss 2.2242 | lr 1.190991e-04\n",
      "step 8000 | train_loss 2.1950 | val_loss 2.2233 | lr 1.187204e-04\n",
      "step 8200 | train_loss 2.1929 | val_loss 2.2207 | lr 1.183330e-04\n",
      "step 8400 | train_loss 2.1789 | val_loss 2.2193 | lr 1.179369e-04\n",
      "step 8600 | train_loss 2.1839 | val_loss 2.2176 | lr 1.175321e-04\n",
      "step 8800 | train_loss 2.1839 | val_loss 2.2160 | lr 1.171188e-04\n",
      "step 9000 | train_loss 2.1824 | val_loss 2.2135 | lr 1.166970e-04\n",
      "step 9200 | train_loss 2.1820 | val_loss 2.2106 | lr 1.162668e-04\n",
      "step 9400 | train_loss 2.1826 | val_loss 2.2077 | lr 1.158281e-04\n",
      "step 9600 | train_loss 2.1769 | val_loss 2.2073 | lr 1.153812e-04\n",
      "step 9800 | train_loss 2.1777 | val_loss 2.2075 | lr 1.149261e-04\n",
      "step 10000 | train_loss 2.1795 | val_loss 2.2047 | lr 1.144628e-04\n",
      "step 10200 | train_loss 2.1687 | val_loss 2.2031 | lr 1.139914e-04\n",
      "step 10400 | train_loss 2.1674 | val_loss 2.2023 | lr 1.135120e-04\n",
      "step 10600 | train_loss 2.1675 | val_loss 2.2004 | lr 1.130247e-04\n",
      "step 10800 | train_loss 2.1657 | val_loss 2.1980 | lr 1.125295e-04\n",
      "step 11000 | train_loss 2.1673 | val_loss 2.1965 | lr 1.120265e-04\n",
      "step 11200 | train_loss 2.1650 | val_loss 2.1949 | lr 1.115158e-04\n",
      "step 11400 | train_loss 2.1645 | val_loss 2.1951 | lr 1.109976e-04\n",
      "step 11600 | train_loss 2.1664 | val_loss 2.1938 | lr 1.104717e-04\n",
      "step 11800 | train_loss 2.1540 | val_loss 2.1922 | lr 1.099385e-04\n",
      "step 12000 | train_loss 2.1553 | val_loss 2.1915 | lr 1.093978e-04\n",
      "step 12200 | train_loss 2.1561 | val_loss 2.1896 | lr 1.088499e-04\n",
      "step 12400 | train_loss 2.1539 | val_loss 2.1882 | lr 1.082948e-04\n",
      "step 12600 | train_loss 2.1544 | val_loss 2.1861 | lr 1.077326e-04\n",
      "step 12800 | train_loss 2.1559 | val_loss 2.1845 | lr 1.071633e-04\n",
      "step 13000 | train_loss 2.1520 | val_loss 2.1851 | lr 1.065871e-04\n",
      "step 13200 | train_loss 2.1524 | val_loss 2.1840 | lr 1.060041e-04\n",
      "step 13400 | train_loss 2.1390 | val_loss 2.1827 | lr 1.054143e-04\n",
      "step 13600 | train_loss 2.1438 | val_loss 2.1821 | lr 1.048179e-04\n",
      "step 13800 | train_loss 2.1439 | val_loss 2.1815 | lr 1.042149e-04\n",
      "step 14000 | train_loss 2.1439 | val_loss 2.1801 | lr 1.036055e-04\n",
      "step 14200 | train_loss 2.1441 | val_loss 2.1780 | lr 1.029897e-04\n",
      "step 14400 | train_loss 2.1460 | val_loss 2.1755 | lr 1.023676e-04\n",
      "step 14600 | train_loss 2.1418 | val_loss 2.1757 | lr 1.017393e-04\n",
      "step 14800 | train_loss 2.1423 | val_loss 2.1763 | lr 1.011050e-04\n",
      "step 15000 | train_loss 2.1450 | val_loss 2.1748 | lr 1.004647e-04\n",
      "step 15200 | train_loss 2.1349 | val_loss 2.1731 | lr 9.981849e-05\n",
      "step 15400 | train_loss 2.1347 | val_loss 2.1735 | lr 9.916654e-05\n",
      "step 15600 | train_loss 2.1361 | val_loss 2.1723 | lr 9.850891e-05\n",
      "step 15800 | train_loss 2.1338 | val_loss 2.1705 | lr 9.784573e-05\n",
      "step 16000 | train_loss 2.1363 | val_loss 2.1695 | lr 9.717708e-05\n",
      "step 16200 | train_loss 2.1354 | val_loss 2.1685 | lr 9.650308e-05\n",
      "step 16400 | train_loss 2.1343 | val_loss 2.1687 | lr 9.582384e-05\n",
      "step 16600 | train_loss 2.1368 | val_loss 2.1681 | lr 9.513945e-05\n",
      "step 16800 | train_loss 2.1245 | val_loss 2.1671 | lr 9.445003e-05\n",
      "step 17000 | train_loss 2.1276 | val_loss 2.1668 | lr 9.375569e-05\n",
      "step 17200 | train_loss 2.1276 | val_loss 2.1656 | lr 9.305653e-05\n",
      "step 17400 | train_loss 2.1269 | val_loss 2.1652 | lr 9.235267e-05\n",
      "step 17600 | train_loss 2.1276 | val_loss 2.1631 | lr 9.164422e-05\n",
      "step 17800 | train_loss 2.1297 | val_loss 2.1616 | lr 9.093129e-05\n",
      "step 18000 | train_loss 2.1261 | val_loss 2.1627 | lr 9.021400e-05\n",
      "step 18200 | train_loss 2.1276 | val_loss 2.1628 | lr 8.949244e-05\n",
      "step 18400 | train_loss 2.1169 | val_loss 2.1614 | lr 8.876675e-05\n",
      "step 18600 | train_loss 2.1199 | val_loss 2.1615 | lr 8.803703e-05\n",
      "step 18800 | train_loss 2.1194 | val_loss 2.1610 | lr 8.730340e-05\n",
      "step 19000 | train_loss 2.1203 | val_loss 2.1601 | lr 8.656598e-05\n",
      "step 19200 | train_loss 2.1203 | val_loss 2.1586 | lr 8.582488e-05\n",
      "step 19400 | train_loss 2.1225 | val_loss 2.1567 | lr 8.508022e-05\n",
      "step 19600 | train_loss 2.1204 | val_loss 2.1566 | lr 8.433211e-05\n",
      "step 19800 | train_loss 2.1202 | val_loss 2.1575 | lr 8.358068e-05\n",
      "step 20000 | train_loss 2.1235 | val_loss 2.1567 | lr 8.282604e-05\n",
      "step 20200 | train_loss 2.1138 | val_loss 2.1554 | lr 8.206831e-05\n",
      "step 20400 | train_loss 2.1141 | val_loss 2.1561 | lr 8.130762e-05\n",
      "step 20600 | train_loss 2.1155 | val_loss 2.1549 | lr 8.054408e-05\n",
      "step 20800 | train_loss 2.1138 | val_loss 2.1540 | lr 7.977781e-05\n",
      "step 21000 | train_loss 2.1163 | val_loss 2.1528 | lr 7.900893e-05\n",
      "step 21200 | train_loss 2.1168 | val_loss 2.1521 | lr 7.823758e-05\n",
      "step 21400 | train_loss 2.1153 | val_loss 2.1523 | lr 7.746385e-05\n",
      "step 21600 | train_loss 2.1172 | val_loss 2.1522 | lr 7.668789e-05\n",
      "step 21800 | train_loss 2.1057 | val_loss 2.1519 | lr 7.590981e-05\n",
      "step 22000 | train_loss 2.1100 | val_loss 2.1516 | lr 7.512973e-05\n",
      "step 22200 | train_loss 2.1094 | val_loss 2.1508 | lr 7.434778e-05\n",
      "step 22400 | train_loss 2.1096 | val_loss 2.1508 | lr 7.356408e-05\n",
      "step 22600 | train_loss 2.1103 | val_loss 2.1487 | lr 7.277876e-05\n",
      "step 22800 | train_loss 2.1124 | val_loss 2.1473 | lr 7.199193e-05\n",
      "step 23000 | train_loss 2.1094 | val_loss 2.1485 | lr 7.120373e-05\n",
      "step 23200 | train_loss 2.1116 | val_loss 2.1491 | lr 7.041428e-05\n",
      "step 23400 | train_loss 2.1066 | val_loss 2.1477 | lr 6.962370e-05\n",
      "step 23600 | train_loss 2.1045 | val_loss 2.1483 | lr 6.883211e-05\n",
      "step 23800 | train_loss 2.1042 | val_loss 2.1478 | lr 6.803965e-05\n",
      "step 24000 | train_loss 2.1057 | val_loss 2.1475 | lr 6.724644e-05\n",
      "step 24200 | train_loss 2.1048 | val_loss 2.1459 | lr 6.645260e-05\n",
      "step 24400 | train_loss 2.1068 | val_loss 2.1450 | lr 6.565825e-05\n",
      "step 24600 | train_loss 2.1060 | val_loss 2.1446 | lr 6.486353e-05\n",
      "step 24800 | train_loss 2.1058 | val_loss 2.1455 | lr 6.406856e-05\n",
      "step 25000 | train_loss 2.1084 | val_loss 2.1448 | lr 6.327346e-05\n",
      "step 25200 | train_loss 2.1004 | val_loss 2.1443 | lr 6.247837e-05\n",
      "step 25400 | train_loss 2.1010 | val_loss 2.1448 | lr 6.168340e-05\n",
      "step 25600 | train_loss 2.1021 | val_loss 2.1439 | lr 6.088868e-05\n",
      "step 25800 | train_loss 2.1007 | val_loss 2.1434 | lr 6.009433e-05\n",
      "step 26000 | train_loss 2.1029 | val_loss 2.1419 | lr 5.930049e-05\n",
      "step 26200 | train_loss 2.1036 | val_loss 2.1416 | lr 5.850728e-05\n",
      "step 26400 | train_loss 2.1026 | val_loss 2.1420 | lr 5.771481e-05\n",
      "step 26600 | train_loss 2.1036 | val_loss 2.1420 | lr 5.692323e-05\n",
      "step 26800 | train_loss 2.0947 | val_loss 2.1419 | lr 5.613265e-05\n",
      "step 27000 | train_loss 2.0984 | val_loss 2.1418 | lr 5.534320e-05\n",
      "step 27200 | train_loss 2.0977 | val_loss 2.1414 | lr 5.455500e-05\n",
      "step 27400 | train_loss 2.0987 | val_loss 2.1413 | lr 5.376817e-05\n",
      "step 27600 | train_loss 2.0984 | val_loss 2.1397 | lr 5.298285e-05\n",
      "step 27800 | train_loss 2.1005 | val_loss 2.1386 | lr 5.219915e-05\n",
      "step 28000 | train_loss 2.0985 | val_loss 2.1393 | lr 5.141720e-05\n",
      "step 28200 | train_loss 2.1000 | val_loss 2.1402 | lr 5.063712e-05\n",
      "step 28400 | train_loss 2.1002 | val_loss 2.1391 | lr 4.985904e-05\n",
      "step 28600 | train_loss 2.0949 | val_loss 2.1395 | lr 4.908308e-05\n",
      "step 28800 | train_loss 2.0946 | val_loss 2.1393 | lr 4.830935e-05\n",
      "step 29000 | train_loss 2.0958 | val_loss 2.1392 | lr 4.753799e-05\n",
      "step 29200 | train_loss 2.0951 | val_loss 2.1379 | lr 4.676912e-05\n",
      "step 29400 | train_loss 2.0966 | val_loss 2.1373 | lr 4.600285e-05\n",
      "step 29600 | train_loss 2.0964 | val_loss 2.1370 | lr 4.523931e-05\n",
      "step 29800 | train_loss 2.0963 | val_loss 2.1376 | lr 4.447861e-05\n",
      "step 30000 | train_loss 2.0981 | val_loss 2.1372 | lr 4.372089e-05\n",
      "step 30200 | train_loss 2.0922 | val_loss 2.1371 | lr 4.296625e-05\n",
      "step 30400 | train_loss 2.0931 | val_loss 2.1373 | lr 4.221482e-05\n",
      "step 30600 | train_loss 2.0935 | val_loss 2.1368 | lr 4.146671e-05\n",
      "step 30800 | train_loss 2.0929 | val_loss 2.1367 | lr 4.072205e-05\n",
      "step 31000 | train_loss 2.0937 | val_loss 2.1353 | lr 3.998095e-05\n",
      "step 31200 | train_loss 2.0946 | val_loss 2.1353 | lr 3.924353e-05\n",
      "step 31400 | train_loss 2.0939 | val_loss 2.1356 | lr 3.850990e-05\n",
      "step 31600 | train_loss 2.0946 | val_loss 2.1356 | lr 3.778018e-05\n",
      "step 31800 | train_loss 2.0887 | val_loss 2.1355 | lr 3.705449e-05\n",
      "step 32000 | train_loss 2.0910 | val_loss 2.1355 | lr 3.633293e-05\n",
      "step 32200 | train_loss 2.0907 | val_loss 2.1354 | lr 3.561564e-05\n",
      "step 32400 | train_loss 2.0918 | val_loss 2.1353 | lr 3.490270e-05\n",
      "step 32600 | train_loss 2.0912 | val_loss 2.1343 | lr 3.419425e-05\n",
      "step 32800 | train_loss 2.0925 | val_loss 2.1334 | lr 3.349040e-05\n",
      "step 33000 | train_loss 2.0914 | val_loss 2.1337 | lr 3.279124e-05\n",
      "step 33200 | train_loss 2.0922 | val_loss 2.1344 | lr 3.209690e-05\n",
      "step 33400 | train_loss 2.0936 | val_loss 2.1338 | lr 3.140748e-05\n",
      "step 33600 | train_loss 2.0892 | val_loss 2.1340 | lr 3.072309e-05\n",
      "step 33800 | train_loss 2.0891 | val_loss 2.1341 | lr 3.004385e-05\n",
      "step 34000 | train_loss 2.0895 | val_loss 2.1339 | lr 2.936985e-05\n",
      "step 34200 | train_loss 2.0894 | val_loss 2.1333 | lr 2.870120e-05\n",
      "step 34400 | train_loss 2.0902 | val_loss 2.1328 | lr 2.803801e-05\n",
      "step 34600 | train_loss 2.0903 | val_loss 2.1326 | lr 2.738039e-05\n",
      "step 34800 | train_loss 2.0902 | val_loss 2.1329 | lr 2.672844e-05\n",
      "step 35000 | train_loss 2.0914 | val_loss 2.1328 | lr 2.608226e-05\n",
      "step 35200 | train_loss 2.0877 | val_loss 2.1327 | lr 2.544194e-05\n",
      "step 35400 | train_loss 2.0883 | val_loss 2.1328 | lr 2.480761e-05\n",
      "step 35600 | train_loss 2.0884 | val_loss 2.1326 | lr 2.417935e-05\n",
      "step 35800 | train_loss 2.0883 | val_loss 2.1327 | lr 2.355726e-05\n",
      "step 36000 | train_loss 2.0883 | val_loss 2.1318 | lr 2.294144e-05\n",
      "step 36200 | train_loss 2.0891 | val_loss 2.1317 | lr 2.233199e-05\n",
      "step 36400 | train_loss 2.0886 | val_loss 2.1317 | lr 2.172901e-05\n",
      "step 36600 | train_loss 2.0890 | val_loss 2.1319 | lr 2.113259e-05\n",
      "step 36800 | train_loss 2.0862 | val_loss 2.1318 | lr 2.054282e-05\n",
      "step 37000 | train_loss 2.0870 | val_loss 2.1318 | lr 1.995980e-05\n",
      "step 37200 | train_loss 2.0870 | val_loss 2.1319 | lr 1.938362e-05\n",
      "step 37400 | train_loss 2.0875 | val_loss 2.1318 | lr 1.881437e-05\n",
      "step 37600 | train_loss 2.0870 | val_loss 2.1314 | lr 1.825214e-05\n",
      "step 37800 | train_loss 2.0878 | val_loss 2.1308 | lr 1.769702e-05\n",
      "step 38000 | train_loss 2.0873 | val_loss 2.1308 | lr 1.714909e-05\n",
      "step 38200 | train_loss 2.0876 | val_loss 2.1311 | lr 1.660845e-05\n",
      "step 38400 | train_loss 2.0888 | val_loss 2.1309 | lr 1.607518e-05\n",
      "step 38600 | train_loss 2.0864 | val_loss 2.1310 | lr 1.554937e-05\n",
      "step 38800 | train_loss 2.0864 | val_loss 2.1311 | lr 1.503109e-05\n",
      "step 39000 | train_loss 2.0865 | val_loss 2.1310 | lr 1.452042e-05\n",
      "step 39200 | train_loss 2.0864 | val_loss 2.1308 | lr 1.401746e-05\n",
      "step 39400 | train_loss 2.0866 | val_loss 2.1305 | lr 1.352227e-05\n",
      "step 39600 | train_loss 2.0868 | val_loss 2.1304 | lr 1.303494e-05\n",
      "step 39800 | train_loss 2.0867 | val_loss 2.1305 | lr 1.255554e-05\n",
      "step 40000 | train_loss 2.0874 | val_loss 2.1304 | lr 1.208416e-05\n",
      "step 40200 | train_loss 2.0857 | val_loss 2.1304 | lr 1.162085e-05\n",
      "step 40400 | train_loss 2.0859 | val_loss 2.1304 | lr 1.116570e-05\n",
      "step 40600 | train_loss 2.0859 | val_loss 2.1304 | lr 1.071878e-05\n",
      "step 40800 | train_loss 2.0860 | val_loss 2.1305 | lr 1.028016e-05\n",
      "step 41000 | train_loss 2.0858 | val_loss 2.1301 | lr 9.849911e-06\n",
      "step 41200 | train_loss 2.0862 | val_loss 2.1300 | lr 9.428095e-06\n",
      "step 41400 | train_loss 2.0860 | val_loss 2.1300 | lr 9.014782e-06\n",
      "step 41600 | train_loss 2.0862 | val_loss 2.1301 | lr 8.610037e-06\n",
      "step 41800 | train_loss 2.0853 | val_loss 2.1300 | lr 8.213924e-06\n",
      "step 42000 | train_loss 2.0854 | val_loss 2.1300 | lr 7.826505e-06\n",
      "step 42200 | train_loss 2.0855 | val_loss 2.1301 | lr 7.447842e-06\n",
      "step 42400 | train_loss 2.0856 | val_loss 2.1301 | lr 7.077994e-06\n",
      "step 42600 | train_loss 2.0855 | val_loss 2.1299 | lr 6.717021e-06\n",
      "step 42800 | train_loss 2.0857 | val_loss 2.1297 | lr 6.364978e-06\n",
      "step 43000 | train_loss 2.0855 | val_loss 2.1297 | lr 6.021922e-06\n",
      "step 43200 | train_loss 2.0856 | val_loss 2.1298 | lr 5.687907e-06\n",
      "step 43400 | train_loss 2.0861 | val_loss 2.1297 | lr 5.362985e-06\n",
      "step 43600 | train_loss 2.0853 | val_loss 2.1297 | lr 5.047207e-06\n",
      "step 43800 | train_loss 2.0853 | val_loss 2.1298 | lr 4.740625e-06\n",
      "step 44000 | train_loss 2.0854 | val_loss 2.1297 | lr 4.443285e-06\n",
      "step 44200 | train_loss 2.0853 | val_loss 2.1297 | lr 4.155235e-06\n",
      "step 44400 | train_loss 2.0853 | val_loss 2.1296 | lr 3.876521e-06\n",
      "step 44600 | train_loss 2.0854 | val_loss 2.1296 | lr 3.607186e-06\n",
      "step 44800 | train_loss 2.0853 | val_loss 2.1296 | lr 3.347274e-06\n",
      "step 45000 | train_loss 2.0855 | val_loss 2.1296 | lr 3.096824e-06\n",
      "step 45200 | train_loss 2.0851 | val_loss 2.1296 | lr 2.855877e-06\n",
      "step 45400 | train_loss 2.0852 | val_loss 2.1296 | lr 2.624470e-06\n",
      "step 45600 | train_loss 2.0852 | val_loss 2.1296 | lr 2.402641e-06\n",
      "step 45800 | train_loss 2.0852 | val_loss 2.1296 | lr 2.190424e-06\n",
      "step 46000 | train_loss 2.0851 | val_loss 2.1295 | lr 1.987852e-06\n",
      "step 46200 | train_loss 2.0852 | val_loss 2.1295 | lr 1.794959e-06\n",
      "step 46400 | train_loss 2.0852 | val_loss 2.1295 | lr 1.611773e-06\n",
      "step 46600 | train_loss 2.0852 | val_loss 2.1295 | lr 1.438325e-06\n",
      "step 46800 | train_loss 2.0851 | val_loss 2.1295 | lr 1.274641e-06\n",
      "step 47000 | train_loss 2.0851 | val_loss 2.1295 | lr 1.120747e-06\n",
      "step 47200 | train_loss 2.0851 | val_loss 2.1295 | lr 9.766680e-07\n",
      "step 47400 | train_loss 2.0851 | val_loss 2.1295 | lr 8.424264e-07\n",
      "step 47600 | train_loss 2.0851 | val_loss 2.1295 | lr 7.180434e-07\n",
      "step 47800 | train_loss 2.0851 | val_loss 2.1295 | lr 6.035386e-07\n",
      "step 48000 | train_loss 2.0851 | val_loss 2.1295 | lr 4.989302e-07\n",
      "step 48200 | train_loss 2.0851 | val_loss 2.1295 | lr 4.042345e-07\n",
      "step 48400 | train_loss 2.0852 | val_loss 2.1295 | lr 3.194666e-07\n",
      "step 48600 | train_loss 2.0851 | val_loss 2.1295 | lr 2.446399e-07\n",
      "step 48800 | train_loss 2.0851 | val_loss 2.1295 | lr 1.797662e-07\n",
      "step 49000 | train_loss 2.0851 | val_loss 2.1295 | lr 1.248557e-07\n",
      "step 49200 | train_loss 2.0851 | val_loss 2.1295 | lr 7.991713e-08\n",
      "step 49400 | train_loss 2.0851 | val_loss 2.1295 | lr 4.495753e-08\n",
      "step 49600 | train_loss 2.0851 | val_loss 2.1295 | lr 1.998244e-08\n",
      "step 49800 | train_loss 2.0851 | val_loss 2.1295 | lr 4.995807e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:04:20,117] Trial 3 finished with value: 2.1294838977711543 and parameters: {'embedding_size': 26, 'hidden_size': 494, 'learning_rate': 0.0001265469285635071, 'batch_size': 128, 'context_length': 3}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.0851 | val_loss 2.1295 | lr 1.248968e-13\n",
      "step 0 | train_loss 3.3011 | val_loss 3.3012 | lr 5.006369e-04\n",
      "step 200 | train_loss 2.3733 | val_loss 2.4022 | lr 5.006171e-04\n",
      "step 400 | train_loss 2.3125 | val_loss 2.3398 | lr 5.005578e-04\n",
      "step 600 | train_loss 2.2856 | val_loss 2.3128 | lr 5.004590e-04\n",
      "step 800 | train_loss 2.2710 | val_loss 2.2871 | lr 5.003207e-04\n",
      "step 1000 | train_loss 2.2531 | val_loss 2.2647 | lr 5.001429e-04\n",
      "step 1200 | train_loss 2.2356 | val_loss 2.2525 | lr 4.999257e-04\n",
      "step 1400 | train_loss 2.2293 | val_loss 2.2457 | lr 4.996690e-04\n",
      "step 1600 | train_loss 2.2189 | val_loss 2.2294 | lr 4.993730e-04\n",
      "step 1800 | train_loss 2.1820 | val_loss 2.2232 | lr 4.990377e-04\n",
      "step 2000 | train_loss 2.1840 | val_loss 2.2159 | lr 4.986630e-04\n",
      "step 2200 | train_loss 2.1746 | val_loss 2.2069 | lr 4.982492e-04\n",
      "step 2400 | train_loss 2.1729 | val_loss 2.2024 | lr 4.977962e-04\n",
      "step 2600 | train_loss 2.1781 | val_loss 2.1968 | lr 4.973041e-04\n",
      "step 2800 | train_loss 2.1634 | val_loss 2.1936 | lr 4.967730e-04\n",
      "step 3000 | train_loss 2.1683 | val_loss 2.1905 | lr 4.962030e-04\n",
      "step 3200 | train_loss 2.1663 | val_loss 2.1834 | lr 4.955942e-04\n",
      "step 3400 | train_loss 2.0947 | val_loss 2.1758 | lr 4.949467e-04\n",
      "step 3600 | train_loss 2.1346 | val_loss 2.1750 | lr 4.942605e-04\n",
      "step 3800 | train_loss 2.1313 | val_loss 2.1726 | lr 4.935358e-04\n",
      "step 4000 | train_loss 2.1271 | val_loss 2.1655 | lr 4.927727e-04\n",
      "step 4200 | train_loss 2.1333 | val_loss 2.1626 | lr 4.919713e-04\n",
      "step 4400 | train_loss 2.1229 | val_loss 2.1564 | lr 4.911317e-04\n",
      "step 4600 | train_loss 2.1214 | val_loss 2.1598 | lr 4.902541e-04\n",
      "step 4800 | train_loss 2.1234 | val_loss 2.1597 | lr 4.893386e-04\n",
      "step 5000 | train_loss 2.1236 | val_loss 2.1485 | lr 4.883854e-04\n",
      "step 5200 | train_loss 2.0921 | val_loss 2.1530 | lr 4.873946e-04\n",
      "step 5400 | train_loss 2.0903 | val_loss 2.1449 | lr 4.863663e-04\n",
      "step 5600 | train_loss 2.0928 | val_loss 2.1480 | lr 4.853008e-04\n",
      "step 5800 | train_loss 2.1064 | val_loss 2.1409 | lr 4.841982e-04\n",
      "step 6000 | train_loss 2.1032 | val_loss 2.1356 | lr 4.830586e-04\n",
      "step 6200 | train_loss 2.0979 | val_loss 2.1384 | lr 4.818823e-04\n",
      "step 6400 | train_loss 2.1040 | val_loss 2.1392 | lr 4.806694e-04\n",
      "step 6600 | train_loss 2.1046 | val_loss 2.1370 | lr 4.794202e-04\n",
      "step 6800 | train_loss 2.0600 | val_loss 2.1344 | lr 4.781347e-04\n",
      "step 7000 | train_loss 2.0702 | val_loss 2.1313 | lr 4.768133e-04\n",
      "step 7200 | train_loss 2.0730 | val_loss 2.1260 | lr 4.754561e-04\n",
      "step 7400 | train_loss 2.0752 | val_loss 2.1259 | lr 4.740634e-04\n",
      "step 7600 | train_loss 2.0819 | val_loss 2.1238 | lr 4.726354e-04\n",
      "step 7800 | train_loss 2.0793 | val_loss 2.1297 | lr 4.711722e-04\n",
      "step 8000 | train_loss 2.0822 | val_loss 2.1270 | lr 4.696741e-04\n",
      "step 8200 | train_loss 2.0805 | val_loss 2.1224 | lr 4.681415e-04\n",
      "step 8400 | train_loss 2.0044 | val_loss 2.1228 | lr 4.665744e-04\n",
      "step 8600 | train_loss 2.0506 | val_loss 2.1192 | lr 4.649732e-04\n",
      "step 8800 | train_loss 2.0545 | val_loss 2.1223 | lr 4.633380e-04\n",
      "step 9000 | train_loss 2.0541 | val_loss 2.1153 | lr 4.616693e-04\n",
      "step 9200 | train_loss 2.0692 | val_loss 2.1174 | lr 4.599671e-04\n",
      "step 9400 | train_loss 2.0611 | val_loss 2.1121 | lr 4.582319e-04\n",
      "step 9600 | train_loss 2.0584 | val_loss 2.1156 | lr 4.564638e-04\n",
      "step 9800 | train_loss 2.0615 | val_loss 2.1168 | lr 4.546632e-04\n",
      "step 10000 | train_loss 2.0641 | val_loss 2.1077 | lr 4.528303e-04\n",
      "step 10200 | train_loss 2.0272 | val_loss 2.1114 | lr 4.509654e-04\n",
      "step 10400 | train_loss 2.0364 | val_loss 2.1099 | lr 4.490689e-04\n",
      "step 10600 | train_loss 2.0399 | val_loss 2.1107 | lr 4.471409e-04\n",
      "step 10800 | train_loss 2.0485 | val_loss 2.1055 | lr 4.451819e-04\n",
      "step 11000 | train_loss 2.0497 | val_loss 2.1067 | lr 4.431921e-04\n",
      "step 11200 | train_loss 2.0474 | val_loss 2.1081 | lr 4.411718e-04\n",
      "step 11400 | train_loss 2.0533 | val_loss 2.1086 | lr 4.391215e-04\n",
      "step 11600 | train_loss 2.0598 | val_loss 2.1083 | lr 4.370413e-04\n",
      "step 11800 | train_loss 2.0055 | val_loss 2.1052 | lr 4.349316e-04\n",
      "step 12000 | train_loss 2.0230 | val_loss 2.1043 | lr 4.327927e-04\n",
      "step 12200 | train_loss 2.0297 | val_loss 2.1004 | lr 4.306251e-04\n",
      "step 12400 | train_loss 2.0277 | val_loss 2.0975 | lr 4.284289e-04\n",
      "step 12600 | train_loss 2.0374 | val_loss 2.0985 | lr 4.262047e-04\n",
      "step 12800 | train_loss 2.0402 | val_loss 2.1025 | lr 4.239526e-04\n",
      "step 13000 | train_loss 2.0385 | val_loss 2.1023 | lr 4.216732e-04\n",
      "step 13200 | train_loss 2.0396 | val_loss 2.1005 | lr 4.193667e-04\n",
      "step 13400 | train_loss 1.9627 | val_loss 2.0996 | lr 4.170335e-04\n",
      "step 13600 | train_loss 2.0066 | val_loss 2.0958 | lr 4.146740e-04\n",
      "step 13800 | train_loss 2.0152 | val_loss 2.1021 | lr 4.122885e-04\n",
      "step 14000 | train_loss 2.0146 | val_loss 2.0942 | lr 4.098774e-04\n",
      "step 14200 | train_loss 2.0297 | val_loss 2.0960 | lr 4.074412e-04\n",
      "step 14400 | train_loss 2.0273 | val_loss 2.0911 | lr 4.049801e-04\n",
      "step 14600 | train_loss 2.0246 | val_loss 2.0944 | lr 4.024946e-04\n",
      "step 14800 | train_loss 2.0257 | val_loss 2.0969 | lr 3.999851e-04\n",
      "step 15000 | train_loss 2.0309 | val_loss 2.0916 | lr 3.974519e-04\n",
      "step 15200 | train_loss 1.9931 | val_loss 2.0920 | lr 3.948955e-04\n",
      "step 15400 | train_loss 2.0051 | val_loss 2.0943 | lr 3.923163e-04\n",
      "step 15600 | train_loss 2.0088 | val_loss 2.0938 | lr 3.897147e-04\n",
      "step 15800 | train_loss 2.0135 | val_loss 2.0886 | lr 3.870910e-04\n",
      "step 16000 | train_loss 2.0178 | val_loss 2.0910 | lr 3.844458e-04\n",
      "step 16200 | train_loss 2.0184 | val_loss 2.0909 | lr 3.817793e-04\n",
      "step 16400 | train_loss 2.0232 | val_loss 2.0921 | lr 3.790921e-04\n",
      "step 16600 | train_loss 2.0296 | val_loss 2.0907 | lr 3.763846e-04\n",
      "step 16800 | train_loss 1.9719 | val_loss 2.0882 | lr 3.736572e-04\n",
      "step 17000 | train_loss 1.9961 | val_loss 2.0885 | lr 3.709103e-04\n",
      "step 17200 | train_loss 1.9997 | val_loss 2.0865 | lr 3.681443e-04\n",
      "step 17400 | train_loss 1.9970 | val_loss 2.0851 | lr 3.653597e-04\n",
      "step 17600 | train_loss 2.0115 | val_loss 2.0855 | lr 3.625570e-04\n",
      "step 17800 | train_loss 2.0143 | val_loss 2.0881 | lr 3.597366e-04\n",
      "step 18000 | train_loss 2.0120 | val_loss 2.0902 | lr 3.568988e-04\n",
      "step 18200 | train_loss 2.0124 | val_loss 2.0892 | lr 3.540443e-04\n",
      "step 18400 | train_loss 1.9484 | val_loss 2.0854 | lr 3.511733e-04\n",
      "step 18600 | train_loss 1.9804 | val_loss 2.0850 | lr 3.482865e-04\n",
      "step 18800 | train_loss 1.9886 | val_loss 2.0887 | lr 3.453841e-04\n",
      "step 19000 | train_loss 1.9904 | val_loss 2.0837 | lr 3.424668e-04\n",
      "step 19200 | train_loss 2.0047 | val_loss 2.0857 | lr 3.395349e-04\n",
      "step 19400 | train_loss 2.0031 | val_loss 2.0793 | lr 3.365889e-04\n",
      "step 19600 | train_loss 2.0041 | val_loss 2.0830 | lr 3.336293e-04\n",
      "step 19800 | train_loss 2.0028 | val_loss 2.0863 | lr 3.306565e-04\n",
      "step 20000 | train_loss 2.0147 | val_loss 2.0845 | lr 3.276711e-04\n",
      "step 20200 | train_loss 1.9722 | val_loss 2.0825 | lr 3.246734e-04\n",
      "step 20400 | train_loss 1.9837 | val_loss 2.0858 | lr 3.216640e-04\n",
      "step 20600 | train_loss 1.9868 | val_loss 2.0828 | lr 3.186433e-04\n",
      "step 20800 | train_loss 1.9910 | val_loss 2.0798 | lr 3.156119e-04\n",
      "step 21000 | train_loss 1.9962 | val_loss 2.0798 | lr 3.125701e-04\n",
      "step 21200 | train_loss 2.0001 | val_loss 2.0806 | lr 3.095185e-04\n",
      "step 21400 | train_loss 2.0024 | val_loss 2.0826 | lr 3.064575e-04\n",
      "step 21600 | train_loss 2.0087 | val_loss 2.0819 | lr 3.033877e-04\n",
      "step 21800 | train_loss 1.9510 | val_loss 2.0798 | lr 3.003095e-04\n",
      "step 22000 | train_loss 1.9767 | val_loss 2.0782 | lr 2.972234e-04\n",
      "step 22200 | train_loss 1.9799 | val_loss 2.0784 | lr 2.941299e-04\n",
      "step 22400 | train_loss 1.9774 | val_loss 2.0790 | lr 2.910295e-04\n",
      "step 22600 | train_loss 1.9937 | val_loss 2.0770 | lr 2.879227e-04\n",
      "step 22800 | train_loss 1.9926 | val_loss 2.0764 | lr 2.848099e-04\n",
      "step 23000 | train_loss 1.9942 | val_loss 2.0812 | lr 2.816917e-04\n",
      "step 23200 | train_loss 1.9953 | val_loss 2.0805 | lr 2.785685e-04\n",
      "step 23400 | train_loss 1.9653 | val_loss 2.0764 | lr 2.754408e-04\n",
      "step 23600 | train_loss 1.9642 | val_loss 2.0779 | lr 2.723092e-04\n",
      "step 23800 | train_loss 1.9713 | val_loss 2.0791 | lr 2.691741e-04\n",
      "step 24000 | train_loss 1.9761 | val_loss 2.0776 | lr 2.660361e-04\n",
      "step 24200 | train_loss 1.9849 | val_loss 2.0765 | lr 2.628955e-04\n",
      "step 24400 | train_loss 1.9852 | val_loss 2.0729 | lr 2.597530e-04\n",
      "step 24600 | train_loss 1.9873 | val_loss 2.0759 | lr 2.566090e-04\n",
      "step 24800 | train_loss 1.9872 | val_loss 2.0780 | lr 2.534639e-04\n",
      "step 25000 | train_loss 1.9965 | val_loss 2.0770 | lr 2.503184e-04\n",
      "step 25200 | train_loss 1.9588 | val_loss 2.0749 | lr 2.471729e-04\n",
      "step 25400 | train_loss 1.9676 | val_loss 2.0768 | lr 2.440279e-04\n",
      "step 25600 | train_loss 1.9709 | val_loss 2.0743 | lr 2.408839e-04\n",
      "step 25800 | train_loss 1.9729 | val_loss 2.0731 | lr 2.377414e-04\n",
      "step 26000 | train_loss 1.9831 | val_loss 2.0714 | lr 2.346008e-04\n",
      "step 26200 | train_loss 1.9827 | val_loss 2.0730 | lr 2.314627e-04\n",
      "step 26400 | train_loss 1.9857 | val_loss 2.0757 | lr 2.283277e-04\n",
      "step 26600 | train_loss 1.9891 | val_loss 2.0748 | lr 2.251960e-04\n",
      "step 26800 | train_loss 1.9400 | val_loss 2.0733 | lr 2.220684e-04\n",
      "step 27000 | train_loss 1.9643 | val_loss 2.0727 | lr 2.189452e-04\n",
      "step 27200 | train_loss 1.9665 | val_loss 2.0738 | lr 2.158270e-04\n",
      "step 27400 | train_loss 1.9663 | val_loss 2.0727 | lr 2.127142e-04\n",
      "step 27600 | train_loss 1.9781 | val_loss 2.0713 | lr 2.096074e-04\n",
      "step 27800 | train_loss 1.9770 | val_loss 2.0692 | lr 2.065069e-04\n",
      "step 28000 | train_loss 1.9774 | val_loss 2.0726 | lr 2.034134e-04\n",
      "step 28200 | train_loss 1.9813 | val_loss 2.0750 | lr 2.003273e-04\n",
      "step 28400 | train_loss 1.9773 | val_loss 2.0696 | lr 1.972491e-04\n",
      "step 28600 | train_loss 1.9549 | val_loss 2.0720 | lr 1.941793e-04\n",
      "step 28800 | train_loss 1.9593 | val_loss 2.0719 | lr 1.911184e-04\n",
      "step 29000 | train_loss 1.9638 | val_loss 2.0728 | lr 1.880668e-04\n",
      "step 29200 | train_loss 1.9701 | val_loss 2.0692 | lr 1.850250e-04\n",
      "step 29400 | train_loss 1.9720 | val_loss 2.0675 | lr 1.819935e-04\n",
      "step 29600 | train_loss 1.9726 | val_loss 2.0698 | lr 1.789729e-04\n",
      "step 29800 | train_loss 1.9751 | val_loss 2.0707 | lr 1.759634e-04\n",
      "step 30000 | train_loss 1.9790 | val_loss 2.0697 | lr 1.729658e-04\n",
      "step 30200 | train_loss 1.9506 | val_loss 2.0693 | lr 1.699803e-04\n",
      "step 30400 | train_loss 1.9587 | val_loss 2.0697 | lr 1.670076e-04\n",
      "step 30600 | train_loss 1.9613 | val_loss 2.0687 | lr 1.640480e-04\n",
      "step 30800 | train_loss 1.9619 | val_loss 2.0689 | lr 1.611020e-04\n",
      "step 31000 | train_loss 1.9707 | val_loss 2.0664 | lr 1.581701e-04\n",
      "step 31200 | train_loss 1.9687 | val_loss 2.0681 | lr 1.552527e-04\n",
      "step 31400 | train_loss 1.9709 | val_loss 2.0697 | lr 1.523504e-04\n",
      "step 31600 | train_loss 1.9739 | val_loss 2.0695 | lr 1.494635e-04\n",
      "step 31800 | train_loss 1.9372 | val_loss 2.0682 | lr 1.465926e-04\n",
      "step 32000 | train_loss 1.9556 | val_loss 2.0679 | lr 1.437380e-04\n",
      "step 32200 | train_loss 1.9574 | val_loss 2.0692 | lr 1.409003e-04\n",
      "step 32400 | train_loss 1.9596 | val_loss 2.0682 | lr 1.380798e-04\n",
      "step 32600 | train_loss 1.9659 | val_loss 2.0669 | lr 1.352771e-04\n",
      "step 32800 | train_loss 1.9657 | val_loss 2.0649 | lr 1.324926e-04\n",
      "step 33000 | train_loss 1.9643 | val_loss 2.0665 | lr 1.297266e-04\n",
      "step 33200 | train_loss 1.9681 | val_loss 2.0695 | lr 1.269797e-04\n",
      "step 33400 | train_loss 1.9706 | val_loss 2.0662 | lr 1.242523e-04\n",
      "step 33600 | train_loss 1.9503 | val_loss 2.0671 | lr 1.215447e-04\n",
      "step 33800 | train_loss 1.9532 | val_loss 2.0677 | lr 1.188575e-04\n",
      "step 34000 | train_loss 1.9562 | val_loss 2.0680 | lr 1.161911e-04\n",
      "step 34200 | train_loss 1.9597 | val_loss 2.0656 | lr 1.135459e-04\n",
      "step 34400 | train_loss 1.9627 | val_loss 2.0640 | lr 1.109222e-04\n",
      "step 34600 | train_loss 1.9621 | val_loss 2.0655 | lr 1.083206e-04\n",
      "step 34800 | train_loss 1.9650 | val_loss 2.0665 | lr 1.057413e-04\n",
      "step 35000 | train_loss 1.9673 | val_loss 2.0662 | lr 1.031849e-04\n",
      "step 35200 | train_loss 1.9469 | val_loss 2.0656 | lr 1.006518e-04\n",
      "step 35400 | train_loss 1.9529 | val_loss 2.0658 | lr 9.814227e-05\n",
      "step 35600 | train_loss 1.9553 | val_loss 2.0655 | lr 9.565678e-05\n",
      "step 35800 | train_loss 1.9554 | val_loss 2.0657 | lr 9.319571e-05\n",
      "step 36000 | train_loss 1.9601 | val_loss 2.0633 | lr 9.075946e-05\n",
      "step 36200 | train_loss 1.9601 | val_loss 2.0646 | lr 8.834840e-05\n",
      "step 36400 | train_loss 1.9606 | val_loss 2.0651 | lr 8.596292e-05\n",
      "step 36600 | train_loss 1.9620 | val_loss 2.0654 | lr 8.360339e-05\n",
      "step 36800 | train_loss 1.9415 | val_loss 2.0648 | lr 8.127018e-05\n",
      "step 37000 | train_loss 1.9502 | val_loss 2.0644 | lr 7.896367e-05\n",
      "step 37200 | train_loss 1.9516 | val_loss 2.0658 | lr 7.668422e-05\n",
      "step 37400 | train_loss 1.9540 | val_loss 2.0650 | lr 7.443219e-05\n",
      "step 37600 | train_loss 1.9566 | val_loss 2.0642 | lr 7.220794e-05\n",
      "step 37800 | train_loss 1.9579 | val_loss 2.0626 | lr 7.001181e-05\n",
      "step 38000 | train_loss 1.9567 | val_loss 2.0630 | lr 6.784415e-05\n",
      "step 38200 | train_loss 1.9587 | val_loss 2.0650 | lr 6.570531e-05\n",
      "step 38400 | train_loss 1.9618 | val_loss 2.0638 | lr 6.359561e-05\n",
      "step 38600 | train_loss 1.9485 | val_loss 2.0636 | lr 6.151541e-05\n",
      "step 38800 | train_loss 1.9505 | val_loss 2.0642 | lr 5.946502e-05\n",
      "step 39000 | train_loss 1.9524 | val_loss 2.0644 | lr 5.744477e-05\n",
      "step 39200 | train_loss 1.9534 | val_loss 2.0636 | lr 5.545497e-05\n",
      "step 39400 | train_loss 1.9557 | val_loss 2.0624 | lr 5.349594e-05\n",
      "step 39600 | train_loss 1.9560 | val_loss 2.0628 | lr 5.156800e-05\n",
      "step 39800 | train_loss 1.9570 | val_loss 2.0634 | lr 4.967144e-05\n",
      "step 40000 | train_loss 1.9589 | val_loss 2.0636 | lr 4.780657e-05\n",
      "step 40200 | train_loss 1.9472 | val_loss 2.0630 | lr 4.597367e-05\n",
      "step 40400 | train_loss 1.9504 | val_loss 2.0631 | lr 4.417304e-05\n",
      "step 40600 | train_loss 1.9513 | val_loss 2.0631 | lr 4.240497e-05\n",
      "step 40800 | train_loss 1.9519 | val_loss 2.0635 | lr 4.066973e-05\n",
      "step 41000 | train_loss 1.9534 | val_loss 2.0624 | lr 3.896759e-05\n",
      "step 41200 | train_loss 1.9545 | val_loss 2.0621 | lr 3.729883e-05\n",
      "step 41400 | train_loss 1.9543 | val_loss 2.0623 | lr 3.566370e-05\n",
      "step 41600 | train_loss 1.9555 | val_loss 2.0630 | lr 3.406248e-05\n",
      "step 41800 | train_loss 1.9477 | val_loss 2.0626 | lr 3.249540e-05\n",
      "step 42000 | train_loss 1.9494 | val_loss 2.0623 | lr 3.096272e-05\n",
      "step 42200 | train_loss 1.9499 | val_loss 2.0631 | lr 2.946468e-05\n",
      "step 42400 | train_loss 1.9512 | val_loss 2.0629 | lr 2.800151e-05\n",
      "step 42600 | train_loss 1.9519 | val_loss 2.0625 | lr 2.657345e-05\n",
      "step 42800 | train_loss 1.9529 | val_loss 2.0617 | lr 2.518072e-05\n",
      "step 43000 | train_loss 1.9528 | val_loss 2.0618 | lr 2.382354e-05\n",
      "step 43200 | train_loss 1.9534 | val_loss 2.0623 | lr 2.250213e-05\n",
      "step 43400 | train_loss 1.9551 | val_loss 2.0622 | lr 2.121670e-05\n",
      "step 43600 | train_loss 1.9498 | val_loss 2.0620 | lr 1.996744e-05\n",
      "step 43800 | train_loss 1.9505 | val_loss 2.0623 | lr 1.875456e-05\n",
      "step 44000 | train_loss 1.9509 | val_loss 2.0624 | lr 1.757824e-05\n",
      "step 44200 | train_loss 1.9511 | val_loss 2.0624 | lr 1.643868e-05\n",
      "step 44400 | train_loss 1.9517 | val_loss 2.0619 | lr 1.533604e-05\n",
      "step 44600 | train_loss 1.9522 | val_loss 2.0618 | lr 1.427052e-05\n",
      "step 44800 | train_loss 1.9523 | val_loss 2.0619 | lr 1.324227e-05\n",
      "step 45000 | train_loss 1.9531 | val_loss 2.0620 | lr 1.225146e-05\n",
      "step 45200 | train_loss 1.9501 | val_loss 2.0619 | lr 1.129824e-05\n",
      "step 45400 | train_loss 1.9508 | val_loss 2.0619 | lr 1.038276e-05\n",
      "step 45600 | train_loss 1.9508 | val_loss 2.0620 | lr 9.505174e-06\n",
      "step 45800 | train_loss 1.9510 | val_loss 2.0622 | lr 8.665614e-06\n",
      "step 46000 | train_loss 1.9512 | val_loss 2.0619 | lr 7.864214e-06\n",
      "step 46200 | train_loss 1.9515 | val_loss 2.0618 | lr 7.101100e-06\n",
      "step 46400 | train_loss 1.9514 | val_loss 2.0618 | lr 6.376393e-06\n",
      "step 46600 | train_loss 1.9517 | val_loss 2.0619 | lr 5.690208e-06\n",
      "step 46800 | train_loss 1.9508 | val_loss 2.0619 | lr 5.042652e-06\n",
      "step 47000 | train_loss 1.9508 | val_loss 2.0618 | lr 4.433828e-06\n",
      "step 47200 | train_loss 1.9509 | val_loss 2.0619 | lr 3.863832e-06\n",
      "step 47400 | train_loss 1.9510 | val_loss 2.0619 | lr 3.332754e-06\n",
      "step 47600 | train_loss 1.9511 | val_loss 2.0619 | lr 2.840677e-06\n",
      "step 47800 | train_loss 1.9512 | val_loss 2.0618 | lr 2.387681e-06\n",
      "step 48000 | train_loss 1.9512 | val_loss 2.0618 | lr 1.973836e-06\n",
      "step 48200 | train_loss 1.9512 | val_loss 2.0619 | lr 1.599207e-06\n",
      "step 48400 | train_loss 1.9514 | val_loss 2.0619 | lr 1.263854e-06\n",
      "step 48600 | train_loss 1.9511 | val_loss 2.0618 | lr 9.678289e-07\n",
      "step 48800 | train_loss 1.9511 | val_loss 2.0619 | lr 7.111796e-07\n",
      "step 49000 | train_loss 1.9511 | val_loss 2.0619 | lr 4.939462e-07\n",
      "step 49200 | train_loss 1.9511 | val_loss 2.0619 | lr 3.161630e-07\n",
      "step 49400 | train_loss 1.9511 | val_loss 2.0619 | lr 1.778581e-07\n",
      "step 49600 | train_loss 1.9511 | val_loss 2.0619 | lr 7.905324e-08\n",
      "step 49800 | train_loss 1.9511 | val_loss 2.0619 | lr 1.976409e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:05:26,384] Trial 4 finished with value: 2.0618603144373213 and parameters: {'embedding_size': 27, 'hidden_size': 824, 'learning_rate': 0.0005006368655812558, 'batch_size': 128, 'context_length': 3}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.9511 | val_loss 2.0619 | lr 4.941088e-13\n",
      "step 0 | train_loss 3.3278 | val_loss 3.3295 | lr 2.115794e-04\n",
      "step 200 | train_loss 2.5743 | val_loss 2.5863 | lr 2.115710e-04\n",
      "step 400 | train_loss 2.4985 | val_loss 2.4893 | lr 2.115460e-04\n",
      "step 600 | train_loss 2.4705 | val_loss 2.4537 | lr 2.115042e-04\n",
      "step 800 | train_loss 2.4455 | val_loss 2.4276 | lr 2.114458e-04\n",
      "step 1000 | train_loss 2.4263 | val_loss 2.4080 | lr 2.113706e-04\n",
      "step 1200 | train_loss 2.3916 | val_loss 2.3959 | lr 2.112788e-04\n",
      "step 1400 | train_loss 2.3933 | val_loss 2.3887 | lr 2.111703e-04\n",
      "step 1600 | train_loss 2.3917 | val_loss 2.3744 | lr 2.110452e-04\n",
      "step 1800 | train_loss 2.3851 | val_loss 2.3737 | lr 2.109035e-04\n",
      "step 2000 | train_loss 2.3820 | val_loss 2.3665 | lr 2.107452e-04\n",
      "step 2200 | train_loss 2.3794 | val_loss 2.3628 | lr 2.105703e-04\n",
      "step 2400 | train_loss 2.3584 | val_loss 2.3553 | lr 2.103788e-04\n",
      "step 2600 | train_loss 2.3580 | val_loss 2.3482 | lr 2.101709e-04\n",
      "step 2800 | train_loss 2.3447 | val_loss 2.3419 | lr 2.099464e-04\n",
      "step 3000 | train_loss 2.3430 | val_loss 2.3375 | lr 2.097055e-04\n",
      "step 3200 | train_loss 2.3481 | val_loss 2.3364 | lr 2.094482e-04\n",
      "step 3400 | train_loss 2.3334 | val_loss 2.3338 | lr 2.091746e-04\n",
      "step 3600 | train_loss 2.3414 | val_loss 2.3277 | lr 2.088846e-04\n",
      "step 3800 | train_loss 2.3325 | val_loss 2.3288 | lr 2.085783e-04\n",
      "step 4000 | train_loss 2.3329 | val_loss 2.3212 | lr 2.082558e-04\n",
      "step 4200 | train_loss 2.3317 | val_loss 2.3182 | lr 2.079171e-04\n",
      "step 4400 | train_loss 2.3271 | val_loss 2.3169 | lr 2.075623e-04\n",
      "step 4600 | train_loss 2.3244 | val_loss 2.3134 | lr 2.071914e-04\n",
      "step 4800 | train_loss 2.3364 | val_loss 2.3136 | lr 2.068045e-04\n",
      "step 5000 | train_loss 2.3182 | val_loss 2.3109 | lr 2.064017e-04\n",
      "step 5200 | train_loss 2.3135 | val_loss 2.3084 | lr 2.059829e-04\n",
      "step 5400 | train_loss 2.3300 | val_loss 2.3066 | lr 2.055484e-04\n",
      "step 5600 | train_loss 2.3134 | val_loss 2.3049 | lr 2.050980e-04\n",
      "step 5800 | train_loss 2.3229 | val_loss 2.3071 | lr 2.046321e-04\n",
      "step 6000 | train_loss 2.3196 | val_loss 2.3011 | lr 2.041504e-04\n",
      "step 6200 | train_loss 2.3278 | val_loss 2.3018 | lr 2.036533e-04\n",
      "step 6400 | train_loss 2.3163 | val_loss 2.2959 | lr 2.031407e-04\n",
      "step 6600 | train_loss 2.3150 | val_loss 2.2943 | lr 2.026128e-04\n",
      "step 6800 | train_loss 2.2710 | val_loss 2.2969 | lr 2.020695e-04\n",
      "step 7000 | train_loss 2.2959 | val_loss 2.2955 | lr 2.015111e-04\n",
      "step 7200 | train_loss 2.3061 | val_loss 2.2935 | lr 2.009375e-04\n",
      "step 7400 | train_loss 2.3068 | val_loss 2.2961 | lr 2.003489e-04\n",
      "step 7600 | train_loss 2.2984 | val_loss 2.2892 | lr 1.997454e-04\n",
      "step 7800 | train_loss 2.2866 | val_loss 2.2898 | lr 1.991270e-04\n",
      "step 8000 | train_loss 2.2857 | val_loss 2.2873 | lr 1.984939e-04\n",
      "step 8200 | train_loss 2.2911 | val_loss 2.2862 | lr 1.978462e-04\n",
      "step 8400 | train_loss 2.2968 | val_loss 2.2899 | lr 1.971839e-04\n",
      "step 8600 | train_loss 2.2951 | val_loss 2.2857 | lr 1.965072e-04\n",
      "step 8800 | train_loss 2.2979 | val_loss 2.2855 | lr 1.958161e-04\n",
      "step 9000 | train_loss 2.2956 | val_loss 2.2859 | lr 1.951109e-04\n",
      "step 9200 | train_loss 2.2863 | val_loss 2.2865 | lr 1.943915e-04\n",
      "step 9400 | train_loss 2.2849 | val_loss 2.2850 | lr 1.936582e-04\n",
      "step 9600 | train_loss 2.2817 | val_loss 2.2825 | lr 1.929109e-04\n",
      "step 9800 | train_loss 2.2919 | val_loss 2.2788 | lr 1.921500e-04\n",
      "step 10000 | train_loss 2.2833 | val_loss 2.2834 | lr 1.913753e-04\n",
      "step 10200 | train_loss 2.2872 | val_loss 2.2798 | lr 1.905872e-04\n",
      "step 10400 | train_loss 2.2951 | val_loss 2.2795 | lr 1.897857e-04\n",
      "step 10600 | train_loss 2.2851 | val_loss 2.2780 | lr 1.889709e-04\n",
      "step 10800 | train_loss 2.2908 | val_loss 2.2756 | lr 1.881430e-04\n",
      "step 11000 | train_loss 2.2852 | val_loss 2.2750 | lr 1.873020e-04\n",
      "step 11200 | train_loss 2.2825 | val_loss 2.2758 | lr 1.864482e-04\n",
      "step 11400 | train_loss 2.2899 | val_loss 2.2754 | lr 1.855817e-04\n",
      "step 11600 | train_loss 2.2856 | val_loss 2.2745 | lr 1.847026e-04\n",
      "step 11800 | train_loss 2.2862 | val_loss 2.2771 | lr 1.838110e-04\n",
      "step 12000 | train_loss 2.2920 | val_loss 2.2787 | lr 1.829070e-04\n",
      "step 12200 | train_loss 2.2843 | val_loss 2.2749 | lr 1.819910e-04\n",
      "step 12400 | train_loss 2.2827 | val_loss 2.2762 | lr 1.810628e-04\n",
      "step 12600 | train_loss 2.2839 | val_loss 2.2731 | lr 1.801228e-04\n",
      "step 12800 | train_loss 2.2967 | val_loss 2.2745 | lr 1.791711e-04\n",
      "step 13000 | train_loss 2.2887 | val_loss 2.2737 | lr 1.782077e-04\n",
      "step 13200 | train_loss 2.2845 | val_loss 2.2708 | lr 1.772329e-04\n",
      "step 13400 | train_loss 2.2385 | val_loss 2.2710 | lr 1.762469e-04\n",
      "step 13600 | train_loss 2.2585 | val_loss 2.2708 | lr 1.752497e-04\n",
      "step 13800 | train_loss 2.2693 | val_loss 2.2718 | lr 1.742415e-04\n",
      "step 14000 | train_loss 2.2767 | val_loss 2.2694 | lr 1.732226e-04\n",
      "step 14200 | train_loss 2.2749 | val_loss 2.2733 | lr 1.721930e-04\n",
      "step 14400 | train_loss 2.2753 | val_loss 2.2693 | lr 1.711529e-04\n",
      "step 14600 | train_loss 2.2544 | val_loss 2.2701 | lr 1.701024e-04\n",
      "step 14800 | train_loss 2.2675 | val_loss 2.2664 | lr 1.690419e-04\n",
      "step 15000 | train_loss 2.2733 | val_loss 2.2685 | lr 1.679713e-04\n",
      "step 15200 | train_loss 2.2718 | val_loss 2.2686 | lr 1.668909e-04\n",
      "step 15400 | train_loss 2.2786 | val_loss 2.2664 | lr 1.658009e-04\n",
      "step 15600 | train_loss 2.2753 | val_loss 2.2678 | lr 1.647014e-04\n",
      "step 15800 | train_loss 2.2697 | val_loss 2.2697 | lr 1.635926e-04\n",
      "step 16000 | train_loss 2.2701 | val_loss 2.2698 | lr 1.624746e-04\n",
      "step 16200 | train_loss 2.2628 | val_loss 2.2652 | lr 1.613477e-04\n",
      "step 16400 | train_loss 2.2662 | val_loss 2.2636 | lr 1.602121e-04\n",
      "step 16600 | train_loss 2.2713 | val_loss 2.2665 | lr 1.590678e-04\n",
      "step 16800 | train_loss 2.2683 | val_loss 2.2646 | lr 1.579152e-04\n",
      "step 17000 | train_loss 2.2713 | val_loss 2.2638 | lr 1.567543e-04\n",
      "step 17200 | train_loss 2.2602 | val_loss 2.2636 | lr 1.555853e-04\n",
      "step 17400 | train_loss 2.2694 | val_loss 2.2627 | lr 1.544085e-04\n",
      "step 17600 | train_loss 2.2722 | val_loss 2.2604 | lr 1.532240e-04\n",
      "step 17800 | train_loss 2.2670 | val_loss 2.2641 | lr 1.520320e-04\n",
      "step 18000 | train_loss 2.2676 | val_loss 2.2621 | lr 1.508327e-04\n",
      "step 18200 | train_loss 2.2734 | val_loss 2.2639 | lr 1.496263e-04\n",
      "step 18400 | train_loss 2.2653 | val_loss 2.2627 | lr 1.484130e-04\n",
      "step 18600 | train_loss 2.2673 | val_loss 2.2620 | lr 1.471930e-04\n",
      "step 18800 | train_loss 2.2704 | val_loss 2.2599 | lr 1.459664e-04\n",
      "step 19000 | train_loss 2.2618 | val_loss 2.2620 | lr 1.447335e-04\n",
      "step 19200 | train_loss 2.2696 | val_loss 2.2642 | lr 1.434944e-04\n",
      "step 19400 | train_loss 2.2713 | val_loss 2.2605 | lr 1.422494e-04\n",
      "step 19600 | train_loss 2.2741 | val_loss 2.2606 | lr 1.409986e-04\n",
      "step 19800 | train_loss 2.2724 | val_loss 2.2618 | lr 1.397422e-04\n",
      "step 20000 | train_loss 2.2696 | val_loss 2.2582 | lr 1.384805e-04\n",
      "step 20200 | train_loss 2.2363 | val_loss 2.2597 | lr 1.372136e-04\n",
      "step 20400 | train_loss 2.2529 | val_loss 2.2590 | lr 1.359418e-04\n",
      "step 20600 | train_loss 2.2601 | val_loss 2.2590 | lr 1.346652e-04\n",
      "step 20800 | train_loss 2.2663 | val_loss 2.2615 | lr 1.333840e-04\n",
      "step 21000 | train_loss 2.2572 | val_loss 2.2571 | lr 1.320985e-04\n",
      "step 21200 | train_loss 2.2461 | val_loss 2.2567 | lr 1.308088e-04\n",
      "step 21400 | train_loss 2.2464 | val_loss 2.2583 | lr 1.295152e-04\n",
      "step 21600 | train_loss 2.2556 | val_loss 2.2570 | lr 1.282179e-04\n",
      "step 21800 | train_loss 2.2611 | val_loss 2.2604 | lr 1.269169e-04\n",
      "step 22000 | train_loss 2.2643 | val_loss 2.2567 | lr 1.256127e-04\n",
      "step 22200 | train_loss 2.2607 | val_loss 2.2587 | lr 1.243053e-04\n",
      "step 22400 | train_loss 2.2570 | val_loss 2.2593 | lr 1.229950e-04\n",
      "step 22600 | train_loss 2.2535 | val_loss 2.2574 | lr 1.216820e-04\n",
      "step 22800 | train_loss 2.2455 | val_loss 2.2566 | lr 1.203665e-04\n",
      "step 23000 | train_loss 2.2498 | val_loss 2.2549 | lr 1.190487e-04\n",
      "step 23200 | train_loss 2.2565 | val_loss 2.2558 | lr 1.177287e-04\n",
      "step 23400 | train_loss 2.2546 | val_loss 2.2569 | lr 1.164069e-04\n",
      "step 23600 | train_loss 2.2549 | val_loss 2.2551 | lr 1.150834e-04\n",
      "step 23800 | train_loss 2.2578 | val_loss 2.2548 | lr 1.137585e-04\n",
      "step 24000 | train_loss 2.2579 | val_loss 2.2533 | lr 1.124323e-04\n",
      "step 24200 | train_loss 2.2584 | val_loss 2.2530 | lr 1.111050e-04\n",
      "step 24400 | train_loss 2.2563 | val_loss 2.2535 | lr 1.097769e-04\n",
      "step 24600 | train_loss 2.2529 | val_loss 2.2528 | lr 1.084482e-04\n",
      "step 24800 | train_loss 2.2607 | val_loss 2.2537 | lr 1.071190e-04\n",
      "step 25000 | train_loss 2.2539 | val_loss 2.2531 | lr 1.057897e-04\n",
      "step 25200 | train_loss 2.2594 | val_loss 2.2548 | lr 1.044603e-04\n",
      "step 25400 | train_loss 2.2608 | val_loss 2.2547 | lr 1.031312e-04\n",
      "step 25600 | train_loss 2.2549 | val_loss 2.2530 | lr 1.018025e-04\n",
      "step 25800 | train_loss 2.2556 | val_loss 2.2552 | lr 1.004744e-04\n",
      "step 26000 | train_loss 2.2582 | val_loss 2.2531 | lr 9.914710e-05\n",
      "step 26200 | train_loss 2.2652 | val_loss 2.2533 | lr 9.782089e-05\n",
      "step 26400 | train_loss 2.2629 | val_loss 2.2523 | lr 9.649594e-05\n",
      "step 26600 | train_loss 2.2566 | val_loss 2.2514 | lr 9.517245e-05\n",
      "step 26800 | train_loss 2.2310 | val_loss 2.2514 | lr 9.385065e-05\n",
      "step 27000 | train_loss 2.2402 | val_loss 2.2515 | lr 9.253072e-05\n",
      "step 27200 | train_loss 2.2484 | val_loss 2.2520 | lr 9.121290e-05\n",
      "step 27400 | train_loss 2.2488 | val_loss 2.2512 | lr 8.989737e-05\n",
      "step 27600 | train_loss 2.2504 | val_loss 2.2524 | lr 8.858435e-05\n",
      "step 27800 | train_loss 2.2490 | val_loss 2.2512 | lr 8.727405e-05\n",
      "step 28000 | train_loss 2.2424 | val_loss 2.2517 | lr 8.596668e-05\n",
      "step 28200 | train_loss 2.2467 | val_loss 2.2500 | lr 8.466243e-05\n",
      "step 28400 | train_loss 2.2508 | val_loss 2.2519 | lr 8.336152e-05\n",
      "step 28600 | train_loss 2.2509 | val_loss 2.2518 | lr 8.206415e-05\n",
      "step 28800 | train_loss 2.2550 | val_loss 2.2504 | lr 8.077053e-05\n",
      "step 29000 | train_loss 2.2543 | val_loss 2.2517 | lr 7.948086e-05\n",
      "step 29200 | train_loss 2.2493 | val_loss 2.2518 | lr 7.819535e-05\n",
      "step 29400 | train_loss 2.2472 | val_loss 2.2521 | lr 7.691419e-05\n",
      "step 29600 | train_loss 2.2440 | val_loss 2.2502 | lr 7.563759e-05\n",
      "step 29800 | train_loss 2.2477 | val_loss 2.2490 | lr 7.436575e-05\n",
      "step 30000 | train_loss 2.2500 | val_loss 2.2505 | lr 7.309888e-05\n",
      "step 30200 | train_loss 2.2477 | val_loss 2.2489 | lr 7.183716e-05\n",
      "step 30400 | train_loss 2.2491 | val_loss 2.2480 | lr 7.058081e-05\n",
      "step 30600 | train_loss 2.2458 | val_loss 2.2484 | lr 6.933002e-05\n",
      "step 30800 | train_loss 2.2495 | val_loss 2.2482 | lr 6.808499e-05\n",
      "step 31000 | train_loss 2.2513 | val_loss 2.2471 | lr 6.684591e-05\n",
      "step 31200 | train_loss 2.2476 | val_loss 2.2495 | lr 6.561298e-05\n",
      "step 31400 | train_loss 2.2485 | val_loss 2.2485 | lr 6.438639e-05\n",
      "step 31600 | train_loss 2.2521 | val_loss 2.2483 | lr 6.316634e-05\n",
      "step 31800 | train_loss 2.2494 | val_loss 2.2477 | lr 6.195302e-05\n",
      "step 32000 | train_loss 2.2504 | val_loss 2.2484 | lr 6.074663e-05\n",
      "step 32200 | train_loss 2.2509 | val_loss 2.2472 | lr 5.954735e-05\n",
      "step 32400 | train_loss 2.2471 | val_loss 2.2479 | lr 5.835537e-05\n",
      "step 32600 | train_loss 2.2514 | val_loss 2.2491 | lr 5.717088e-05\n",
      "step 32800 | train_loss 2.2503 | val_loss 2.2474 | lr 5.599407e-05\n",
      "step 33000 | train_loss 2.2542 | val_loss 2.2480 | lr 5.482512e-05\n",
      "step 33200 | train_loss 2.2542 | val_loss 2.2482 | lr 5.366422e-05\n",
      "step 33400 | train_loss 2.2514 | val_loss 2.2464 | lr 5.251155e-05\n",
      "step 33600 | train_loss 2.2358 | val_loss 2.2473 | lr 5.136729e-05\n",
      "step 33800 | train_loss 2.2400 | val_loss 2.2463 | lr 5.023163e-05\n",
      "step 34000 | train_loss 2.2438 | val_loss 2.2469 | lr 4.910474e-05\n",
      "step 34200 | train_loss 2.2467 | val_loss 2.2471 | lr 4.798680e-05\n",
      "step 34400 | train_loss 2.2459 | val_loss 2.2462 | lr 4.687799e-05\n",
      "step 34600 | train_loss 2.2392 | val_loss 2.2466 | lr 4.577848e-05\n",
      "step 34800 | train_loss 2.2403 | val_loss 2.2473 | lr 4.468845e-05\n",
      "step 35000 | train_loss 2.2434 | val_loss 2.2463 | lr 4.360807e-05\n",
      "step 35200 | train_loss 2.2447 | val_loss 2.2477 | lr 4.253751e-05\n",
      "step 35400 | train_loss 2.2465 | val_loss 2.2467 | lr 4.147693e-05\n",
      "step 35600 | train_loss 2.2469 | val_loss 2.2475 | lr 4.042651e-05\n",
      "step 35800 | train_loss 2.2450 | val_loss 2.2477 | lr 3.938641e-05\n",
      "step 36000 | train_loss 2.2450 | val_loss 2.2470 | lr 3.835680e-05\n",
      "step 36200 | train_loss 2.2414 | val_loss 2.2467 | lr 3.733784e-05\n",
      "step 36400 | train_loss 2.2405 | val_loss 2.2459 | lr 3.632969e-05\n",
      "step 36600 | train_loss 2.2424 | val_loss 2.2457 | lr 3.533250e-05\n",
      "step 36800 | train_loss 2.2410 | val_loss 2.2460 | lr 3.434644e-05\n",
      "step 37000 | train_loss 2.2418 | val_loss 2.2453 | lr 3.337166e-05\n",
      "step 37200 | train_loss 2.2427 | val_loss 2.2455 | lr 3.240832e-05\n",
      "step 37400 | train_loss 2.2434 | val_loss 2.2448 | lr 3.145657e-05\n",
      "step 37600 | train_loss 2.2446 | val_loss 2.2448 | lr 3.051655e-05\n",
      "step 37800 | train_loss 2.2439 | val_loss 2.2447 | lr 2.958842e-05\n",
      "step 38000 | train_loss 2.2436 | val_loss 2.2450 | lr 2.867232e-05\n",
      "step 38200 | train_loss 2.2464 | val_loss 2.2452 | lr 2.776841e-05\n",
      "step 38400 | train_loss 2.2436 | val_loss 2.2447 | lr 2.687681e-05\n",
      "step 38600 | train_loss 2.2447 | val_loss 2.2449 | lr 2.599767e-05\n",
      "step 38800 | train_loss 2.2468 | val_loss 2.2448 | lr 2.513113e-05\n",
      "step 39000 | train_loss 2.2444 | val_loss 2.2445 | lr 2.427733e-05\n",
      "step 39200 | train_loss 2.2450 | val_loss 2.2451 | lr 2.343640e-05\n",
      "step 39400 | train_loss 2.2457 | val_loss 2.2449 | lr 2.260848e-05\n",
      "step 39600 | train_loss 2.2481 | val_loss 2.2445 | lr 2.179369e-05\n",
      "step 39800 | train_loss 2.2471 | val_loss 2.2442 | lr 2.099217e-05\n",
      "step 40000 | train_loss 2.2473 | val_loss 2.2442 | lr 2.020403e-05\n",
      "step 40200 | train_loss 2.2399 | val_loss 2.2444 | lr 1.942941e-05\n",
      "step 40400 | train_loss 2.2408 | val_loss 2.2442 | lr 1.866843e-05\n",
      "step 40600 | train_loss 2.2418 | val_loss 2.2440 | lr 1.792121e-05\n",
      "step 40800 | train_loss 2.2424 | val_loss 2.2442 | lr 1.718786e-05\n",
      "step 41000 | train_loss 2.2425 | val_loss 2.2441 | lr 1.646850e-05\n",
      "step 41200 | train_loss 2.2423 | val_loss 2.2442 | lr 1.576325e-05\n",
      "step 41400 | train_loss 2.2410 | val_loss 2.2441 | lr 1.507221e-05\n",
      "step 41600 | train_loss 2.2412 | val_loss 2.2440 | lr 1.439550e-05\n",
      "step 41800 | train_loss 2.2416 | val_loss 2.2442 | lr 1.373322e-05\n",
      "step 42000 | train_loss 2.2419 | val_loss 2.2443 | lr 1.308548e-05\n",
      "step 42200 | train_loss 2.2428 | val_loss 2.2442 | lr 1.245237e-05\n",
      "step 42400 | train_loss 2.2427 | val_loss 2.2444 | lr 1.183401e-05\n",
      "step 42600 | train_loss 2.2421 | val_loss 2.2445 | lr 1.123048e-05\n",
      "step 42800 | train_loss 2.2421 | val_loss 2.2445 | lr 1.064189e-05\n",
      "step 43000 | train_loss 2.2411 | val_loss 2.2443 | lr 1.006832e-05\n",
      "step 43200 | train_loss 2.2413 | val_loss 2.2439 | lr 9.509861e-06\n",
      "step 43400 | train_loss 2.2410 | val_loss 2.2440 | lr 8.966610e-06\n",
      "step 43600 | train_loss 2.2406 | val_loss 2.2438 | lr 8.438648e-06\n",
      "step 43800 | train_loss 2.2408 | val_loss 2.2435 | lr 7.926059e-06\n",
      "step 44000 | train_loss 2.2404 | val_loss 2.2435 | lr 7.428924e-06\n",
      "step 44200 | train_loss 2.2411 | val_loss 2.2435 | lr 6.947321e-06\n",
      "step 44400 | train_loss 2.2412 | val_loss 2.2434 | lr 6.481326e-06\n",
      "step 44600 | train_loss 2.2410 | val_loss 2.2436 | lr 6.031013e-06\n",
      "step 44800 | train_loss 2.2412 | val_loss 2.2436 | lr 5.596454e-06\n",
      "step 45000 | train_loss 2.2415 | val_loss 2.2435 | lr 5.177716e-06\n",
      "step 45200 | train_loss 2.2414 | val_loss 2.2435 | lr 4.774866e-06\n",
      "step 45400 | train_loss 2.2418 | val_loss 2.2436 | lr 4.387967e-06\n",
      "step 45600 | train_loss 2.2419 | val_loss 2.2435 | lr 4.017081e-06\n",
      "step 45800 | train_loss 2.2417 | val_loss 2.2436 | lr 3.662266e-06\n",
      "step 46000 | train_loss 2.2418 | val_loss 2.2436 | lr 3.323578e-06\n",
      "step 46200 | train_loss 2.2422 | val_loss 2.2436 | lr 3.001070e-06\n",
      "step 46400 | train_loss 2.2423 | val_loss 2.2435 | lr 2.694794e-06\n",
      "step 46600 | train_loss 2.2424 | val_loss 2.2435 | lr 2.404798e-06\n",
      "step 46800 | train_loss 2.2418 | val_loss 2.2435 | lr 2.131128e-06\n",
      "step 47000 | train_loss 2.2416 | val_loss 2.2435 | lr 1.873826e-06\n",
      "step 47200 | train_loss 2.2417 | val_loss 2.2435 | lr 1.632934e-06\n",
      "step 47400 | train_loss 2.2418 | val_loss 2.2435 | lr 1.408490e-06\n",
      "step 47600 | train_loss 2.2419 | val_loss 2.2435 | lr 1.200528e-06\n",
      "step 47800 | train_loss 2.2419 | val_loss 2.2435 | lr 1.009083e-06\n",
      "step 48000 | train_loss 2.2418 | val_loss 2.2435 | lr 8.341833e-07\n",
      "step 48200 | train_loss 2.2418 | val_loss 2.2435 | lr 6.758575e-07\n",
      "step 48400 | train_loss 2.2418 | val_loss 2.2435 | lr 5.341303e-07\n",
      "step 48600 | train_loss 2.2418 | val_loss 2.2435 | lr 4.090243e-07\n",
      "step 48800 | train_loss 2.2418 | val_loss 2.2435 | lr 3.005591e-07\n",
      "step 49000 | train_loss 2.2418 | val_loss 2.2435 | lr 2.087518e-07\n",
      "step 49200 | train_loss 2.2418 | val_loss 2.2435 | lr 1.336170e-07\n",
      "step 49400 | train_loss 2.2418 | val_loss 2.2435 | lr 7.516647e-08\n",
      "step 49600 | train_loss 2.2418 | val_loss 2.2435 | lr 3.340952e-08\n",
      "step 49800 | train_loss 2.2418 | val_loss 2.2435 | lr 8.352709e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:06:29,167] Trial 5 finished with value: 2.243515248809542 and parameters: {'embedding_size': 51, 'hidden_size': 597, 'learning_rate': 0.00021157937340316298, 'batch_size': 32, 'context_length': 2}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.2418 | val_loss 2.2435 | lr 2.088205e-13\n",
      "step 0 | train_loss 3.3253 | val_loss 3.3269 | lr 4.124402e-03\n",
      "step 200 | train_loss 2.3704 | val_loss 2.3981 | lr 4.124240e-03\n",
      "step 400 | train_loss 2.3647 | val_loss 2.3683 | lr 4.123751e-03\n",
      "step 600 | train_loss 2.3279 | val_loss 2.3587 | lr 4.122937e-03\n",
      "step 800 | train_loss 2.3119 | val_loss 2.3292 | lr 4.121798e-03\n",
      "step 1000 | train_loss 2.3239 | val_loss 2.3418 | lr 4.120333e-03\n",
      "step 1200 | train_loss 2.3158 | val_loss 2.3335 | lr 4.118543e-03\n",
      "step 1400 | train_loss 2.3012 | val_loss 2.3168 | lr 4.116429e-03\n",
      "step 1600 | train_loss 2.3360 | val_loss 2.3280 | lr 4.113990e-03\n",
      "step 1800 | train_loss 2.2562 | val_loss 2.3102 | lr 4.111228e-03\n",
      "step 2000 | train_loss 2.2574 | val_loss 2.2928 | lr 4.108141e-03\n",
      "step 2200 | train_loss 2.2803 | val_loss 2.3061 | lr 4.104732e-03\n",
      "step 2400 | train_loss 2.2669 | val_loss 2.2976 | lr 4.101000e-03\n",
      "step 2600 | train_loss 2.2904 | val_loss 2.3113 | lr 4.096946e-03\n",
      "step 2800 | train_loss 2.3025 | val_loss 2.3157 | lr 4.092571e-03\n",
      "step 3000 | train_loss 2.3077 | val_loss 2.3305 | lr 4.087875e-03\n",
      "step 3200 | train_loss 2.2981 | val_loss 2.2999 | lr 4.082859e-03\n",
      "step 3400 | train_loss 2.1967 | val_loss 2.3063 | lr 4.077525e-03\n",
      "step 3600 | train_loss 2.2822 | val_loss 2.3133 | lr 4.071872e-03\n",
      "step 3800 | train_loss 2.2784 | val_loss 2.3009 | lr 4.065901e-03\n",
      "step 4000 | train_loss 2.2729 | val_loss 2.3131 | lr 4.059614e-03\n",
      "step 4200 | train_loss 2.2907 | val_loss 2.3033 | lr 4.053012e-03\n",
      "step 4400 | train_loss 2.2663 | val_loss 2.2988 | lr 4.046096e-03\n",
      "step 4600 | train_loss 2.2758 | val_loss 2.2979 | lr 4.038866e-03\n",
      "step 4800 | train_loss 2.3014 | val_loss 2.3278 | lr 4.031324e-03\n",
      "step 5000 | train_loss 2.2832 | val_loss 2.2936 | lr 4.023471e-03\n",
      "step 5200 | train_loss 2.2515 | val_loss 2.2993 | lr 4.015308e-03\n",
      "step 5400 | train_loss 2.2469 | val_loss 2.2751 | lr 4.006837e-03\n",
      "step 5600 | train_loss 2.2490 | val_loss 2.3029 | lr 3.998059e-03\n",
      "step 5800 | train_loss 2.2848 | val_loss 2.3070 | lr 3.988975e-03\n",
      "step 6000 | train_loss 2.2745 | val_loss 2.2897 | lr 3.979587e-03\n",
      "step 6200 | train_loss 2.3013 | val_loss 2.3163 | lr 3.969896e-03\n",
      "step 6400 | train_loss 2.2682 | val_loss 2.2895 | lr 3.959904e-03\n",
      "step 6600 | train_loss 2.2883 | val_loss 2.2999 | lr 3.949613e-03\n",
      "step 6800 | train_loss 2.2454 | val_loss 2.2893 | lr 3.939023e-03\n",
      "step 7000 | train_loss 2.2564 | val_loss 2.2889 | lr 3.928137e-03\n",
      "step 7200 | train_loss 2.2600 | val_loss 2.2877 | lr 3.916956e-03\n",
      "step 7400 | train_loss 2.2593 | val_loss 2.2891 | lr 3.905482e-03\n",
      "step 7600 | train_loss 2.2939 | val_loss 2.3100 | lr 3.893717e-03\n",
      "step 7800 | train_loss 2.2683 | val_loss 2.3036 | lr 3.881663e-03\n",
      "step 8000 | train_loss 2.2739 | val_loss 2.3019 | lr 3.869322e-03\n",
      "step 8200 | train_loss 2.2689 | val_loss 2.2958 | lr 3.856695e-03\n",
      "step 8400 | train_loss 2.2063 | val_loss 2.3137 | lr 3.843785e-03\n",
      "step 8600 | train_loss 2.2548 | val_loss 2.2933 | lr 3.830594e-03\n",
      "step 8800 | train_loss 2.2658 | val_loss 2.3111 | lr 3.817123e-03\n",
      "step 9000 | train_loss 2.2520 | val_loss 2.2818 | lr 3.803375e-03\n",
      "step 9200 | train_loss 2.2654 | val_loss 2.2863 | lr 3.789352e-03\n",
      "step 9400 | train_loss 2.2613 | val_loss 2.2919 | lr 3.775057e-03\n",
      "step 9600 | train_loss 2.2483 | val_loss 2.2781 | lr 3.760491e-03\n",
      "step 9800 | train_loss 2.2813 | val_loss 2.3077 | lr 3.745657e-03\n",
      "step 10000 | train_loss 2.2675 | val_loss 2.2885 | lr 3.730557e-03\n",
      "step 10200 | train_loss 2.2285 | val_loss 2.2789 | lr 3.715194e-03\n",
      "step 10400 | train_loss 2.2360 | val_loss 2.2778 | lr 3.699569e-03\n",
      "step 10600 | train_loss 2.2497 | val_loss 2.2919 | lr 3.683686e-03\n",
      "step 10800 | train_loss 2.2561 | val_loss 2.2823 | lr 3.667547e-03\n",
      "step 11000 | train_loss 2.2520 | val_loss 2.2960 | lr 3.651154e-03\n",
      "step 11200 | train_loss 2.2761 | val_loss 2.2987 | lr 3.634511e-03\n",
      "step 11400 | train_loss 2.2680 | val_loss 2.3041 | lr 3.617619e-03\n",
      "step 11600 | train_loss 2.2657 | val_loss 2.2809 | lr 3.600482e-03\n",
      "step 11800 | train_loss 2.2268 | val_loss 2.2966 | lr 3.583102e-03\n",
      "step 12000 | train_loss 2.2306 | val_loss 2.2715 | lr 3.565481e-03\n",
      "step 12200 | train_loss 2.2517 | val_loss 2.2829 | lr 3.547623e-03\n",
      "step 12400 | train_loss 2.2587 | val_loss 2.2914 | lr 3.529531e-03\n",
      "step 12600 | train_loss 2.2709 | val_loss 2.2853 | lr 3.511207e-03\n",
      "step 12800 | train_loss 2.2609 | val_loss 2.2942 | lr 3.492654e-03\n",
      "step 13000 | train_loss 2.2592 | val_loss 2.2985 | lr 3.473875e-03\n",
      "step 13200 | train_loss 2.2583 | val_loss 2.2877 | lr 3.454873e-03\n",
      "step 13400 | train_loss 2.1824 | val_loss 2.2830 | lr 3.435652e-03\n",
      "step 13600 | train_loss 2.2264 | val_loss 2.2755 | lr 3.416213e-03\n",
      "step 13800 | train_loss 2.2377 | val_loss 2.2805 | lr 3.396561e-03\n",
      "step 14000 | train_loss 2.2447 | val_loss 2.2753 | lr 3.376698e-03\n",
      "step 14200 | train_loss 2.2514 | val_loss 2.2806 | lr 3.356627e-03\n",
      "step 14400 | train_loss 2.2561 | val_loss 2.2885 | lr 3.336352e-03\n",
      "step 14600 | train_loss 2.2477 | val_loss 2.2761 | lr 3.315876e-03\n",
      "step 14800 | train_loss 2.2394 | val_loss 2.2786 | lr 3.295202e-03\n",
      "step 15000 | train_loss 2.2762 | val_loss 2.2924 | lr 3.274333e-03\n",
      "step 15200 | train_loss 2.2290 | val_loss 2.2786 | lr 3.253272e-03\n",
      "step 15400 | train_loss 2.2222 | val_loss 2.2646 | lr 3.232024e-03\n",
      "step 15600 | train_loss 2.2293 | val_loss 2.2688 | lr 3.210591e-03\n",
      "step 15800 | train_loss 2.2476 | val_loss 2.2838 | lr 3.188976e-03\n",
      "step 16000 | train_loss 2.2551 | val_loss 2.2878 | lr 3.167184e-03\n",
      "step 16200 | train_loss 2.2500 | val_loss 2.2859 | lr 3.145217e-03\n",
      "step 16400 | train_loss 2.2427 | val_loss 2.2745 | lr 3.123079e-03\n",
      "step 16600 | train_loss 2.2568 | val_loss 2.2771 | lr 3.100774e-03\n",
      "step 16800 | train_loss 2.2178 | val_loss 2.2771 | lr 3.078304e-03\n",
      "step 17000 | train_loss 2.2228 | val_loss 2.2697 | lr 3.055674e-03\n",
      "step 17200 | train_loss 2.2534 | val_loss 2.2893 | lr 3.032887e-03\n",
      "step 17400 | train_loss 2.2445 | val_loss 2.2970 | lr 3.009947e-03\n",
      "step 17600 | train_loss 2.2452 | val_loss 2.2826 | lr 2.986858e-03\n",
      "step 17800 | train_loss 2.2446 | val_loss 2.2801 | lr 2.963622e-03\n",
      "step 18000 | train_loss 2.2341 | val_loss 2.2651 | lr 2.940244e-03\n",
      "step 18200 | train_loss 2.2349 | val_loss 2.2706 | lr 2.916727e-03\n",
      "step 18400 | train_loss 2.1798 | val_loss 2.2698 | lr 2.893075e-03\n",
      "step 18600 | train_loss 2.2230 | val_loss 2.2727 | lr 2.869292e-03\n",
      "step 18800 | train_loss 2.2287 | val_loss 2.2706 | lr 2.845382e-03\n",
      "step 19000 | train_loss 2.2358 | val_loss 2.2868 | lr 2.821348e-03\n",
      "step 19200 | train_loss 2.2439 | val_loss 2.2870 | lr 2.797194e-03\n",
      "step 19400 | train_loss 2.2345 | val_loss 2.2631 | lr 2.772924e-03\n",
      "step 19600 | train_loss 2.2326 | val_loss 2.2641 | lr 2.748542e-03\n",
      "step 19800 | train_loss 2.2407 | val_loss 2.2782 | lr 2.724051e-03\n",
      "step 20000 | train_loss 2.2464 | val_loss 2.2812 | lr 2.699456e-03\n",
      "step 20200 | train_loss 2.2100 | val_loss 2.2583 | lr 2.674761e-03\n",
      "step 20400 | train_loss 2.2081 | val_loss 2.2534 | lr 2.649968e-03\n",
      "step 20600 | train_loss 2.2278 | val_loss 2.2695 | lr 2.625083e-03\n",
      "step 20800 | train_loss 2.2384 | val_loss 2.2677 | lr 2.600109e-03\n",
      "step 21000 | train_loss 2.2402 | val_loss 2.2777 | lr 2.575050e-03\n",
      "step 21200 | train_loss 2.2341 | val_loss 2.2711 | lr 2.549910e-03\n",
      "step 21400 | train_loss 2.2309 | val_loss 2.2634 | lr 2.524693e-03\n",
      "step 21600 | train_loss 2.2643 | val_loss 2.2852 | lr 2.499402e-03\n",
      "step 21800 | train_loss 2.1959 | val_loss 2.2587 | lr 2.474043e-03\n",
      "step 22000 | train_loss 2.2100 | val_loss 2.2605 | lr 2.448619e-03\n",
      "step 22200 | train_loss 2.2249 | val_loss 2.2750 | lr 2.423134e-03\n",
      "step 22400 | train_loss 2.2171 | val_loss 2.2603 | lr 2.397592e-03\n",
      "step 22600 | train_loss 2.2291 | val_loss 2.2589 | lr 2.371997e-03\n",
      "step 22800 | train_loss 2.2216 | val_loss 2.2645 | lr 2.346352e-03\n",
      "step 23000 | train_loss 2.2428 | val_loss 2.2916 | lr 2.320664e-03\n",
      "step 23200 | train_loss 2.2429 | val_loss 2.2870 | lr 2.294934e-03\n",
      "step 23400 | train_loss 2.1966 | val_loss 2.2580 | lr 2.269167e-03\n",
      "step 23600 | train_loss 2.2037 | val_loss 2.2575 | lr 2.243368e-03\n",
      "step 23800 | train_loss 2.2198 | val_loss 2.2617 | lr 2.217540e-03\n",
      "step 24000 | train_loss 2.2206 | val_loss 2.2678 | lr 2.191688e-03\n",
      "step 24200 | train_loss 2.2285 | val_loss 2.2606 | lr 2.165815e-03\n",
      "step 24400 | train_loss 2.2423 | val_loss 2.2678 | lr 2.139926e-03\n",
      "step 24600 | train_loss 2.2272 | val_loss 2.2543 | lr 2.114024e-03\n",
      "step 24800 | train_loss 2.2317 | val_loss 2.2678 | lr 2.088115e-03\n",
      "step 25000 | train_loss 2.2277 | val_loss 2.2545 | lr 2.062201e-03\n",
      "step 25200 | train_loss 2.2097 | val_loss 2.2688 | lr 2.036287e-03\n",
      "step 25400 | train_loss 2.2094 | val_loss 2.2617 | lr 2.010378e-03\n",
      "step 25600 | train_loss 2.2205 | val_loss 2.2661 | lr 1.984476e-03\n",
      "step 25800 | train_loss 2.2092 | val_loss 2.2547 | lr 1.958587e-03\n",
      "step 26000 | train_loss 2.2232 | val_loss 2.2610 | lr 1.932714e-03\n",
      "step 26200 | train_loss 2.2295 | val_loss 2.2599 | lr 1.906862e-03\n",
      "step 26400 | train_loss 2.2287 | val_loss 2.2659 | lr 1.881034e-03\n",
      "step 26600 | train_loss 2.2328 | val_loss 2.2660 | lr 1.855235e-03\n",
      "step 26800 | train_loss 2.1878 | val_loss 2.2566 | lr 1.829469e-03\n",
      "step 27000 | train_loss 2.2112 | val_loss 2.2528 | lr 1.803739e-03\n",
      "step 27200 | train_loss 2.2179 | val_loss 2.2626 | lr 1.778050e-03\n",
      "step 27400 | train_loss 2.2145 | val_loss 2.2575 | lr 1.752406e-03\n",
      "step 27600 | train_loss 2.2240 | val_loss 2.2508 | lr 1.726811e-03\n",
      "step 27800 | train_loss 2.2300 | val_loss 2.2600 | lr 1.701268e-03\n",
      "step 28000 | train_loss 2.2475 | val_loss 2.2807 | lr 1.675783e-03\n",
      "step 28200 | train_loss 2.2198 | val_loss 2.2583 | lr 1.650359e-03\n",
      "step 28400 | train_loss 2.2167 | val_loss 2.2512 | lr 1.625000e-03\n",
      "step 28600 | train_loss 2.2006 | val_loss 2.2512 | lr 1.599710e-03\n",
      "step 28800 | train_loss 2.2088 | val_loss 2.2522 | lr 1.574493e-03\n",
      "step 29000 | train_loss 2.2203 | val_loss 2.2616 | lr 1.549353e-03\n",
      "step 29200 | train_loss 2.2114 | val_loss 2.2540 | lr 1.524294e-03\n",
      "step 29400 | train_loss 2.2203 | val_loss 2.2566 | lr 1.499319e-03\n",
      "step 29600 | train_loss 2.2278 | val_loss 2.2574 | lr 1.474434e-03\n",
      "step 29800 | train_loss 2.2193 | val_loss 2.2570 | lr 1.449642e-03\n",
      "step 30000 | train_loss 2.2359 | val_loss 2.2623 | lr 1.424946e-03\n",
      "step 30200 | train_loss 2.1953 | val_loss 2.2490 | lr 1.400351e-03\n",
      "step 30400 | train_loss 2.2014 | val_loss 2.2519 | lr 1.375860e-03\n",
      "step 30600 | train_loss 2.2086 | val_loss 2.2530 | lr 1.351478e-03\n",
      "step 30800 | train_loss 2.2018 | val_loss 2.2476 | lr 1.327208e-03\n",
      "step 31000 | train_loss 2.2132 | val_loss 2.2490 | lr 1.303054e-03\n",
      "step 31200 | train_loss 2.2216 | val_loss 2.2563 | lr 1.279020e-03\n",
      "step 31400 | train_loss 2.2262 | val_loss 2.2607 | lr 1.255110e-03\n",
      "step 31600 | train_loss 2.2231 | val_loss 2.2525 | lr 1.231327e-03\n",
      "step 31800 | train_loss 2.1817 | val_loss 2.2478 | lr 1.207675e-03\n",
      "step 32000 | train_loss 2.2007 | val_loss 2.2485 | lr 1.184159e-03\n",
      "step 32200 | train_loss 2.1972 | val_loss 2.2455 | lr 1.160781e-03\n",
      "step 32400 | train_loss 2.2060 | val_loss 2.2525 | lr 1.137545e-03\n",
      "step 32600 | train_loss 2.2082 | val_loss 2.2418 | lr 1.114455e-03\n",
      "step 32800 | train_loss 2.2126 | val_loss 2.2433 | lr 1.091515e-03\n",
      "step 33000 | train_loss 2.2135 | val_loss 2.2496 | lr 1.068728e-03\n",
      "step 33200 | train_loss 2.2206 | val_loss 2.2533 | lr 1.046098e-03\n",
      "step 33400 | train_loss 2.2105 | val_loss 2.2449 | lr 1.023629e-03\n",
      "step 33600 | train_loss 2.1937 | val_loss 2.2433 | lr 1.001323e-03\n",
      "step 33800 | train_loss 2.1943 | val_loss 2.2438 | lr 9.791854e-04\n",
      "step 34000 | train_loss 2.2060 | val_loss 2.2490 | lr 9.572185e-04\n",
      "step 34200 | train_loss 2.2142 | val_loss 2.2498 | lr 9.354261e-04\n",
      "step 34400 | train_loss 2.2071 | val_loss 2.2449 | lr 9.138116e-04\n",
      "step 34600 | train_loss 2.2097 | val_loss 2.2472 | lr 8.923785e-04\n",
      "step 34800 | train_loss 2.2147 | val_loss 2.2491 | lr 8.711301e-04\n",
      "step 35000 | train_loss 2.2126 | val_loss 2.2439 | lr 8.500697e-04\n",
      "step 35200 | train_loss 2.1906 | val_loss 2.2409 | lr 8.292008e-04\n",
      "step 35400 | train_loss 2.1977 | val_loss 2.2428 | lr 8.085266e-04\n",
      "step 35600 | train_loss 2.1992 | val_loss 2.2440 | lr 7.880503e-04\n",
      "step 35800 | train_loss 2.1992 | val_loss 2.2399 | lr 7.677753e-04\n",
      "step 36000 | train_loss 2.2143 | val_loss 2.2435 | lr 7.477047e-04\n",
      "step 36200 | train_loss 2.2036 | val_loss 2.2451 | lr 7.278416e-04\n",
      "step 36400 | train_loss 2.2038 | val_loss 2.2432 | lr 7.081893e-04\n",
      "step 36600 | train_loss 2.2032 | val_loss 2.2434 | lr 6.887507e-04\n",
      "step 36800 | train_loss 2.1886 | val_loss 2.2439 | lr 6.695291e-04\n",
      "step 37000 | train_loss 2.1951 | val_loss 2.2471 | lr 6.505273e-04\n",
      "step 37200 | train_loss 2.1969 | val_loss 2.2446 | lr 6.317485e-04\n",
      "step 37400 | train_loss 2.2036 | val_loss 2.2449 | lr 6.131956e-04\n",
      "step 37600 | train_loss 2.1952 | val_loss 2.2378 | lr 5.948715e-04\n",
      "step 37800 | train_loss 2.2011 | val_loss 2.2406 | lr 5.767791e-04\n",
      "step 38000 | train_loss 2.2030 | val_loss 2.2385 | lr 5.589212e-04\n",
      "step 38200 | train_loss 2.2065 | val_loss 2.2419 | lr 5.413008e-04\n",
      "step 38400 | train_loss 2.2020 | val_loss 2.2410 | lr 5.239205e-04\n",
      "step 38600 | train_loss 2.1943 | val_loss 2.2454 | lr 5.067831e-04\n",
      "step 38800 | train_loss 2.1929 | val_loss 2.2403 | lr 4.898913e-04\n",
      "step 39000 | train_loss 2.1933 | val_loss 2.2393 | lr 4.732479e-04\n",
      "step 39200 | train_loss 2.2002 | val_loss 2.2399 | lr 4.568553e-04\n",
      "step 39400 | train_loss 2.1966 | val_loss 2.2390 | lr 4.407162e-04\n",
      "step 39600 | train_loss 2.1982 | val_loss 2.2396 | lr 4.248332e-04\n",
      "step 39800 | train_loss 2.2024 | val_loss 2.2409 | lr 4.092088e-04\n",
      "step 40000 | train_loss 2.2055 | val_loss 2.2409 | lr 3.938454e-04\n",
      "step 40200 | train_loss 2.1945 | val_loss 2.2412 | lr 3.787454e-04\n",
      "step 40400 | train_loss 2.1916 | val_loss 2.2392 | lr 3.639113e-04\n",
      "step 40600 | train_loss 2.1932 | val_loss 2.2381 | lr 3.493453e-04\n",
      "step 40800 | train_loss 2.1917 | val_loss 2.2379 | lr 3.350499e-04\n",
      "step 41000 | train_loss 2.1973 | val_loss 2.2374 | lr 3.210271e-04\n",
      "step 41200 | train_loss 2.1974 | val_loss 2.2407 | lr 3.072794e-04\n",
      "step 41400 | train_loss 2.1963 | val_loss 2.2386 | lr 2.938087e-04\n",
      "step 41600 | train_loss 2.1945 | val_loss 2.2367 | lr 2.806173e-04\n",
      "step 41800 | train_loss 2.1881 | val_loss 2.2366 | lr 2.677072e-04\n",
      "step 42000 | train_loss 2.1898 | val_loss 2.2359 | lr 2.550805e-04\n",
      "step 42200 | train_loss 2.1918 | val_loss 2.2370 | lr 2.427392e-04\n",
      "step 42400 | train_loss 2.1927 | val_loss 2.2377 | lr 2.306851e-04\n",
      "step 42600 | train_loss 2.1944 | val_loss 2.2351 | lr 2.189203e-04\n",
      "step 42800 | train_loss 2.1941 | val_loss 2.2350 | lr 2.074466e-04\n",
      "step 43000 | train_loss 2.1943 | val_loss 2.2351 | lr 1.962658e-04\n",
      "step 43200 | train_loss 2.1951 | val_loss 2.2358 | lr 1.853796e-04\n",
      "step 43400 | train_loss 2.1961 | val_loss 2.2355 | lr 1.747898e-04\n",
      "step 43600 | train_loss 2.1908 | val_loss 2.2356 | lr 1.644980e-04\n",
      "step 43800 | train_loss 2.1905 | val_loss 2.2359 | lr 1.545059e-04\n",
      "step 44000 | train_loss 2.1902 | val_loss 2.2371 | lr 1.448150e-04\n",
      "step 44200 | train_loss 2.1914 | val_loss 2.2366 | lr 1.354269e-04\n",
      "step 44400 | train_loss 2.1939 | val_loss 2.2359 | lr 1.263431e-04\n",
      "step 44600 | train_loss 2.1916 | val_loss 2.2351 | lr 1.175650e-04\n",
      "step 44800 | train_loss 2.1934 | val_loss 2.2356 | lr 1.090939e-04\n",
      "step 45000 | train_loss 2.1952 | val_loss 2.2348 | lr 1.009313e-04\n",
      "step 45200 | train_loss 2.1917 | val_loss 2.2346 | lr 9.307839e-05\n",
      "step 45400 | train_loss 2.1915 | val_loss 2.2353 | lr 8.553641e-05\n",
      "step 45600 | train_loss 2.1908 | val_loss 2.2350 | lr 7.830658e-05\n",
      "step 45800 | train_loss 2.1910 | val_loss 2.2358 | lr 7.139002e-05\n",
      "step 46000 | train_loss 2.1920 | val_loss 2.2339 | lr 6.478784e-05\n",
      "step 46200 | train_loss 2.1918 | val_loss 2.2338 | lr 5.850107e-05\n",
      "step 46400 | train_loss 2.1918 | val_loss 2.2352 | lr 5.253071e-05\n",
      "step 46600 | train_loss 2.1927 | val_loss 2.2343 | lr 4.687770e-05\n",
      "step 46800 | train_loss 2.1904 | val_loss 2.2342 | lr 4.154293e-05\n",
      "step 47000 | train_loss 2.1907 | val_loss 2.2342 | lr 3.652725e-05\n",
      "step 47200 | train_loss 2.1909 | val_loss 2.2348 | lr 3.183145e-05\n",
      "step 47400 | train_loss 2.1908 | val_loss 2.2350 | lr 2.745626e-05\n",
      "step 47600 | train_loss 2.1906 | val_loss 2.2346 | lr 2.340239e-05\n",
      "step 47800 | train_loss 2.1914 | val_loss 2.2340 | lr 1.967046e-05\n",
      "step 48000 | train_loss 2.1910 | val_loss 2.2340 | lr 1.626107e-05\n",
      "step 48200 | train_loss 2.1913 | val_loss 2.2342 | lr 1.317476e-05\n",
      "step 48400 | train_loss 2.1920 | val_loss 2.2340 | lr 1.041202e-05\n",
      "step 48600 | train_loss 2.1912 | val_loss 2.2340 | lr 7.973276e-06\n",
      "step 48800 | train_loss 2.1912 | val_loss 2.2342 | lr 5.858919e-06\n",
      "step 49000 | train_loss 2.1912 | val_loss 2.2342 | lr 4.069283e-06\n",
      "step 49200 | train_loss 2.1911 | val_loss 2.2343 | lr 2.604650e-06\n",
      "step 49400 | train_loss 2.1911 | val_loss 2.2342 | lr 1.465250e-06\n",
      "step 49600 | train_loss 2.1911 | val_loss 2.2342 | lr 6.512652e-07\n",
      "step 49800 | train_loss 2.1911 | val_loss 2.2342 | lr 1.628227e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:07:27,227] Trial 6 finished with value: 2.2341749710696086 and parameters: {'embedding_size': 40, 'hidden_size': 854, 'learning_rate': 0.004124402324038016, 'batch_size': 128, 'context_length': 2}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.1911 | val_loss 2.2342 | lr 4.070622e-12\n",
      "step 0 | train_loss 3.3170 | val_loss 3.3171 | lr 1.176443e-04\n",
      "step 200 | train_loss 2.7022 | val_loss 2.7229 | lr 1.176397e-04\n",
      "step 400 | train_loss 2.5648 | val_loss 2.5713 | lr 1.176257e-04\n",
      "step 600 | train_loss 2.5080 | val_loss 2.5052 | lr 1.176025e-04\n",
      "step 800 | train_loss 2.4773 | val_loss 2.4657 | lr 1.175700e-04\n",
      "step 1000 | train_loss 2.4536 | val_loss 2.4372 | lr 1.175282e-04\n",
      "step 1200 | train_loss 2.4219 | val_loss 2.4186 | lr 1.174772e-04\n",
      "step 1400 | train_loss 2.4109 | val_loss 2.4057 | lr 1.174169e-04\n",
      "step 1600 | train_loss 2.4049 | val_loss 2.3897 | lr 1.173473e-04\n",
      "step 1800 | train_loss 2.3945 | val_loss 2.3820 | lr 1.172685e-04\n",
      "step 2000 | train_loss 2.3872 | val_loss 2.3712 | lr 1.171805e-04\n",
      "step 2200 | train_loss 2.3807 | val_loss 2.3638 | lr 1.170832e-04\n",
      "step 2400 | train_loss 2.3692 | val_loss 2.3568 | lr 1.169768e-04\n",
      "step 2600 | train_loss 2.3595 | val_loss 2.3488 | lr 1.168611e-04\n",
      "step 2800 | train_loss 2.3501 | val_loss 2.3426 | lr 1.167363e-04\n",
      "step 3000 | train_loss 2.3414 | val_loss 2.3374 | lr 1.166024e-04\n",
      "step 3200 | train_loss 2.3420 | val_loss 2.3319 | lr 1.164593e-04\n",
      "step 3400 | train_loss 2.3306 | val_loss 2.3270 | lr 1.163072e-04\n",
      "step 3600 | train_loss 2.3327 | val_loss 2.3210 | lr 1.161459e-04\n",
      "step 3800 | train_loss 2.3297 | val_loss 2.3188 | lr 1.159756e-04\n",
      "step 4000 | train_loss 2.3253 | val_loss 2.3122 | lr 1.157963e-04\n",
      "step 4200 | train_loss 2.3200 | val_loss 2.3085 | lr 1.156080e-04\n",
      "step 4400 | train_loss 2.3138 | val_loss 2.3050 | lr 1.154107e-04\n",
      "step 4600 | train_loss 2.3090 | val_loss 2.3014 | lr 1.152045e-04\n",
      "step 4800 | train_loss 2.3150 | val_loss 2.2986 | lr 1.149893e-04\n",
      "step 5000 | train_loss 2.3037 | val_loss 2.2959 | lr 1.147653e-04\n",
      "step 5200 | train_loss 2.3020 | val_loss 2.2945 | lr 1.145325e-04\n",
      "step 5400 | train_loss 2.3100 | val_loss 2.2897 | lr 1.142909e-04\n",
      "step 5600 | train_loss 2.2966 | val_loss 2.2857 | lr 1.140405e-04\n",
      "step 5800 | train_loss 2.2990 | val_loss 2.2851 | lr 1.137814e-04\n",
      "step 6000 | train_loss 2.2995 | val_loss 2.2818 | lr 1.135136e-04\n",
      "step 6200 | train_loss 2.3047 | val_loss 2.2797 | lr 1.132372e-04\n",
      "step 6400 | train_loss 2.2958 | val_loss 2.2750 | lr 1.129522e-04\n",
      "step 6600 | train_loss 2.2891 | val_loss 2.2722 | lr 1.126586e-04\n",
      "step 6800 | train_loss 2.2612 | val_loss 2.2718 | lr 1.123565e-04\n",
      "step 7000 | train_loss 2.2664 | val_loss 2.2702 | lr 1.120460e-04\n",
      "step 7200 | train_loss 2.2747 | val_loss 2.2668 | lr 1.117271e-04\n",
      "step 7400 | train_loss 2.2750 | val_loss 2.2665 | lr 1.113998e-04\n",
      "step 7600 | train_loss 2.2687 | val_loss 2.2618 | lr 1.110642e-04\n",
      "step 7800 | train_loss 2.2617 | val_loss 2.2605 | lr 1.107204e-04\n",
      "step 8000 | train_loss 2.2593 | val_loss 2.2591 | lr 1.103684e-04\n",
      "step 8200 | train_loss 2.2586 | val_loss 2.2570 | lr 1.100082e-04\n",
      "step 8400 | train_loss 2.2628 | val_loss 2.2583 | lr 1.096400e-04\n",
      "step 8600 | train_loss 2.2593 | val_loss 2.2531 | lr 1.092637e-04\n",
      "step 8800 | train_loss 2.2632 | val_loss 2.2520 | lr 1.088795e-04\n",
      "step 9000 | train_loss 2.2622 | val_loss 2.2513 | lr 1.084873e-04\n",
      "step 9200 | train_loss 2.2536 | val_loss 2.2503 | lr 1.080873e-04\n",
      "step 9400 | train_loss 2.2558 | val_loss 2.2492 | lr 1.076796e-04\n",
      "step 9600 | train_loss 2.2483 | val_loss 2.2459 | lr 1.072641e-04\n",
      "step 9800 | train_loss 2.2515 | val_loss 2.2441 | lr 1.068410e-04\n",
      "step 10000 | train_loss 2.2456 | val_loss 2.2445 | lr 1.064103e-04\n",
      "step 10200 | train_loss 2.2461 | val_loss 2.2409 | lr 1.059720e-04\n",
      "step 10400 | train_loss 2.2533 | val_loss 2.2395 | lr 1.055264e-04\n",
      "step 10600 | train_loss 2.2439 | val_loss 2.2373 | lr 1.050733e-04\n",
      "step 10800 | train_loss 2.2471 | val_loss 2.2353 | lr 1.046130e-04\n",
      "step 11000 | train_loss 2.2422 | val_loss 2.2333 | lr 1.041454e-04\n",
      "step 11200 | train_loss 2.2389 | val_loss 2.2338 | lr 1.036707e-04\n",
      "step 11400 | train_loss 2.2389 | val_loss 2.2329 | lr 1.031888e-04\n",
      "step 11600 | train_loss 2.2423 | val_loss 2.2328 | lr 1.027000e-04\n",
      "step 11800 | train_loss 2.2429 | val_loss 2.2325 | lr 1.022043e-04\n",
      "step 12000 | train_loss 2.2441 | val_loss 2.2311 | lr 1.017016e-04\n",
      "step 12200 | train_loss 2.2428 | val_loss 2.2290 | lr 1.011923e-04\n",
      "step 12400 | train_loss 2.2384 | val_loss 2.2272 | lr 1.006762e-04\n",
      "step 12600 | train_loss 2.2367 | val_loss 2.2273 | lr 1.001535e-04\n",
      "step 12800 | train_loss 2.2470 | val_loss 2.2255 | lr 9.962432e-05\n",
      "step 13000 | train_loss 2.2442 | val_loss 2.2252 | lr 9.908868e-05\n",
      "step 13200 | train_loss 2.2390 | val_loss 2.2234 | lr 9.854667e-05\n",
      "step 13400 | train_loss 2.2048 | val_loss 2.2200 | lr 9.799840e-05\n",
      "step 13600 | train_loss 2.2096 | val_loss 2.2215 | lr 9.744393e-05\n",
      "step 13800 | train_loss 2.2175 | val_loss 2.2203 | lr 9.688337e-05\n",
      "step 14000 | train_loss 2.2205 | val_loss 2.2175 | lr 9.631680e-05\n",
      "step 14200 | train_loss 2.2216 | val_loss 2.2192 | lr 9.574430e-05\n",
      "step 14400 | train_loss 2.2246 | val_loss 2.2161 | lr 9.516598e-05\n",
      "step 14600 | train_loss 2.2085 | val_loss 2.2165 | lr 9.458191e-05\n",
      "step 14800 | train_loss 2.2154 | val_loss 2.2149 | lr 9.399220e-05\n",
      "step 15000 | train_loss 2.2189 | val_loss 2.2146 | lr 9.339694e-05\n",
      "step 15200 | train_loss 2.2166 | val_loss 2.2135 | lr 9.279621e-05\n",
      "step 15400 | train_loss 2.2188 | val_loss 2.2121 | lr 9.219013e-05\n",
      "step 15600 | train_loss 2.2188 | val_loss 2.2116 | lr 9.157877e-05\n",
      "step 15800 | train_loss 2.2160 | val_loss 2.2123 | lr 9.096224e-05\n",
      "step 16000 | train_loss 2.2129 | val_loss 2.2108 | lr 9.034063e-05\n",
      "step 16200 | train_loss 2.2092 | val_loss 2.2083 | lr 8.971405e-05\n",
      "step 16400 | train_loss 2.2061 | val_loss 2.2072 | lr 8.908259e-05\n",
      "step 16600 | train_loss 2.2075 | val_loss 2.2067 | lr 8.844635e-05\n",
      "step 16800 | train_loss 2.2057 | val_loss 2.2056 | lr 8.780543e-05\n",
      "step 17000 | train_loss 2.2085 | val_loss 2.2042 | lr 8.715993e-05\n",
      "step 17200 | train_loss 2.2069 | val_loss 2.2044 | lr 8.650996e-05\n",
      "step 17400 | train_loss 2.2097 | val_loss 2.2019 | lr 8.585562e-05\n",
      "step 17600 | train_loss 2.2084 | val_loss 2.2004 | lr 8.519701e-05\n",
      "step 17800 | train_loss 2.2035 | val_loss 2.2013 | lr 8.453424e-05\n",
      "step 18000 | train_loss 2.2019 | val_loss 2.2005 | lr 8.386740e-05\n",
      "step 18200 | train_loss 2.2083 | val_loss 2.2016 | lr 8.319661e-05\n",
      "step 18400 | train_loss 2.2039 | val_loss 2.2009 | lr 8.252197e-05\n",
      "step 18600 | train_loss 2.2077 | val_loss 2.2007 | lr 8.184359e-05\n",
      "step 18800 | train_loss 2.2088 | val_loss 2.1986 | lr 8.116157e-05\n",
      "step 19000 | train_loss 2.2039 | val_loss 2.1979 | lr 8.047602e-05\n",
      "step 19200 | train_loss 2.2028 | val_loss 2.1985 | lr 7.978706e-05\n",
      "step 19400 | train_loss 2.2090 | val_loss 2.1968 | lr 7.909478e-05\n",
      "step 19600 | train_loss 2.2120 | val_loss 2.1968 | lr 7.839931e-05\n",
      "step 19800 | train_loss 2.2077 | val_loss 2.1964 | lr 7.770074e-05\n",
      "step 20000 | train_loss 2.2048 | val_loss 2.1935 | lr 7.699919e-05\n",
      "step 20200 | train_loss 2.1808 | val_loss 2.1941 | lr 7.629477e-05\n",
      "step 20400 | train_loss 2.1860 | val_loss 2.1938 | lr 7.558759e-05\n",
      "step 20600 | train_loss 2.1908 | val_loss 2.1932 | lr 7.487777e-05\n",
      "step 20800 | train_loss 2.1970 | val_loss 2.1941 | lr 7.416540e-05\n",
      "step 21000 | train_loss 2.1923 | val_loss 2.1913 | lr 7.345062e-05\n",
      "step 21200 | train_loss 2.1842 | val_loss 2.1907 | lr 7.273353e-05\n",
      "step 21400 | train_loss 2.1842 | val_loss 2.1914 | lr 7.201424e-05\n",
      "step 21600 | train_loss 2.1877 | val_loss 2.1907 | lr 7.129286e-05\n",
      "step 21800 | train_loss 2.1915 | val_loss 2.1917 | lr 7.056952e-05\n",
      "step 22000 | train_loss 2.1904 | val_loss 2.1883 | lr 6.984432e-05\n",
      "step 22200 | train_loss 2.1888 | val_loss 2.1884 | lr 6.911738e-05\n",
      "step 22400 | train_loss 2.1905 | val_loss 2.1895 | lr 6.838881e-05\n",
      "step 22600 | train_loss 2.1857 | val_loss 2.1880 | lr 6.765874e-05\n",
      "step 22800 | train_loss 2.1825 | val_loss 2.1874 | lr 6.692727e-05\n",
      "step 23000 | train_loss 2.1815 | val_loss 2.1859 | lr 6.619452e-05\n",
      "step 23200 | train_loss 2.1841 | val_loss 2.1859 | lr 6.546060e-05\n",
      "step 23400 | train_loss 2.1817 | val_loss 2.1851 | lr 6.472564e-05\n",
      "step 23600 | train_loss 2.1833 | val_loss 2.1846 | lr 6.398974e-05\n",
      "step 23800 | train_loss 2.1860 | val_loss 2.1836 | lr 6.325303e-05\n",
      "step 24000 | train_loss 2.1846 | val_loss 2.1823 | lr 6.251562e-05\n",
      "step 24200 | train_loss 2.1840 | val_loss 2.1816 | lr 6.177763e-05\n",
      "step 24400 | train_loss 2.1830 | val_loss 2.1815 | lr 6.103916e-05\n",
      "step 24600 | train_loss 2.1784 | val_loss 2.1810 | lr 6.030035e-05\n",
      "step 24800 | train_loss 2.1822 | val_loss 2.1814 | lr 5.956131e-05\n",
      "step 25000 | train_loss 2.1807 | val_loss 2.1816 | lr 5.882215e-05\n",
      "step 25200 | train_loss 2.1840 | val_loss 2.1822 | lr 5.808299e-05\n",
      "step 25400 | train_loss 2.1840 | val_loss 2.1817 | lr 5.734394e-05\n",
      "step 25600 | train_loss 2.1831 | val_loss 2.1806 | lr 5.660513e-05\n",
      "step 25800 | train_loss 2.1838 | val_loss 2.1804 | lr 5.586667e-05\n",
      "step 26000 | train_loss 2.1853 | val_loss 2.1804 | lr 5.512867e-05\n",
      "step 26200 | train_loss 2.1897 | val_loss 2.1797 | lr 5.439126e-05\n",
      "step 26400 | train_loss 2.1883 | val_loss 2.1795 | lr 5.365455e-05\n",
      "step 26600 | train_loss 2.1839 | val_loss 2.1789 | lr 5.291866e-05\n",
      "step 26800 | train_loss 2.1642 | val_loss 2.1774 | lr 5.218369e-05\n",
      "step 27000 | train_loss 2.1661 | val_loss 2.1779 | lr 5.144978e-05\n",
      "step 27200 | train_loss 2.1714 | val_loss 2.1778 | lr 5.071703e-05\n",
      "step 27400 | train_loss 2.1721 | val_loss 2.1765 | lr 4.998556e-05\n",
      "step 27600 | train_loss 2.1731 | val_loss 2.1773 | lr 4.925548e-05\n",
      "step 27800 | train_loss 2.1738 | val_loss 2.1759 | lr 4.852692e-05\n",
      "step 28000 | train_loss 2.1692 | val_loss 2.1764 | lr 4.779998e-05\n",
      "step 28200 | train_loss 2.1714 | val_loss 2.1759 | lr 4.707478e-05\n",
      "step 28400 | train_loss 2.1737 | val_loss 2.1766 | lr 4.635143e-05\n",
      "step 28600 | train_loss 2.1717 | val_loss 2.1758 | lr 4.563006e-05\n",
      "step 28800 | train_loss 2.1733 | val_loss 2.1749 | lr 4.491077e-05\n",
      "step 29000 | train_loss 2.1747 | val_loss 2.1755 | lr 4.419367e-05\n",
      "step 29200 | train_loss 2.1717 | val_loss 2.1754 | lr 4.347889e-05\n",
      "step 29400 | train_loss 2.1706 | val_loss 2.1750 | lr 4.276653e-05\n",
      "step 29600 | train_loss 2.1687 | val_loss 2.1738 | lr 4.205670e-05\n",
      "step 29800 | train_loss 2.1677 | val_loss 2.1729 | lr 4.134952e-05\n",
      "step 30000 | train_loss 2.1682 | val_loss 2.1731 | lr 4.064510e-05\n",
      "step 30200 | train_loss 2.1659 | val_loss 2.1721 | lr 3.994356e-05\n",
      "step 30400 | train_loss 2.1692 | val_loss 2.1712 | lr 3.924499e-05\n",
      "step 30600 | train_loss 2.1675 | val_loss 2.1718 | lr 3.854951e-05\n",
      "step 30800 | train_loss 2.1696 | val_loss 2.1708 | lr 3.785724e-05\n",
      "step 31000 | train_loss 2.1702 | val_loss 2.1700 | lr 3.716827e-05\n",
      "step 31200 | train_loss 2.1674 | val_loss 2.1709 | lr 3.648273e-05\n",
      "step 31400 | train_loss 2.1664 | val_loss 2.1706 | lr 3.580071e-05\n",
      "step 31600 | train_loss 2.1693 | val_loss 2.1706 | lr 3.512233e-05\n",
      "step 31800 | train_loss 2.1682 | val_loss 2.1706 | lr 3.444769e-05\n",
      "step 32000 | train_loss 2.1695 | val_loss 2.1708 | lr 3.377690e-05\n",
      "step 32200 | train_loss 2.1699 | val_loss 2.1704 | lr 3.311006e-05\n",
      "step 32400 | train_loss 2.1689 | val_loss 2.1701 | lr 3.244728e-05\n",
      "step 32600 | train_loss 2.1693 | val_loss 2.1702 | lr 3.178867e-05\n",
      "step 32800 | train_loss 2.1698 | val_loss 2.1694 | lr 3.113433e-05\n",
      "step 33000 | train_loss 2.1727 | val_loss 2.1697 | lr 3.048436e-05\n",
      "step 33200 | train_loss 2.1730 | val_loss 2.1699 | lr 2.983887e-05\n",
      "step 33400 | train_loss 2.1712 | val_loss 2.1685 | lr 2.919795e-05\n",
      "step 33600 | train_loss 2.1595 | val_loss 2.1691 | lr 2.856171e-05\n",
      "step 33800 | train_loss 2.1603 | val_loss 2.1684 | lr 2.793025e-05\n",
      "step 34000 | train_loss 2.1622 | val_loss 2.1684 | lr 2.730366e-05\n",
      "step 34200 | train_loss 2.1644 | val_loss 2.1684 | lr 2.668206e-05\n",
      "step 34400 | train_loss 2.1649 | val_loss 2.1678 | lr 2.606553e-05\n",
      "step 34600 | train_loss 2.1608 | val_loss 2.1677 | lr 2.545417e-05\n",
      "step 34800 | train_loss 2.1607 | val_loss 2.1679 | lr 2.484808e-05\n",
      "step 35000 | train_loss 2.1629 | val_loss 2.1676 | lr 2.424736e-05\n",
      "step 35200 | train_loss 2.1629 | val_loss 2.1679 | lr 2.365209e-05\n",
      "step 35400 | train_loss 2.1631 | val_loss 2.1675 | lr 2.306238e-05\n",
      "step 35600 | train_loss 2.1633 | val_loss 2.1675 | lr 2.247832e-05\n",
      "step 35800 | train_loss 2.1636 | val_loss 2.1678 | lr 2.189999e-05\n",
      "step 36000 | train_loss 2.1630 | val_loss 2.1674 | lr 2.132750e-05\n",
      "step 36200 | train_loss 2.1615 | val_loss 2.1671 | lr 2.076093e-05\n",
      "step 36400 | train_loss 2.1598 | val_loss 2.1665 | lr 2.020036e-05\n",
      "step 36600 | train_loss 2.1599 | val_loss 2.1660 | lr 1.964590e-05\n",
      "step 36800 | train_loss 2.1582 | val_loss 2.1658 | lr 1.909762e-05\n",
      "step 37000 | train_loss 2.1583 | val_loss 2.1656 | lr 1.855562e-05\n",
      "step 37200 | train_loss 2.1597 | val_loss 2.1654 | lr 1.801997e-05\n",
      "step 37400 | train_loss 2.1599 | val_loss 2.1650 | lr 1.749077e-05\n",
      "step 37600 | train_loss 2.1602 | val_loss 2.1648 | lr 1.696809e-05\n",
      "step 37800 | train_loss 2.1602 | val_loss 2.1647 | lr 1.645202e-05\n",
      "step 38000 | train_loss 2.1597 | val_loss 2.1647 | lr 1.594265e-05\n",
      "step 38200 | train_loss 2.1611 | val_loss 2.1648 | lr 1.544004e-05\n",
      "step 38400 | train_loss 2.1595 | val_loss 2.1647 | lr 1.494429e-05\n",
      "step 38600 | train_loss 2.1600 | val_loss 2.1649 | lr 1.445546e-05\n",
      "step 38800 | train_loss 2.1620 | val_loss 2.1649 | lr 1.397364e-05\n",
      "step 39000 | train_loss 2.1608 | val_loss 2.1648 | lr 1.349890e-05\n",
      "step 39200 | train_loss 2.1618 | val_loss 2.1648 | lr 1.303132e-05\n",
      "step 39400 | train_loss 2.1623 | val_loss 2.1647 | lr 1.257097e-05\n",
      "step 39600 | train_loss 2.1634 | val_loss 2.1644 | lr 1.211793e-05\n",
      "step 39800 | train_loss 2.1633 | val_loss 2.1644 | lr 1.167226e-05\n",
      "step 40000 | train_loss 2.1633 | val_loss 2.1644 | lr 1.123403e-05\n",
      "step 40200 | train_loss 2.1586 | val_loss 2.1643 | lr 1.080332e-05\n",
      "step 40400 | train_loss 2.1584 | val_loss 2.1644 | lr 1.038019e-05\n",
      "step 40600 | train_loss 2.1591 | val_loss 2.1642 | lr 9.964713e-06\n",
      "step 40800 | train_loss 2.1593 | val_loss 2.1641 | lr 9.556950e-06\n",
      "step 41000 | train_loss 2.1593 | val_loss 2.1640 | lr 9.156966e-06\n",
      "step 41200 | train_loss 2.1595 | val_loss 2.1640 | lr 8.764825e-06\n",
      "step 41400 | train_loss 2.1587 | val_loss 2.1639 | lr 8.380588e-06\n",
      "step 41600 | train_loss 2.1587 | val_loss 2.1639 | lr 8.004317e-06\n",
      "step 41800 | train_loss 2.1590 | val_loss 2.1641 | lr 7.636070e-06\n",
      "step 42000 | train_loss 2.1591 | val_loss 2.1640 | lr 7.275907e-06\n",
      "step 42200 | train_loss 2.1594 | val_loss 2.1639 | lr 6.923883e-06\n",
      "step 42400 | train_loss 2.1594 | val_loss 2.1640 | lr 6.580054e-06\n",
      "step 42600 | train_loss 2.1589 | val_loss 2.1640 | lr 6.244475e-06\n",
      "step 42800 | train_loss 2.1592 | val_loss 2.1640 | lr 5.917199e-06\n",
      "step 43000 | train_loss 2.1587 | val_loss 2.1638 | lr 5.598277e-06\n",
      "step 43200 | train_loss 2.1584 | val_loss 2.1636 | lr 5.287760e-06\n",
      "step 43400 | train_loss 2.1578 | val_loss 2.1635 | lr 4.985696e-06\n",
      "step 43600 | train_loss 2.1573 | val_loss 2.1634 | lr 4.692134e-06\n",
      "step 43800 | train_loss 2.1575 | val_loss 2.1631 | lr 4.407120e-06\n",
      "step 44000 | train_loss 2.1572 | val_loss 2.1632 | lr 4.130698e-06\n",
      "step 44200 | train_loss 2.1575 | val_loss 2.1631 | lr 3.862913e-06\n",
      "step 44400 | train_loss 2.1575 | val_loss 2.1630 | lr 3.603806e-06\n",
      "step 44600 | train_loss 2.1574 | val_loss 2.1631 | lr 3.353419e-06\n",
      "step 44800 | train_loss 2.1573 | val_loss 2.1631 | lr 3.111791e-06\n",
      "step 45000 | train_loss 2.1575 | val_loss 2.1630 | lr 2.878961e-06\n",
      "step 45200 | train_loss 2.1575 | val_loss 2.1630 | lr 2.654964e-06\n",
      "step 45400 | train_loss 2.1577 | val_loss 2.1631 | lr 2.439837e-06\n",
      "step 45600 | train_loss 2.1578 | val_loss 2.1631 | lr 2.233614e-06\n",
      "step 45800 | train_loss 2.1577 | val_loss 2.1630 | lr 2.036326e-06\n",
      "step 46000 | train_loss 2.1578 | val_loss 2.1630 | lr 1.848006e-06\n",
      "step 46200 | train_loss 2.1581 | val_loss 2.1630 | lr 1.668682e-06\n",
      "step 46400 | train_loss 2.1582 | val_loss 2.1630 | lr 1.498384e-06\n",
      "step 46600 | train_loss 2.1582 | val_loss 2.1630 | lr 1.337138e-06\n",
      "step 46800 | train_loss 2.1578 | val_loss 2.1630 | lr 1.184969e-06\n",
      "step 47000 | train_loss 2.1577 | val_loss 2.1630 | lr 1.041902e-06\n",
      "step 47200 | train_loss 2.1577 | val_loss 2.1630 | lr 9.079590e-07\n",
      "step 47400 | train_loss 2.1578 | val_loss 2.1630 | lr 7.831614e-07\n",
      "step 47600 | train_loss 2.1578 | val_loss 2.1630 | lr 6.675287e-07\n",
      "step 47800 | train_loss 2.1579 | val_loss 2.1630 | lr 5.610794e-07\n",
      "step 48000 | train_loss 2.1578 | val_loss 2.1630 | lr 4.638302e-07\n",
      "step 48200 | train_loss 2.1578 | val_loss 2.1630 | lr 3.757964e-07\n",
      "step 48400 | train_loss 2.1579 | val_loss 2.1630 | lr 2.969920e-07\n",
      "step 48600 | train_loss 2.1579 | val_loss 2.1630 | lr 2.274294e-07\n",
      "step 48800 | train_loss 2.1579 | val_loss 2.1630 | lr 1.671196e-07\n",
      "step 49000 | train_loss 2.1579 | val_loss 2.1630 | lr 1.160721e-07\n",
      "step 49200 | train_loss 2.1579 | val_loss 2.1630 | lr 7.429492e-08\n",
      "step 49400 | train_loss 2.1579 | val_loss 2.1630 | lr 4.179475e-08\n",
      "step 49600 | train_loss 2.1579 | val_loss 2.1630 | lr 1.857666e-08\n",
      "step 49800 | train_loss 2.1579 | val_loss 2.1630 | lr 4.644349e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:08:32,775] Trial 7 finished with value: 2.162983306284462 and parameters: {'embedding_size': 33, 'hidden_size': 710, 'learning_rate': 0.00011764429463011084, 'batch_size': 32, 'context_length': 3}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.1579 | val_loss 2.1630 | lr 1.161103e-13\n",
      "step 0 | train_loss 3.3282 | val_loss 3.3295 | lr 1.571625e-03\n",
      "step 200 | train_loss 2.4244 | val_loss 2.4242 | lr 1.571563e-03\n",
      "step 400 | train_loss 2.3718 | val_loss 2.3758 | lr 1.571377e-03\n",
      "step 600 | train_loss 2.3495 | val_loss 2.3547 | lr 1.571067e-03\n",
      "step 800 | train_loss 2.3617 | val_loss 2.3392 | lr 1.570632e-03\n",
      "step 1000 | train_loss 2.3677 | val_loss 2.3372 | lr 1.570074e-03\n",
      "step 1200 | train_loss 2.3256 | val_loss 2.3312 | lr 1.569392e-03\n",
      "step 1400 | train_loss 2.3363 | val_loss 2.3182 | lr 1.568587e-03\n",
      "step 1600 | train_loss 2.3336 | val_loss 2.3231 | lr 1.567657e-03\n",
      "step 1800 | train_loss 2.3378 | val_loss 2.3144 | lr 1.566605e-03\n",
      "step 2000 | train_loss 2.3103 | val_loss 2.3053 | lr 1.565429e-03\n",
      "step 2200 | train_loss 2.3191 | val_loss 2.3070 | lr 1.564129e-03\n",
      "step 2400 | train_loss 2.3198 | val_loss 2.3011 | lr 1.562707e-03\n",
      "step 2600 | train_loss 2.3100 | val_loss 2.2994 | lr 1.561163e-03\n",
      "step 2800 | train_loss 2.3101 | val_loss 2.2982 | lr 1.559495e-03\n",
      "step 3000 | train_loss 2.3075 | val_loss 2.2923 | lr 1.557706e-03\n",
      "step 3200 | train_loss 2.3260 | val_loss 2.2841 | lr 1.555795e-03\n",
      "step 3400 | train_loss 2.2084 | val_loss 2.2900 | lr 1.553762e-03\n",
      "step 3600 | train_loss 2.2734 | val_loss 2.2854 | lr 1.551608e-03\n",
      "step 3800 | train_loss 2.2868 | val_loss 2.2840 | lr 1.549333e-03\n",
      "step 4000 | train_loss 2.2870 | val_loss 2.2785 | lr 1.546937e-03\n",
      "step 4200 | train_loss 2.3006 | val_loss 2.2956 | lr 1.544421e-03\n",
      "step 4400 | train_loss 2.2910 | val_loss 2.2832 | lr 1.541786e-03\n",
      "step 4600 | train_loss 2.2850 | val_loss 2.2931 | lr 1.539031e-03\n",
      "step 4800 | train_loss 2.2828 | val_loss 2.2823 | lr 1.536157e-03\n",
      "step 5000 | train_loss 2.2961 | val_loss 2.2839 | lr 1.533165e-03\n",
      "step 5200 | train_loss 2.3039 | val_loss 2.2795 | lr 1.530054e-03\n",
      "step 5400 | train_loss 2.2982 | val_loss 2.2783 | lr 1.526826e-03\n",
      "step 5600 | train_loss 2.2956 | val_loss 2.2800 | lr 1.523481e-03\n",
      "step 5800 | train_loss 2.2933 | val_loss 2.2825 | lr 1.520020e-03\n",
      "step 6000 | train_loss 2.3087 | val_loss 2.2897 | lr 1.516442e-03\n",
      "step 6200 | train_loss 2.2850 | val_loss 2.2805 | lr 1.512750e-03\n",
      "step 6400 | train_loss 2.2985 | val_loss 2.2791 | lr 1.508942e-03\n",
      "step 6600 | train_loss 2.2901 | val_loss 2.2741 | lr 1.505020e-03\n",
      "step 6800 | train_loss 2.2214 | val_loss 2.2747 | lr 1.500985e-03\n",
      "step 7000 | train_loss 2.2579 | val_loss 2.2673 | lr 1.496837e-03\n",
      "step 7200 | train_loss 2.2687 | val_loss 2.2713 | lr 1.492576e-03\n",
      "step 7400 | train_loss 2.2728 | val_loss 2.2640 | lr 1.488204e-03\n",
      "step 7600 | train_loss 2.2789 | val_loss 2.2770 | lr 1.483721e-03\n",
      "step 7800 | train_loss 2.2698 | val_loss 2.2756 | lr 1.479128e-03\n",
      "step 8000 | train_loss 2.2787 | val_loss 2.2765 | lr 1.474425e-03\n",
      "step 8200 | train_loss 2.2785 | val_loss 2.2665 | lr 1.469614e-03\n",
      "step 8400 | train_loss 2.2843 | val_loss 2.2704 | lr 1.464694e-03\n",
      "step 8600 | train_loss 2.2683 | val_loss 2.2737 | lr 1.459668e-03\n",
      "step 8800 | train_loss 2.2801 | val_loss 2.2677 | lr 1.454535e-03\n",
      "step 9000 | train_loss 2.2809 | val_loss 2.2700 | lr 1.449296e-03\n",
      "step 9200 | train_loss 2.2796 | val_loss 2.2739 | lr 1.443952e-03\n",
      "step 9400 | train_loss 2.2772 | val_loss 2.2693 | lr 1.438505e-03\n",
      "step 9600 | train_loss 2.2854 | val_loss 2.2781 | lr 1.432955e-03\n",
      "step 9800 | train_loss 2.3010 | val_loss 2.2684 | lr 1.427302e-03\n",
      "step 10000 | train_loss 2.2823 | val_loss 2.2615 | lr 1.421548e-03\n",
      "step 10200 | train_loss 2.2376 | val_loss 2.2638 | lr 1.415694e-03\n",
      "step 10400 | train_loss 2.2498 | val_loss 2.2706 | lr 1.409740e-03\n",
      "step 10600 | train_loss 2.2492 | val_loss 2.2574 | lr 1.403688e-03\n",
      "step 10800 | train_loss 2.2638 | val_loss 2.2615 | lr 1.397538e-03\n",
      "step 11000 | train_loss 2.2615 | val_loss 2.2621 | lr 1.391291e-03\n",
      "step 11200 | train_loss 2.2507 | val_loss 2.2724 | lr 1.384949e-03\n",
      "step 11400 | train_loss 2.2528 | val_loss 2.2642 | lr 1.378513e-03\n",
      "step 11600 | train_loss 2.2731 | val_loss 2.2656 | lr 1.371982e-03\n",
      "step 11800 | train_loss 2.2705 | val_loss 2.2658 | lr 1.365359e-03\n",
      "step 12000 | train_loss 2.2665 | val_loss 2.2626 | lr 1.358645e-03\n",
      "step 12200 | train_loss 2.2664 | val_loss 2.2641 | lr 1.351840e-03\n",
      "step 12400 | train_loss 2.2702 | val_loss 2.2642 | lr 1.344946e-03\n",
      "step 12600 | train_loss 2.2662 | val_loss 2.2729 | lr 1.337964e-03\n",
      "step 12800 | train_loss 2.2728 | val_loss 2.2675 | lr 1.330894e-03\n",
      "step 13000 | train_loss 2.2610 | val_loss 2.2680 | lr 1.323738e-03\n",
      "step 13200 | train_loss 2.2743 | val_loss 2.2622 | lr 1.316497e-03\n",
      "step 13400 | train_loss 2.1715 | val_loss 2.2561 | lr 1.309173e-03\n",
      "step 13600 | train_loss 2.2322 | val_loss 2.2570 | lr 1.301766e-03\n",
      "step 13800 | train_loss 2.2369 | val_loss 2.2591 | lr 1.294277e-03\n",
      "step 14000 | train_loss 2.2417 | val_loss 2.2573 | lr 1.286708e-03\n",
      "step 14200 | train_loss 2.2559 | val_loss 2.2624 | lr 1.279060e-03\n",
      "step 14400 | train_loss 2.2578 | val_loss 2.2572 | lr 1.271334e-03\n",
      "step 14600 | train_loss 2.2491 | val_loss 2.2633 | lr 1.263532e-03\n",
      "step 14800 | train_loss 2.2535 | val_loss 2.2568 | lr 1.255654e-03\n",
      "step 15000 | train_loss 2.2586 | val_loss 2.2595 | lr 1.247701e-03\n",
      "step 15200 | train_loss 2.2711 | val_loss 2.2559 | lr 1.239676e-03\n",
      "step 15400 | train_loss 2.2669 | val_loss 2.2576 | lr 1.231579e-03\n",
      "step 15600 | train_loss 2.2585 | val_loss 2.2650 | lr 1.223412e-03\n",
      "step 15800 | train_loss 2.2572 | val_loss 2.2568 | lr 1.215176e-03\n",
      "step 16000 | train_loss 2.2636 | val_loss 2.2611 | lr 1.206872e-03\n",
      "step 16200 | train_loss 2.2535 | val_loss 2.2601 | lr 1.198501e-03\n",
      "step 16400 | train_loss 2.2546 | val_loss 2.2575 | lr 1.190065e-03\n",
      "step 16600 | train_loss 2.2735 | val_loss 2.2568 | lr 1.181566e-03\n",
      "step 16800 | train_loss 2.1920 | val_loss 2.2542 | lr 1.173004e-03\n",
      "step 17000 | train_loss 2.2331 | val_loss 2.2577 | lr 1.164380e-03\n",
      "step 17200 | train_loss 2.2345 | val_loss 2.2541 | lr 1.155697e-03\n",
      "step 17400 | train_loss 2.2377 | val_loss 2.2555 | lr 1.146956e-03\n",
      "step 17600 | train_loss 2.2519 | val_loss 2.2610 | lr 1.138158e-03\n",
      "step 17800 | train_loss 2.2515 | val_loss 2.2609 | lr 1.129303e-03\n",
      "step 18000 | train_loss 2.2551 | val_loss 2.2586 | lr 1.120395e-03\n",
      "step 18200 | train_loss 2.2499 | val_loss 2.2541 | lr 1.111434e-03\n",
      "step 18400 | train_loss 2.2544 | val_loss 2.2542 | lr 1.102421e-03\n",
      "step 18600 | train_loss 2.2531 | val_loss 2.2540 | lr 1.093359e-03\n",
      "step 18800 | train_loss 2.2574 | val_loss 2.2550 | lr 1.084248e-03\n",
      "step 19000 | train_loss 2.2580 | val_loss 2.2541 | lr 1.075089e-03\n",
      "step 19200 | train_loss 2.2553 | val_loss 2.2551 | lr 1.065885e-03\n",
      "step 19400 | train_loss 2.2622 | val_loss 2.2590 | lr 1.056637e-03\n",
      "step 19600 | train_loss 2.2564 | val_loss 2.2587 | lr 1.047346e-03\n",
      "step 19800 | train_loss 2.2747 | val_loss 2.2569 | lr 1.038014e-03\n",
      "step 20000 | train_loss 2.2576 | val_loss 2.2484 | lr 1.028642e-03\n",
      "step 20200 | train_loss 2.2119 | val_loss 2.2505 | lr 1.019231e-03\n",
      "step 20400 | train_loss 2.2325 | val_loss 2.2510 | lr 1.009784e-03\n",
      "step 20600 | train_loss 2.2311 | val_loss 2.2468 | lr 1.000301e-03\n",
      "step 20800 | train_loss 2.2382 | val_loss 2.2467 | lr 9.907850e-04\n",
      "step 21000 | train_loss 2.2397 | val_loss 2.2517 | lr 9.812361e-04\n",
      "step 21200 | train_loss 2.2355 | val_loss 2.2529 | lr 9.716563e-04\n",
      "step 21400 | train_loss 2.2398 | val_loss 2.2532 | lr 9.620472e-04\n",
      "step 21600 | train_loss 2.2499 | val_loss 2.2473 | lr 9.524103e-04\n",
      "step 21800 | train_loss 2.2518 | val_loss 2.2489 | lr 9.427470e-04\n",
      "step 22000 | train_loss 2.2452 | val_loss 2.2501 | lr 9.330590e-04\n",
      "step 22200 | train_loss 2.2530 | val_loss 2.2491 | lr 9.233478e-04\n",
      "step 22400 | train_loss 2.2428 | val_loss 2.2530 | lr 9.136148e-04\n",
      "step 22600 | train_loss 2.2553 | val_loss 2.2582 | lr 9.038616e-04\n",
      "step 22800 | train_loss 2.2509 | val_loss 2.2542 | lr 8.940898e-04\n",
      "step 23000 | train_loss 2.2523 | val_loss 2.2520 | lr 8.843009e-04\n",
      "step 23200 | train_loss 2.2593 | val_loss 2.2506 | lr 8.744964e-04\n",
      "step 23400 | train_loss 2.2164 | val_loss 2.2458 | lr 8.646779e-04\n",
      "step 23600 | train_loss 2.2203 | val_loss 2.2477 | lr 8.548470e-04\n",
      "step 23800 | train_loss 2.2268 | val_loss 2.2466 | lr 8.450052e-04\n",
      "step 24000 | train_loss 2.2244 | val_loss 2.2463 | lr 8.351540e-04\n",
      "step 24200 | train_loss 2.2370 | val_loss 2.2466 | lr 8.252951e-04\n",
      "step 24400 | train_loss 2.2429 | val_loss 2.2463 | lr 8.154299e-04\n",
      "step 24600 | train_loss 2.2363 | val_loss 2.2541 | lr 8.055600e-04\n",
      "step 24800 | train_loss 2.2349 | val_loss 2.2468 | lr 7.956870e-04\n",
      "step 25000 | train_loss 2.2396 | val_loss 2.2476 | lr 7.858125e-04\n",
      "step 25200 | train_loss 2.2422 | val_loss 2.2460 | lr 7.759379e-04\n",
      "step 25400 | train_loss 2.2404 | val_loss 2.2452 | lr 7.660649e-04\n",
      "step 25600 | train_loss 2.2421 | val_loss 2.2485 | lr 7.561950e-04\n",
      "step 25800 | train_loss 2.2433 | val_loss 2.2488 | lr 7.463298e-04\n",
      "step 26000 | train_loss 2.2439 | val_loss 2.2478 | lr 7.364709e-04\n",
      "step 26200 | train_loss 2.2397 | val_loss 2.2469 | lr 7.266197e-04\n",
      "step 26400 | train_loss 2.2415 | val_loss 2.2468 | lr 7.167779e-04\n",
      "step 26600 | train_loss 2.2557 | val_loss 2.2469 | lr 7.069470e-04\n",
      "step 26800 | train_loss 2.1951 | val_loss 2.2443 | lr 6.971285e-04\n",
      "step 27000 | train_loss 2.2184 | val_loss 2.2423 | lr 6.873240e-04\n",
      "step 27200 | train_loss 2.2221 | val_loss 2.2428 | lr 6.775351e-04\n",
      "step 27400 | train_loss 2.2239 | val_loss 2.2420 | lr 6.677633e-04\n",
      "step 27600 | train_loss 2.2314 | val_loss 2.2456 | lr 6.580101e-04\n",
      "step 27800 | train_loss 2.2314 | val_loss 2.2440 | lr 6.482772e-04\n",
      "step 28000 | train_loss 2.2284 | val_loss 2.2450 | lr 6.385659e-04\n",
      "step 28200 | train_loss 2.2291 | val_loss 2.2419 | lr 6.288779e-04\n",
      "step 28400 | train_loss 2.2384 | val_loss 2.2425 | lr 6.192146e-04\n",
      "step 28600 | train_loss 2.2391 | val_loss 2.2413 | lr 6.095777e-04\n",
      "step 28800 | train_loss 2.2409 | val_loss 2.2412 | lr 5.999686e-04\n",
      "step 29000 | train_loss 2.2377 | val_loss 2.2420 | lr 5.903888e-04\n",
      "step 29200 | train_loss 2.2351 | val_loss 2.2423 | lr 5.808399e-04\n",
      "step 29400 | train_loss 2.2456 | val_loss 2.2469 | lr 5.713234e-04\n",
      "step 29600 | train_loss 2.2348 | val_loss 2.2438 | lr 5.618408e-04\n",
      "step 29800 | train_loss 2.2424 | val_loss 2.2432 | lr 5.523935e-04\n",
      "step 30000 | train_loss 2.2360 | val_loss 2.2406 | lr 5.429831e-04\n",
      "step 30200 | train_loss 2.2044 | val_loss 2.2400 | lr 5.336110e-04\n",
      "step 30400 | train_loss 2.2206 | val_loss 2.2386 | lr 5.242787e-04\n",
      "step 30600 | train_loss 2.2215 | val_loss 2.2404 | lr 5.149878e-04\n",
      "step 30800 | train_loss 2.2208 | val_loss 2.2371 | lr 5.057396e-04\n",
      "step 31000 | train_loss 2.2240 | val_loss 2.2397 | lr 4.965356e-04\n",
      "step 31200 | train_loss 2.2264 | val_loss 2.2424 | lr 4.873773e-04\n",
      "step 31400 | train_loss 2.2284 | val_loss 2.2432 | lr 4.782662e-04\n",
      "step 31600 | train_loss 2.2294 | val_loss 2.2400 | lr 4.692036e-04\n",
      "step 31800 | train_loss 2.2305 | val_loss 2.2375 | lr 4.601910e-04\n",
      "step 32000 | train_loss 2.2307 | val_loss 2.2383 | lr 4.512298e-04\n",
      "step 32200 | train_loss 2.2359 | val_loss 2.2367 | lr 4.423214e-04\n",
      "step 32400 | train_loss 2.2294 | val_loss 2.2387 | lr 4.334673e-04\n",
      "step 32600 | train_loss 2.2322 | val_loss 2.2404 | lr 4.246689e-04\n",
      "step 32800 | train_loss 2.2365 | val_loss 2.2406 | lr 4.159274e-04\n",
      "step 33000 | train_loss 2.2384 | val_loss 2.2423 | lr 4.072444e-04\n",
      "step 33200 | train_loss 2.2426 | val_loss 2.2401 | lr 3.986212e-04\n",
      "step 33400 | train_loss 2.2349 | val_loss 2.2368 | lr 3.900591e-04\n",
      "step 33600 | train_loss 2.2151 | val_loss 2.2370 | lr 3.815595e-04\n",
      "step 33800 | train_loss 2.2194 | val_loss 2.2376 | lr 3.731237e-04\n",
      "step 34000 | train_loss 2.2184 | val_loss 2.2363 | lr 3.647531e-04\n",
      "step 34200 | train_loss 2.2205 | val_loss 2.2357 | lr 3.564490e-04\n",
      "step 34400 | train_loss 2.2243 | val_loss 2.2368 | lr 3.482127e-04\n",
      "step 34600 | train_loss 2.2217 | val_loss 2.2393 | lr 3.400454e-04\n",
      "step 34800 | train_loss 2.2195 | val_loss 2.2373 | lr 3.319486e-04\n",
      "step 35000 | train_loss 2.2284 | val_loss 2.2374 | lr 3.239235e-04\n",
      "step 35200 | train_loss 2.2247 | val_loss 2.2358 | lr 3.159713e-04\n",
      "step 35400 | train_loss 2.2272 | val_loss 2.2351 | lr 3.080933e-04\n",
      "step 35600 | train_loss 2.2272 | val_loss 2.2353 | lr 3.002907e-04\n",
      "step 35800 | train_loss 2.2274 | val_loss 2.2375 | lr 2.925648e-04\n",
      "step 36000 | train_loss 2.2262 | val_loss 2.2396 | lr 2.849167e-04\n",
      "step 36200 | train_loss 2.2302 | val_loss 2.2375 | lr 2.773478e-04\n",
      "step 36400 | train_loss 2.2279 | val_loss 2.2373 | lr 2.698592e-04\n",
      "step 36600 | train_loss 2.2331 | val_loss 2.2359 | lr 2.624520e-04\n",
      "step 36800 | train_loss 2.2072 | val_loss 2.2357 | lr 2.551275e-04\n",
      "step 37000 | train_loss 2.2151 | val_loss 2.2355 | lr 2.478868e-04\n",
      "step 37200 | train_loss 2.2185 | val_loss 2.2348 | lr 2.407310e-04\n",
      "step 37400 | train_loss 2.2175 | val_loss 2.2351 | lr 2.336614e-04\n",
      "step 37600 | train_loss 2.2198 | val_loss 2.2354 | lr 2.266789e-04\n",
      "step 37800 | train_loss 2.2235 | val_loss 2.2351 | lr 2.197847e-04\n",
      "step 38000 | train_loss 2.2193 | val_loss 2.2358 | lr 2.129798e-04\n",
      "step 38200 | train_loss 2.2172 | val_loss 2.2347 | lr 2.062655e-04\n",
      "step 38400 | train_loss 2.2215 | val_loss 2.2345 | lr 1.996426e-04\n",
      "step 38600 | train_loss 2.2237 | val_loss 2.2323 | lr 1.931123e-04\n",
      "step 38800 | train_loss 2.2252 | val_loss 2.2327 | lr 1.866756e-04\n",
      "step 39000 | train_loss 2.2238 | val_loss 2.2345 | lr 1.803336e-04\n",
      "step 39200 | train_loss 2.2234 | val_loss 2.2336 | lr 1.740871e-04\n",
      "step 39400 | train_loss 2.2247 | val_loss 2.2355 | lr 1.679372e-04\n",
      "step 39600 | train_loss 2.2241 | val_loss 2.2348 | lr 1.618849e-04\n",
      "step 39800 | train_loss 2.2251 | val_loss 2.2346 | lr 1.559311e-04\n",
      "step 40000 | train_loss 2.2304 | val_loss 2.2349 | lr 1.500768e-04\n",
      "step 40200 | train_loss 2.2123 | val_loss 2.2337 | lr 1.443229e-04\n",
      "step 40400 | train_loss 2.2172 | val_loss 2.2329 | lr 1.386703e-04\n",
      "step 40600 | train_loss 2.2177 | val_loss 2.2335 | lr 1.331199e-04\n",
      "step 40800 | train_loss 2.2165 | val_loss 2.2336 | lr 1.276725e-04\n",
      "step 41000 | train_loss 2.2174 | val_loss 2.2335 | lr 1.223291e-04\n",
      "step 41200 | train_loss 2.2192 | val_loss 2.2339 | lr 1.170904e-04\n",
      "step 41400 | train_loss 2.2197 | val_loss 2.2340 | lr 1.119573e-04\n",
      "step 41600 | train_loss 2.2176 | val_loss 2.2329 | lr 1.069307e-04\n",
      "step 41800 | train_loss 2.2195 | val_loss 2.2324 | lr 1.020112e-04\n",
      "step 42000 | train_loss 2.2200 | val_loss 2.2320 | lr 9.719975e-05\n",
      "step 42200 | train_loss 2.2221 | val_loss 2.2317 | lr 9.249702e-05\n",
      "step 42400 | train_loss 2.2217 | val_loss 2.2323 | lr 8.790377e-05\n",
      "step 42600 | train_loss 2.2209 | val_loss 2.2323 | lr 8.342073e-05\n",
      "step 42800 | train_loss 2.2219 | val_loss 2.2332 | lr 7.904861e-05\n",
      "step 43000 | train_loss 2.2225 | val_loss 2.2334 | lr 7.478809e-05\n",
      "step 43200 | train_loss 2.2242 | val_loss 2.2327 | lr 7.063985e-05\n",
      "step 43400 | train_loss 2.2245 | val_loss 2.2328 | lr 6.660454e-05\n",
      "step 43600 | train_loss 2.2170 | val_loss 2.2324 | lr 6.268281e-05\n",
      "step 43800 | train_loss 2.2188 | val_loss 2.2326 | lr 5.887526e-05\n",
      "step 44000 | train_loss 2.2186 | val_loss 2.2324 | lr 5.518251e-05\n",
      "step 44200 | train_loss 2.2178 | val_loss 2.2324 | lr 5.160513e-05\n",
      "step 44400 | train_loss 2.2179 | val_loss 2.2324 | lr 4.814370e-05\n",
      "step 44600 | train_loss 2.2182 | val_loss 2.2328 | lr 4.479875e-05\n",
      "step 44800 | train_loss 2.2182 | val_loss 2.2329 | lr 4.157081e-05\n",
      "step 45000 | train_loss 2.2180 | val_loss 2.2323 | lr 3.846040e-05\n",
      "step 45200 | train_loss 2.2184 | val_loss 2.2318 | lr 3.546800e-05\n",
      "step 45400 | train_loss 2.2185 | val_loss 2.2317 | lr 3.259409e-05\n",
      "step 45600 | train_loss 2.2196 | val_loss 2.2315 | lr 2.983913e-05\n",
      "step 45800 | train_loss 2.2191 | val_loss 2.2319 | lr 2.720354e-05\n",
      "step 46000 | train_loss 2.2191 | val_loss 2.2319 | lr 2.468774e-05\n",
      "step 46200 | train_loss 2.2196 | val_loss 2.2321 | lr 2.229214e-05\n",
      "step 46400 | train_loss 2.2199 | val_loss 2.2321 | lr 2.001710e-05\n",
      "step 46600 | train_loss 2.2205 | val_loss 2.2320 | lr 1.786299e-05\n",
      "step 46800 | train_loss 2.2192 | val_loss 2.2319 | lr 1.583015e-05\n",
      "step 47000 | train_loss 2.2189 | val_loss 2.2319 | lr 1.391890e-05\n",
      "step 47200 | train_loss 2.2191 | val_loss 2.2320 | lr 1.212954e-05\n",
      "step 47400 | train_loss 2.2191 | val_loss 2.2320 | lr 1.046235e-05\n",
      "step 47600 | train_loss 2.2190 | val_loss 2.2320 | lr 8.917600e-06\n",
      "step 47800 | train_loss 2.2190 | val_loss 2.2320 | lr 7.495530e-06\n",
      "step 48000 | train_loss 2.2190 | val_loss 2.2321 | lr 6.196366e-06\n",
      "step 48200 | train_loss 2.2189 | val_loss 2.2321 | lr 5.020312e-06\n",
      "step 48400 | train_loss 2.2189 | val_loss 2.2320 | lr 3.967554e-06\n",
      "step 48600 | train_loss 2.2189 | val_loss 2.2319 | lr 3.038258e-06\n",
      "step 48800 | train_loss 2.2189 | val_loss 2.2319 | lr 2.232572e-06\n",
      "step 49000 | train_loss 2.2189 | val_loss 2.2319 | lr 1.550621e-06\n",
      "step 49200 | train_loss 2.2189 | val_loss 2.2319 | lr 9.925152e-07\n",
      "step 49400 | train_loss 2.2189 | val_loss 2.2319 | lr 5.583412e-07\n",
      "step 49600 | train_loss 2.2189 | val_loss 2.2319 | lr 2.481680e-07\n",
      "step 49800 | train_loss 2.2189 | val_loss 2.2319 | lr 6.204445e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:09:35,397] Trial 8 finished with value: 2.2319393126027927 and parameters: {'embedding_size': 40, 'hidden_size': 463, 'learning_rate': 0.0015716249077989233, 'batch_size': 64, 'context_length': 2}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.2189 | val_loss 2.2319 | lr 1.551132e-12\n",
      "step 0 | train_loss 3.3138 | val_loss 3.3137 | lr 1.871399e-04\n",
      "step 200 | train_loss 2.5141 | val_loss 2.5152 | lr 1.871325e-04\n",
      "step 400 | train_loss 2.4370 | val_loss 2.4364 | lr 1.871104e-04\n",
      "step 600 | train_loss 2.3973 | val_loss 2.4008 | lr 1.870734e-04\n",
      "step 800 | train_loss 2.3860 | val_loss 2.3807 | lr 1.870217e-04\n",
      "step 1000 | train_loss 2.3796 | val_loss 2.3687 | lr 1.869553e-04\n",
      "step 1200 | train_loss 2.3596 | val_loss 2.3612 | lr 1.868741e-04\n",
      "step 1400 | train_loss 2.3467 | val_loss 2.3486 | lr 1.867781e-04\n",
      "step 1600 | train_loss 2.3470 | val_loss 2.3440 | lr 1.866675e-04\n",
      "step 1800 | train_loss 2.3406 | val_loss 2.3358 | lr 1.865421e-04\n",
      "step 2000 | train_loss 2.3367 | val_loss 2.3279 | lr 1.864021e-04\n",
      "step 2200 | train_loss 2.3327 | val_loss 2.3249 | lr 1.862474e-04\n",
      "step 2400 | train_loss 2.3335 | val_loss 2.3212 | lr 1.860781e-04\n",
      "step 2600 | train_loss 2.3230 | val_loss 2.3170 | lr 1.858941e-04\n",
      "step 2800 | train_loss 2.3240 | val_loss 2.3126 | lr 1.856956e-04\n",
      "step 3000 | train_loss 2.3203 | val_loss 2.3094 | lr 1.854825e-04\n",
      "step 3200 | train_loss 2.3233 | val_loss 2.3030 | lr 1.852550e-04\n",
      "step 3400 | train_loss 2.2802 | val_loss 2.3034 | lr 1.850129e-04\n",
      "step 3600 | train_loss 2.3031 | val_loss 2.3029 | lr 1.847564e-04\n",
      "step 3800 | train_loss 2.3032 | val_loss 2.2972 | lr 1.844855e-04\n",
      "step 4000 | train_loss 2.3034 | val_loss 2.2952 | lr 1.842002e-04\n",
      "step 4200 | train_loss 2.3052 | val_loss 2.2952 | lr 1.839007e-04\n",
      "step 4400 | train_loss 2.3040 | val_loss 2.2915 | lr 1.835868e-04\n",
      "step 4600 | train_loss 2.2958 | val_loss 2.2920 | lr 1.832588e-04\n",
      "step 4800 | train_loss 2.2907 | val_loss 2.2885 | lr 1.829166e-04\n",
      "step 5000 | train_loss 2.2974 | val_loss 2.2878 | lr 1.825603e-04\n",
      "step 5200 | train_loss 2.3066 | val_loss 2.2853 | lr 1.821899e-04\n",
      "step 5400 | train_loss 2.3036 | val_loss 2.2815 | lr 1.818055e-04\n",
      "step 5600 | train_loss 2.2996 | val_loss 2.2838 | lr 1.814072e-04\n",
      "step 5800 | train_loss 2.2934 | val_loss 2.2820 | lr 1.809951e-04\n",
      "step 6000 | train_loss 2.3008 | val_loss 2.2854 | lr 1.805691e-04\n",
      "step 6200 | train_loss 2.2897 | val_loss 2.2815 | lr 1.801294e-04\n",
      "step 6400 | train_loss 2.2981 | val_loss 2.2800 | lr 1.796760e-04\n",
      "step 6600 | train_loss 2.2957 | val_loss 2.2756 | lr 1.792090e-04\n",
      "step 6800 | train_loss 2.2592 | val_loss 2.2758 | lr 1.787285e-04\n",
      "step 7000 | train_loss 2.2841 | val_loss 2.2746 | lr 1.782346e-04\n",
      "step 7200 | train_loss 2.2820 | val_loss 2.2742 | lr 1.777273e-04\n",
      "step 7400 | train_loss 2.2790 | val_loss 2.2715 | lr 1.772067e-04\n",
      "step 7600 | train_loss 2.2841 | val_loss 2.2734 | lr 1.766728e-04\n",
      "step 7800 | train_loss 2.2814 | val_loss 2.2727 | lr 1.761259e-04\n",
      "step 8000 | train_loss 2.2833 | val_loss 2.2731 | lr 1.755659e-04\n",
      "step 8200 | train_loss 2.2754 | val_loss 2.2689 | lr 1.749930e-04\n",
      "step 8400 | train_loss 2.2809 | val_loss 2.2685 | lr 1.744072e-04\n",
      "step 8600 | train_loss 2.2775 | val_loss 2.2680 | lr 1.738087e-04\n",
      "step 8800 | train_loss 2.2830 | val_loss 2.2661 | lr 1.731975e-04\n",
      "step 9000 | train_loss 2.2804 | val_loss 2.2669 | lr 1.725737e-04\n",
      "step 9200 | train_loss 2.2782 | val_loss 2.2687 | lr 1.719374e-04\n",
      "step 9400 | train_loss 2.2778 | val_loss 2.2660 | lr 1.712888e-04\n",
      "step 9600 | train_loss 2.2843 | val_loss 2.2702 | lr 1.706279e-04\n",
      "step 9800 | train_loss 2.2907 | val_loss 2.2670 | lr 1.699548e-04\n",
      "step 10000 | train_loss 2.2809 | val_loss 2.2617 | lr 1.692696e-04\n",
      "step 10200 | train_loss 2.2589 | val_loss 2.2633 | lr 1.685725e-04\n",
      "step 10400 | train_loss 2.2672 | val_loss 2.2665 | lr 1.678636e-04\n",
      "step 10600 | train_loss 2.2633 | val_loss 2.2597 | lr 1.671429e-04\n",
      "step 10800 | train_loss 2.2659 | val_loss 2.2597 | lr 1.664106e-04\n",
      "step 11000 | train_loss 2.2732 | val_loss 2.2594 | lr 1.656669e-04\n",
      "step 11200 | train_loss 2.2639 | val_loss 2.2636 | lr 1.649117e-04\n",
      "step 11400 | train_loss 2.2584 | val_loss 2.2595 | lr 1.641452e-04\n",
      "step 11600 | train_loss 2.2703 | val_loss 2.2606 | lr 1.633676e-04\n",
      "step 11800 | train_loss 2.2709 | val_loss 2.2591 | lr 1.625790e-04\n",
      "step 12000 | train_loss 2.2713 | val_loss 2.2575 | lr 1.617795e-04\n",
      "step 12200 | train_loss 2.2697 | val_loss 2.2579 | lr 1.609692e-04\n",
      "step 12400 | train_loss 2.2674 | val_loss 2.2594 | lr 1.601483e-04\n",
      "step 12600 | train_loss 2.2706 | val_loss 2.2615 | lr 1.593169e-04\n",
      "step 12800 | train_loss 2.2730 | val_loss 2.2590 | lr 1.584751e-04\n",
      "step 13000 | train_loss 2.2689 | val_loss 2.2589 | lr 1.576230e-04\n",
      "step 13200 | train_loss 2.2735 | val_loss 2.2579 | lr 1.567608e-04\n",
      "step 13400 | train_loss 2.2299 | val_loss 2.2562 | lr 1.558887e-04\n",
      "step 13600 | train_loss 2.2572 | val_loss 2.2584 | lr 1.550067e-04\n",
      "step 13800 | train_loss 2.2580 | val_loss 2.2583 | lr 1.541150e-04\n",
      "step 14000 | train_loss 2.2575 | val_loss 2.2568 | lr 1.532137e-04\n",
      "step 14200 | train_loss 2.2631 | val_loss 2.2573 | lr 1.523030e-04\n",
      "step 14400 | train_loss 2.2645 | val_loss 2.2545 | lr 1.513831e-04\n",
      "step 14600 | train_loss 2.2597 | val_loss 2.2562 | lr 1.504540e-04\n",
      "step 14800 | train_loss 2.2562 | val_loss 2.2535 | lr 1.495159e-04\n",
      "step 15000 | train_loss 2.2600 | val_loss 2.2562 | lr 1.485690e-04\n",
      "step 15200 | train_loss 2.2659 | val_loss 2.2511 | lr 1.476134e-04\n",
      "step 15400 | train_loss 2.2649 | val_loss 2.2528 | lr 1.466493e-04\n",
      "step 15600 | train_loss 2.2615 | val_loss 2.2565 | lr 1.456768e-04\n",
      "step 15800 | train_loss 2.2608 | val_loss 2.2542 | lr 1.446961e-04\n",
      "step 16000 | train_loss 2.2665 | val_loss 2.2556 | lr 1.437072e-04\n",
      "step 16200 | train_loss 2.2579 | val_loss 2.2543 | lr 1.427105e-04\n",
      "step 16400 | train_loss 2.2636 | val_loss 2.2531 | lr 1.417060e-04\n",
      "step 16600 | train_loss 2.2758 | val_loss 2.2554 | lr 1.406940e-04\n",
      "step 16800 | train_loss 2.2326 | val_loss 2.2520 | lr 1.396744e-04\n",
      "step 17000 | train_loss 2.2529 | val_loss 2.2531 | lr 1.386476e-04\n",
      "step 17200 | train_loss 2.2503 | val_loss 2.2498 | lr 1.376137e-04\n",
      "step 17400 | train_loss 2.2498 | val_loss 2.2513 | lr 1.365728e-04\n",
      "step 17600 | train_loss 2.2569 | val_loss 2.2530 | lr 1.355252e-04\n",
      "step 17800 | train_loss 2.2578 | val_loss 2.2528 | lr 1.344709e-04\n",
      "step 18000 | train_loss 2.2573 | val_loss 2.2511 | lr 1.334101e-04\n",
      "step 18200 | train_loss 2.2476 | val_loss 2.2492 | lr 1.323431e-04\n",
      "step 18400 | train_loss 2.2567 | val_loss 2.2505 | lr 1.312699e-04\n",
      "step 18600 | train_loss 2.2583 | val_loss 2.2502 | lr 1.301908e-04\n",
      "step 18800 | train_loss 2.2587 | val_loss 2.2483 | lr 1.291059e-04\n",
      "step 19000 | train_loss 2.2617 | val_loss 2.2494 | lr 1.280154e-04\n",
      "step 19200 | train_loss 2.2542 | val_loss 2.2487 | lr 1.269194e-04\n",
      "step 19400 | train_loss 2.2594 | val_loss 2.2506 | lr 1.258182e-04\n",
      "step 19600 | train_loss 2.2585 | val_loss 2.2530 | lr 1.247119e-04\n",
      "step 19800 | train_loss 2.2668 | val_loss 2.2509 | lr 1.236006e-04\n",
      "step 20000 | train_loss 2.2597 | val_loss 2.2463 | lr 1.224847e-04\n",
      "step 20200 | train_loss 2.2364 | val_loss 2.2480 | lr 1.213641e-04\n",
      "step 20400 | train_loss 2.2462 | val_loss 2.2487 | lr 1.202392e-04\n",
      "step 20600 | train_loss 2.2470 | val_loss 2.2479 | lr 1.191101e-04\n",
      "step 20800 | train_loss 2.2474 | val_loss 2.2465 | lr 1.179769e-04\n",
      "step 21000 | train_loss 2.2514 | val_loss 2.2475 | lr 1.168399e-04\n",
      "step 21200 | train_loss 2.2503 | val_loss 2.2487 | lr 1.156992e-04\n",
      "step 21400 | train_loss 2.2488 | val_loss 2.2480 | lr 1.145550e-04\n",
      "step 21600 | train_loss 2.2489 | val_loss 2.2460 | lr 1.134075e-04\n",
      "step 21800 | train_loss 2.2521 | val_loss 2.2451 | lr 1.122568e-04\n",
      "step 22000 | train_loss 2.2513 | val_loss 2.2447 | lr 1.111032e-04\n",
      "step 22200 | train_loss 2.2535 | val_loss 2.2445 | lr 1.099469e-04\n",
      "step 22400 | train_loss 2.2488 | val_loss 2.2467 | lr 1.087879e-04\n",
      "step 22600 | train_loss 2.2562 | val_loss 2.2480 | lr 1.076266e-04\n",
      "step 22800 | train_loss 2.2549 | val_loss 2.2465 | lr 1.064630e-04\n",
      "step 23000 | train_loss 2.2584 | val_loss 2.2479 | lr 1.052974e-04\n",
      "step 23200 | train_loss 2.2612 | val_loss 2.2478 | lr 1.041299e-04\n",
      "step 23400 | train_loss 2.2398 | val_loss 2.2439 | lr 1.029608e-04\n",
      "step 23600 | train_loss 2.2419 | val_loss 2.2454 | lr 1.017902e-04\n",
      "step 23800 | train_loss 2.2433 | val_loss 2.2464 | lr 1.006183e-04\n",
      "step 24000 | train_loss 2.2419 | val_loss 2.2450 | lr 9.944526e-05\n",
      "step 24200 | train_loss 2.2462 | val_loss 2.2444 | lr 9.827132e-05\n",
      "step 24400 | train_loss 2.2539 | val_loss 2.2441 | lr 9.709663e-05\n",
      "step 24600 | train_loss 2.2464 | val_loss 2.2474 | lr 9.592138e-05\n",
      "step 24800 | train_loss 2.2431 | val_loss 2.2432 | lr 9.474576e-05\n",
      "step 25000 | train_loss 2.2444 | val_loss 2.2440 | lr 9.356996e-05\n",
      "step 25200 | train_loss 2.2479 | val_loss 2.2426 | lr 9.239415e-05\n",
      "step 25400 | train_loss 2.2486 | val_loss 2.2426 | lr 9.121854e-05\n",
      "step 25600 | train_loss 2.2478 | val_loss 2.2438 | lr 9.004329e-05\n",
      "step 25800 | train_loss 2.2487 | val_loss 2.2450 | lr 8.886860e-05\n",
      "step 26000 | train_loss 2.2497 | val_loss 2.2442 | lr 8.769465e-05\n",
      "step 26200 | train_loss 2.2490 | val_loss 2.2437 | lr 8.652163e-05\n",
      "step 26400 | train_loss 2.2502 | val_loss 2.2442 | lr 8.534973e-05\n",
      "step 26600 | train_loss 2.2561 | val_loss 2.2428 | lr 8.417912e-05\n",
      "step 26800 | train_loss 2.2272 | val_loss 2.2426 | lr 8.300999e-05\n",
      "step 27000 | train_loss 2.2395 | val_loss 2.2432 | lr 8.184253e-05\n",
      "step 27200 | train_loss 2.2397 | val_loss 2.2420 | lr 8.067693e-05\n",
      "step 27400 | train_loss 2.2405 | val_loss 2.2423 | lr 7.951336e-05\n",
      "step 27600 | train_loss 2.2441 | val_loss 2.2438 | lr 7.835201e-05\n",
      "step 27800 | train_loss 2.2464 | val_loss 2.2426 | lr 7.719306e-05\n",
      "step 28000 | train_loss 2.2433 | val_loss 2.2429 | lr 7.603670e-05\n",
      "step 28200 | train_loss 2.2402 | val_loss 2.2414 | lr 7.488310e-05\n",
      "step 28400 | train_loss 2.2437 | val_loss 2.2417 | lr 7.373246e-05\n",
      "step 28600 | train_loss 2.2465 | val_loss 2.2404 | lr 7.258495e-05\n",
      "step 28800 | train_loss 2.2482 | val_loss 2.2400 | lr 7.144076e-05\n",
      "step 29000 | train_loss 2.2468 | val_loss 2.2405 | lr 7.030006e-05\n",
      "step 29200 | train_loss 2.2431 | val_loss 2.2410 | lr 6.916303e-05\n",
      "step 29400 | train_loss 2.2472 | val_loss 2.2430 | lr 6.802986e-05\n",
      "step 29600 | train_loss 2.2437 | val_loss 2.2430 | lr 6.690072e-05\n",
      "step 29800 | train_loss 2.2491 | val_loss 2.2422 | lr 6.577579e-05\n",
      "step 30000 | train_loss 2.2495 | val_loss 2.2407 | lr 6.465525e-05\n",
      "step 30200 | train_loss 2.2297 | val_loss 2.2402 | lr 6.353928e-05\n",
      "step 30400 | train_loss 2.2384 | val_loss 2.2398 | lr 6.242805e-05\n",
      "step 30600 | train_loss 2.2387 | val_loss 2.2405 | lr 6.132173e-05\n",
      "step 30800 | train_loss 2.2379 | val_loss 2.2402 | lr 6.022051e-05\n",
      "step 31000 | train_loss 2.2404 | val_loss 2.2407 | lr 5.912456e-05\n",
      "step 31200 | train_loss 2.2429 | val_loss 2.2414 | lr 5.803404e-05\n",
      "step 31400 | train_loss 2.2430 | val_loss 2.2416 | lr 5.694914e-05\n",
      "step 31600 | train_loss 2.2386 | val_loss 2.2395 | lr 5.587002e-05\n",
      "step 31800 | train_loss 2.2415 | val_loss 2.2390 | lr 5.479685e-05\n",
      "step 32000 | train_loss 2.2422 | val_loss 2.2385 | lr 5.372981e-05\n",
      "step 32200 | train_loss 2.2449 | val_loss 2.2379 | lr 5.266905e-05\n",
      "step 32400 | train_loss 2.2428 | val_loss 2.2392 | lr 5.161476e-05\n",
      "step 32600 | train_loss 2.2418 | val_loss 2.2396 | lr 5.056709e-05\n",
      "step 32800 | train_loss 2.2429 | val_loss 2.2393 | lr 4.952621e-05\n",
      "step 33000 | train_loss 2.2454 | val_loss 2.2412 | lr 4.849229e-05\n",
      "step 33200 | train_loss 2.2479 | val_loss 2.2402 | lr 4.746548e-05\n",
      "step 33400 | train_loss 2.2465 | val_loss 2.2382 | lr 4.644596e-05\n",
      "step 33600 | train_loss 2.2343 | val_loss 2.2382 | lr 4.543387e-05\n",
      "step 33800 | train_loss 2.2373 | val_loss 2.2392 | lr 4.442939e-05\n",
      "step 34000 | train_loss 2.2360 | val_loss 2.2386 | lr 4.343267e-05\n",
      "step 34200 | train_loss 2.2369 | val_loss 2.2386 | lr 4.244386e-05\n",
      "step 34400 | train_loss 2.2411 | val_loss 2.2385 | lr 4.146313e-05\n",
      "step 34600 | train_loss 2.2392 | val_loss 2.2398 | lr 4.049063e-05\n",
      "step 34800 | train_loss 2.2383 | val_loss 2.2390 | lr 3.952650e-05\n",
      "step 35000 | train_loss 2.2382 | val_loss 2.2383 | lr 3.857092e-05\n",
      "step 35200 | train_loss 2.2390 | val_loss 2.2377 | lr 3.762401e-05\n",
      "step 35400 | train_loss 2.2407 | val_loss 2.2372 | lr 3.668595e-05\n",
      "step 35600 | train_loss 2.2406 | val_loss 2.2372 | lr 3.575686e-05\n",
      "step 35800 | train_loss 2.2403 | val_loss 2.2380 | lr 3.483690e-05\n",
      "step 36000 | train_loss 2.2393 | val_loss 2.2383 | lr 3.392622e-05\n",
      "step 36200 | train_loss 2.2410 | val_loss 2.2381 | lr 3.302496e-05\n",
      "step 36400 | train_loss 2.2412 | val_loss 2.2386 | lr 3.213326e-05\n",
      "step 36600 | train_loss 2.2435 | val_loss 2.2378 | lr 3.125126e-05\n",
      "step 36800 | train_loss 2.2328 | val_loss 2.2377 | lr 3.037910e-05\n",
      "step 37000 | train_loss 2.2352 | val_loss 2.2376 | lr 2.951691e-05\n",
      "step 37200 | train_loss 2.2363 | val_loss 2.2378 | lr 2.866485e-05\n",
      "step 37400 | train_loss 2.2361 | val_loss 2.2377 | lr 2.782303e-05\n",
      "step 37600 | train_loss 2.2365 | val_loss 2.2379 | lr 2.699159e-05\n",
      "step 37800 | train_loss 2.2391 | val_loss 2.2377 | lr 2.617067e-05\n",
      "step 38000 | train_loss 2.2379 | val_loss 2.2381 | lr 2.536039e-05\n",
      "step 38200 | train_loss 2.2368 | val_loss 2.2376 | lr 2.456089e-05\n",
      "step 38400 | train_loss 2.2367 | val_loss 2.2374 | lr 2.377228e-05\n",
      "step 38600 | train_loss 2.2384 | val_loss 2.2360 | lr 2.299469e-05\n",
      "step 38800 | train_loss 2.2395 | val_loss 2.2365 | lr 2.222824e-05\n",
      "step 39000 | train_loss 2.2387 | val_loss 2.2367 | lr 2.147307e-05\n",
      "step 39200 | train_loss 2.2383 | val_loss 2.2368 | lr 2.072927e-05\n",
      "step 39400 | train_loss 2.2392 | val_loss 2.2374 | lr 1.999698e-05\n",
      "step 39600 | train_loss 2.2383 | val_loss 2.2371 | lr 1.927631e-05\n",
      "step 39800 | train_loss 2.2394 | val_loss 2.2373 | lr 1.856737e-05\n",
      "step 40000 | train_loss 2.2420 | val_loss 2.2369 | lr 1.787027e-05\n",
      "step 40200 | train_loss 2.2344 | val_loss 2.2367 | lr 1.718513e-05\n",
      "step 40400 | train_loss 2.2362 | val_loss 2.2365 | lr 1.651205e-05\n",
      "step 40600 | train_loss 2.2362 | val_loss 2.2368 | lr 1.585113e-05\n",
      "step 40800 | train_loss 2.2358 | val_loss 2.2369 | lr 1.520249e-05\n",
      "step 41000 | train_loss 2.2358 | val_loss 2.2370 | lr 1.456623e-05\n",
      "step 41200 | train_loss 2.2373 | val_loss 2.2370 | lr 1.394244e-05\n",
      "step 41400 | train_loss 2.2376 | val_loss 2.2371 | lr 1.333122e-05\n",
      "step 41600 | train_loss 2.2363 | val_loss 2.2366 | lr 1.273268e-05\n",
      "step 41800 | train_loss 2.2364 | val_loss 2.2364 | lr 1.214690e-05\n",
      "step 42000 | train_loss 2.2367 | val_loss 2.2361 | lr 1.157398e-05\n",
      "step 42200 | train_loss 2.2378 | val_loss 2.2360 | lr 1.101401e-05\n",
      "step 42400 | train_loss 2.2379 | val_loss 2.2361 | lr 1.046707e-05\n",
      "step 42600 | train_loss 2.2368 | val_loss 2.2362 | lr 9.933254e-06\n",
      "step 42800 | train_loss 2.2375 | val_loss 2.2364 | lr 9.412646e-06\n",
      "step 43000 | train_loss 2.2376 | val_loss 2.2367 | lr 8.905329e-06\n",
      "step 43200 | train_loss 2.2385 | val_loss 2.2365 | lr 8.411381e-06\n",
      "step 43400 | train_loss 2.2391 | val_loss 2.2363 | lr 7.930880e-06\n",
      "step 43600 | train_loss 2.2359 | val_loss 2.2363 | lr 7.463903e-06\n",
      "step 43800 | train_loss 2.2365 | val_loss 2.2363 | lr 7.010523e-06\n",
      "step 44000 | train_loss 2.2365 | val_loss 2.2363 | lr 6.570811e-06\n",
      "step 44200 | train_loss 2.2361 | val_loss 2.2363 | lr 6.144838e-06\n",
      "step 44400 | train_loss 2.2364 | val_loss 2.2364 | lr 5.732670e-06\n",
      "step 44600 | train_loss 2.2367 | val_loss 2.2364 | lr 5.334373e-06\n",
      "step 44800 | train_loss 2.2367 | val_loss 2.2365 | lr 4.950009e-06\n",
      "step 45000 | train_loss 2.2365 | val_loss 2.2363 | lr 4.579640e-06\n",
      "step 45200 | train_loss 2.2364 | val_loss 2.2361 | lr 4.223323e-06\n",
      "step 45400 | train_loss 2.2365 | val_loss 2.2360 | lr 3.881114e-06\n",
      "step 45600 | train_loss 2.2367 | val_loss 2.2359 | lr 3.553069e-06\n",
      "step 45800 | train_loss 2.2367 | val_loss 2.2360 | lr 3.239239e-06\n",
      "step 46000 | train_loss 2.2366 | val_loss 2.2360 | lr 2.939672e-06\n",
      "step 46200 | train_loss 2.2368 | val_loss 2.2361 | lr 2.654418e-06\n",
      "step 46400 | train_loss 2.2368 | val_loss 2.2362 | lr 2.383519e-06\n",
      "step 46600 | train_loss 2.2370 | val_loss 2.2361 | lr 2.127021e-06\n",
      "step 46800 | train_loss 2.2366 | val_loss 2.2361 | lr 1.884962e-06\n",
      "step 47000 | train_loss 2.2365 | val_loss 2.2361 | lr 1.657381e-06\n",
      "step 47200 | train_loss 2.2366 | val_loss 2.2361 | lr 1.444315e-06\n",
      "step 47400 | train_loss 2.2365 | val_loss 2.2361 | lr 1.245796e-06\n",
      "step 47600 | train_loss 2.2365 | val_loss 2.2361 | lr 1.061856e-06\n",
      "step 47800 | train_loss 2.2366 | val_loss 2.2361 | lr 8.925240e-07\n",
      "step 48000 | train_loss 2.2366 | val_loss 2.2362 | lr 7.378271e-07\n",
      "step 48200 | train_loss 2.2366 | val_loss 2.2361 | lr 5.977894e-07\n",
      "step 48400 | train_loss 2.2366 | val_loss 2.2361 | lr 4.724331e-07\n",
      "step 48600 | train_loss 2.2366 | val_loss 2.2361 | lr 3.617780e-07\n",
      "step 48800 | train_loss 2.2366 | val_loss 2.2361 | lr 2.658416e-07\n",
      "step 49000 | train_loss 2.2366 | val_loss 2.2361 | lr 1.846389e-07\n",
      "step 49200 | train_loss 2.2366 | val_loss 2.2361 | lr 1.181829e-07\n",
      "step 49400 | train_loss 2.2366 | val_loss 2.2361 | lr 6.648402e-08\n",
      "step 49600 | train_loss 2.2366 | val_loss 2.2361 | lr 2.955040e-08\n",
      "step 49800 | train_loss 2.2366 | val_loss 2.2361 | lr 7.387891e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:10:36,750] Trial 9 finished with value: 2.236103798661913 and parameters: {'embedding_size': 57, 'hidden_size': 945, 'learning_rate': 0.00018713991693097823, 'batch_size': 64, 'context_length': 2}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.2366 | val_loss 2.2361 | lr 1.846997e-13\n",
      "step 0 | train_loss 3.3121 | val_loss 3.3107 | lr 8.316679e-03\n",
      "step 200 | train_loss 2.3933 | val_loss 2.3870 | lr 8.316350e-03\n",
      "step 400 | train_loss 2.3001 | val_loss 2.3121 | lr 8.315365e-03\n",
      "step 600 | train_loss 2.2760 | val_loss 2.2836 | lr 8.313724e-03\n",
      "step 800 | train_loss 2.2774 | val_loss 2.2533 | lr 8.311426e-03\n",
      "step 1000 | train_loss 2.2648 | val_loss 2.2505 | lr 8.308473e-03\n",
      "step 1200 | train_loss 2.2331 | val_loss 2.2454 | lr 8.304864e-03\n",
      "step 1400 | train_loss 2.2577 | val_loss 2.2431 | lr 8.300601e-03\n",
      "step 1600 | train_loss 2.2451 | val_loss 2.2164 | lr 8.295683e-03\n",
      "step 1800 | train_loss 2.2519 | val_loss 2.2297 | lr 8.290112e-03\n",
      "step 2000 | train_loss 2.2413 | val_loss 2.2155 | lr 8.283889e-03\n",
      "step 2200 | train_loss 2.2249 | val_loss 2.1991 | lr 8.277014e-03\n",
      "step 2400 | train_loss 2.2315 | val_loss 2.2036 | lr 8.269489e-03\n",
      "step 2600 | train_loss 2.2329 | val_loss 2.1998 | lr 8.261314e-03\n",
      "step 2800 | train_loss 2.2110 | val_loss 2.1987 | lr 8.252492e-03\n",
      "step 3000 | train_loss 2.2227 | val_loss 2.1865 | lr 8.243023e-03\n",
      "step 3200 | train_loss 2.2378 | val_loss 2.1976 | lr 8.232909e-03\n",
      "step 3400 | train_loss 1.9926 | val_loss 2.1901 | lr 8.222152e-03\n",
      "step 3600 | train_loss 2.1358 | val_loss 2.1811 | lr 8.210753e-03\n",
      "step 3800 | train_loss 2.1530 | val_loss 2.1745 | lr 8.198714e-03\n",
      "step 4000 | train_loss 2.1486 | val_loss 2.1787 | lr 8.186037e-03\n",
      "step 4200 | train_loss 2.1957 | val_loss 2.2000 | lr 8.172724e-03\n",
      "step 4400 | train_loss 2.1755 | val_loss 2.1880 | lr 8.158777e-03\n",
      "step 4600 | train_loss 2.1808 | val_loss 2.1907 | lr 8.144198e-03\n",
      "step 4800 | train_loss 2.1647 | val_loss 2.1645 | lr 8.128990e-03\n",
      "step 5000 | train_loss 2.1851 | val_loss 2.1783 | lr 8.113155e-03\n",
      "step 5200 | train_loss 2.1974 | val_loss 2.1804 | lr 8.096695e-03\n",
      "step 5400 | train_loss 2.1894 | val_loss 2.1704 | lr 8.079614e-03\n",
      "step 5600 | train_loss 2.1867 | val_loss 2.1759 | lr 8.061913e-03\n",
      "step 5800 | train_loss 2.1750 | val_loss 2.1776 | lr 8.043596e-03\n",
      "step 6000 | train_loss 2.1964 | val_loss 2.1725 | lr 8.024665e-03\n",
      "step 6200 | train_loss 2.1692 | val_loss 2.1708 | lr 8.005124e-03\n",
      "step 6400 | train_loss 2.1977 | val_loss 2.1624 | lr 7.984976e-03\n",
      "step 6600 | train_loss 2.1548 | val_loss 2.1588 | lr 7.964223e-03\n",
      "step 6800 | train_loss 1.9901 | val_loss 2.1538 | lr 7.942869e-03\n",
      "step 7000 | train_loss 2.0910 | val_loss 2.1443 | lr 7.920917e-03\n",
      "step 7200 | train_loss 2.1236 | val_loss 2.1566 | lr 7.898372e-03\n",
      "step 7400 | train_loss 2.1259 | val_loss 2.1534 | lr 7.875235e-03\n",
      "step 7600 | train_loss 2.1635 | val_loss 2.1539 | lr 7.851512e-03\n",
      "step 7800 | train_loss 2.1394 | val_loss 2.1625 | lr 7.827206e-03\n",
      "step 8000 | train_loss 2.1370 | val_loss 2.1728 | lr 7.802320e-03\n",
      "step 8200 | train_loss 2.1420 | val_loss 2.1453 | lr 7.776859e-03\n",
      "step 8400 | train_loss 2.1711 | val_loss 2.1468 | lr 7.750826e-03\n",
      "step 8600 | train_loss 2.1526 | val_loss 2.1508 | lr 7.724226e-03\n",
      "step 8800 | train_loss 2.1565 | val_loss 2.1376 | lr 7.697063e-03\n",
      "step 9000 | train_loss 2.1715 | val_loss 2.1414 | lr 7.669341e-03\n",
      "step 9200 | train_loss 2.1653 | val_loss 2.1537 | lr 7.641065e-03\n",
      "step 9400 | train_loss 2.1589 | val_loss 2.1515 | lr 7.612239e-03\n",
      "step 9600 | train_loss 2.1600 | val_loss 2.1578 | lr 7.582867e-03\n",
      "step 9800 | train_loss 2.1737 | val_loss 2.1516 | lr 7.552955e-03\n",
      "step 10000 | train_loss 2.1563 | val_loss 2.1469 | lr 7.522506e-03\n",
      "step 10200 | train_loss 2.0461 | val_loss 2.1318 | lr 7.491527e-03\n",
      "step 10400 | train_loss 2.0680 | val_loss 2.1253 | lr 7.460021e-03\n",
      "step 10600 | train_loss 2.0990 | val_loss 2.1263 | lr 7.427993e-03\n",
      "step 10800 | train_loss 2.1032 | val_loss 2.1310 | lr 7.395450e-03\n",
      "step 11000 | train_loss 2.1190 | val_loss 2.1339 | lr 7.362395e-03\n",
      "step 11200 | train_loss 2.1068 | val_loss 2.1457 | lr 7.328834e-03\n",
      "step 11400 | train_loss 2.1065 | val_loss 2.1392 | lr 7.294772e-03\n",
      "step 11600 | train_loss 2.1441 | val_loss 2.1587 | lr 7.260216e-03\n",
      "step 11800 | train_loss 2.1457 | val_loss 2.1586 | lr 7.225169e-03\n",
      "step 12000 | train_loss 2.1313 | val_loss 2.1419 | lr 7.189638e-03\n",
      "step 12200 | train_loss 2.1285 | val_loss 2.1406 | lr 7.153629e-03\n",
      "step 12400 | train_loss 2.1730 | val_loss 2.1393 | lr 7.117146e-03\n",
      "step 12600 | train_loss 2.1414 | val_loss 2.1231 | lr 7.080196e-03\n",
      "step 12800 | train_loss 2.1275 | val_loss 2.1178 | lr 7.042785e-03\n",
      "step 13000 | train_loss 2.1284 | val_loss 2.1286 | lr 7.004918e-03\n",
      "step 13200 | train_loss 2.1176 | val_loss 2.1197 | lr 6.966602e-03\n",
      "step 13400 | train_loss 1.8904 | val_loss 2.1250 | lr 6.927843e-03\n",
      "step 13600 | train_loss 2.0069 | val_loss 2.1227 | lr 6.888646e-03\n",
      "step 13800 | train_loss 2.0717 | val_loss 2.1234 | lr 6.849018e-03\n",
      "step 14000 | train_loss 2.0810 | val_loss 2.1278 | lr 6.808965e-03\n",
      "step 14200 | train_loss 2.0851 | val_loss 2.1294 | lr 6.768493e-03\n",
      "step 14400 | train_loss 2.0869 | val_loss 2.1180 | lr 6.727609e-03\n",
      "step 14600 | train_loss 2.0807 | val_loss 2.1234 | lr 6.686320e-03\n",
      "step 14800 | train_loss 2.0745 | val_loss 2.1186 | lr 6.644631e-03\n",
      "step 15000 | train_loss 2.1111 | val_loss 2.1159 | lr 6.602550e-03\n",
      "step 15200 | train_loss 2.1308 | val_loss 2.1323 | lr 6.560083e-03\n",
      "step 15400 | train_loss 2.1067 | val_loss 2.1206 | lr 6.517236e-03\n",
      "step 15600 | train_loss 2.0961 | val_loss 2.1249 | lr 6.474017e-03\n",
      "step 15800 | train_loss 2.1005 | val_loss 2.1141 | lr 6.430432e-03\n",
      "step 16000 | train_loss 2.1374 | val_loss 2.1344 | lr 6.386489e-03\n",
      "step 16200 | train_loss 2.1035 | val_loss 2.1150 | lr 6.342194e-03\n",
      "step 16400 | train_loss 2.1225 | val_loss 2.1102 | lr 6.297553e-03\n",
      "step 16600 | train_loss 2.1344 | val_loss 2.1228 | lr 6.252575e-03\n",
      "step 16800 | train_loss 1.9363 | val_loss 2.1256 | lr 6.207267e-03\n",
      "step 17000 | train_loss 2.0124 | val_loss 2.1219 | lr 6.161635e-03\n",
      "step 17200 | train_loss 2.0355 | val_loss 2.0997 | lr 6.115686e-03\n",
      "step 17400 | train_loss 2.0394 | val_loss 2.1123 | lr 6.069428e-03\n",
      "step 17600 | train_loss 2.0688 | val_loss 2.1186 | lr 6.022869e-03\n",
      "step 17800 | train_loss 2.0571 | val_loss 2.1038 | lr 5.976015e-03\n",
      "step 18000 | train_loss 2.0639 | val_loss 2.1061 | lr 5.928874e-03\n",
      "step 18200 | train_loss 2.0625 | val_loss 2.1112 | lr 5.881454e-03\n",
      "step 18400 | train_loss 2.0624 | val_loss 2.1047 | lr 5.833761e-03\n",
      "step 18600 | train_loss 2.0652 | val_loss 2.1100 | lr 5.785804e-03\n",
      "step 18800 | train_loss 2.0899 | val_loss 2.0956 | lr 5.737590e-03\n",
      "step 19000 | train_loss 2.0831 | val_loss 2.1011 | lr 5.689126e-03\n",
      "step 19200 | train_loss 2.0859 | val_loss 2.0990 | lr 5.640421e-03\n",
      "step 19400 | train_loss 2.0833 | val_loss 2.1019 | lr 5.591482e-03\n",
      "step 19600 | train_loss 2.0923 | val_loss 2.1146 | lr 5.542316e-03\n",
      "step 19800 | train_loss 2.1063 | val_loss 2.0975 | lr 5.492932e-03\n",
      "step 20000 | train_loss 2.0706 | val_loss 2.1089 | lr 5.443337e-03\n",
      "step 20200 | train_loss 1.9501 | val_loss 2.1089 | lr 5.393539e-03\n",
      "step 20400 | train_loss 1.9960 | val_loss 2.1076 | lr 5.343546e-03\n",
      "step 20600 | train_loss 2.0288 | val_loss 2.1005 | lr 5.293366e-03\n",
      "step 20800 | train_loss 2.0326 | val_loss 2.0981 | lr 5.243007e-03\n",
      "step 21000 | train_loss 2.0461 | val_loss 2.0933 | lr 5.192476e-03\n",
      "step 21200 | train_loss 2.0386 | val_loss 2.0993 | lr 5.141782e-03\n",
      "step 21400 | train_loss 2.0535 | val_loss 2.1086 | lr 5.090933e-03\n",
      "step 21600 | train_loss 2.0462 | val_loss 2.0949 | lr 5.039937e-03\n",
      "step 21800 | train_loss 2.0552 | val_loss 2.0911 | lr 4.988801e-03\n",
      "step 22000 | train_loss 2.0447 | val_loss 2.0949 | lr 4.937534e-03\n",
      "step 22200 | train_loss 2.0761 | val_loss 2.1004 | lr 4.886145e-03\n",
      "step 22400 | train_loss 2.0653 | val_loss 2.0965 | lr 4.834640e-03\n",
      "step 22600 | train_loss 2.0737 | val_loss 2.0902 | lr 4.783028e-03\n",
      "step 22800 | train_loss 2.0505 | val_loss 2.0981 | lr 4.731318e-03\n",
      "step 23000 | train_loss 2.0563 | val_loss 2.0857 | lr 4.679517e-03\n",
      "step 23200 | train_loss 2.0590 | val_loss 2.0857 | lr 4.627634e-03\n",
      "step 23400 | train_loss 1.9167 | val_loss 2.0893 | lr 4.575677e-03\n",
      "step 23600 | train_loss 1.9527 | val_loss 2.1027 | lr 4.523654e-03\n",
      "step 23800 | train_loss 1.9712 | val_loss 2.0844 | lr 4.471574e-03\n",
      "step 24000 | train_loss 1.9824 | val_loss 2.0807 | lr 4.419444e-03\n",
      "step 24200 | train_loss 1.9933 | val_loss 2.0888 | lr 4.367272e-03\n",
      "step 24400 | train_loss 2.0011 | val_loss 2.0885 | lr 4.315068e-03\n",
      "step 24600 | train_loss 2.0046 | val_loss 2.0944 | lr 4.262839e-03\n",
      "step 24800 | train_loss 2.0019 | val_loss 2.0838 | lr 4.210593e-03\n",
      "step 25000 | train_loss 2.0182 | val_loss 2.0891 | lr 4.158339e-03\n",
      "step 25200 | train_loss 2.0243 | val_loss 2.0833 | lr 4.106085e-03\n",
      "step 25400 | train_loss 2.0336 | val_loss 2.0887 | lr 4.053840e-03\n",
      "step 25600 | train_loss 2.0430 | val_loss 2.0886 | lr 4.001611e-03\n",
      "step 25800 | train_loss 2.0341 | val_loss 2.0829 | lr 3.949406e-03\n",
      "step 26000 | train_loss 2.0383 | val_loss 2.0827 | lr 3.897235e-03\n",
      "step 26200 | train_loss 2.0396 | val_loss 2.0763 | lr 3.845105e-03\n",
      "step 26400 | train_loss 2.0487 | val_loss 2.0750 | lr 3.793024e-03\n",
      "step 26600 | train_loss 2.0357 | val_loss 2.0825 | lr 3.741001e-03\n",
      "step 26800 | train_loss 1.8755 | val_loss 2.0711 | lr 3.689044e-03\n",
      "step 27000 | train_loss 1.9364 | val_loss 2.0721 | lr 3.637161e-03\n",
      "step 27200 | train_loss 1.9426 | val_loss 2.0742 | lr 3.585361e-03\n",
      "step 27400 | train_loss 1.9627 | val_loss 2.0790 | lr 3.533650e-03\n",
      "step 27600 | train_loss 1.9798 | val_loss 2.0786 | lr 3.482039e-03\n",
      "step 27800 | train_loss 1.9741 | val_loss 2.0764 | lr 3.430534e-03\n",
      "step 28000 | train_loss 1.9821 | val_loss 2.0746 | lr 3.379144e-03\n",
      "step 28200 | train_loss 1.9837 | val_loss 2.0762 | lr 3.327877e-03\n",
      "step 28400 | train_loss 1.9905 | val_loss 2.0705 | lr 3.276742e-03\n",
      "step 28600 | train_loss 2.0028 | val_loss 2.0737 | lr 3.225745e-03\n",
      "step 28800 | train_loss 2.0117 | val_loss 2.0791 | lr 3.174896e-03\n",
      "step 29000 | train_loss 2.0115 | val_loss 2.0838 | lr 3.124202e-03\n",
      "step 29200 | train_loss 2.0038 | val_loss 2.0715 | lr 3.073672e-03\n",
      "step 29400 | train_loss 2.0153 | val_loss 2.0737 | lr 3.023313e-03\n",
      "step 29600 | train_loss 2.0058 | val_loss 2.0733 | lr 2.973133e-03\n",
      "step 29800 | train_loss 2.0181 | val_loss 2.0660 | lr 2.923140e-03\n",
      "step 30000 | train_loss 1.9954 | val_loss 2.0766 | lr 2.873342e-03\n",
      "step 30200 | train_loss 1.8793 | val_loss 2.0680 | lr 2.823747e-03\n",
      "step 30400 | train_loss 1.9305 | val_loss 2.0645 | lr 2.774363e-03\n",
      "step 30600 | train_loss 1.9441 | val_loss 2.0694 | lr 2.725197e-03\n",
      "step 30800 | train_loss 1.9382 | val_loss 2.0633 | lr 2.676258e-03\n",
      "step 31000 | train_loss 1.9584 | val_loss 2.0676 | lr 2.627553e-03\n",
      "step 31200 | train_loss 1.9630 | val_loss 2.0681 | lr 2.579089e-03\n",
      "step 31400 | train_loss 1.9664 | val_loss 2.0730 | lr 2.530875e-03\n",
      "step 31600 | train_loss 1.9666 | val_loss 2.0641 | lr 2.482918e-03\n",
      "step 31800 | train_loss 1.9775 | val_loss 2.0631 | lr 2.435225e-03\n",
      "step 32000 | train_loss 1.9898 | val_loss 2.0703 | lr 2.387805e-03\n",
      "step 32200 | train_loss 1.9894 | val_loss 2.0629 | lr 2.340664e-03\n",
      "step 32400 | train_loss 1.9754 | val_loss 2.0643 | lr 2.293810e-03\n",
      "step 32600 | train_loss 1.9799 | val_loss 2.0688 | lr 2.247250e-03\n",
      "step 32800 | train_loss 1.9888 | val_loss 2.0590 | lr 2.200993e-03\n",
      "step 33000 | train_loss 1.9883 | val_loss 2.0608 | lr 2.155044e-03\n",
      "step 33200 | train_loss 1.9923 | val_loss 2.0598 | lr 2.109412e-03\n",
      "step 33400 | train_loss 1.9743 | val_loss 2.0600 | lr 2.064103e-03\n",
      "step 33600 | train_loss 1.8955 | val_loss 2.0598 | lr 2.019125e-03\n",
      "step 33800 | train_loss 1.9137 | val_loss 2.0581 | lr 1.974485e-03\n",
      "step 34000 | train_loss 1.9359 | val_loss 2.0596 | lr 1.930190e-03\n",
      "step 34200 | train_loss 1.9289 | val_loss 2.0547 | lr 1.886246e-03\n",
      "step 34400 | train_loss 1.9385 | val_loss 2.0583 | lr 1.842662e-03\n",
      "step 34600 | train_loss 1.9458 | val_loss 2.0635 | lr 1.799443e-03\n",
      "step 34800 | train_loss 1.9343 | val_loss 2.0609 | lr 1.756596e-03\n",
      "step 35000 | train_loss 1.9492 | val_loss 2.0595 | lr 1.714129e-03\n",
      "step 35200 | train_loss 1.9498 | val_loss 2.0608 | lr 1.672048e-03\n",
      "step 35400 | train_loss 1.9574 | val_loss 2.0570 | lr 1.630359e-03\n",
      "step 35600 | train_loss 1.9672 | val_loss 2.0631 | lr 1.589069e-03\n",
      "step 35800 | train_loss 1.9649 | val_loss 2.0605 | lr 1.548186e-03\n",
      "step 36000 | train_loss 1.9569 | val_loss 2.0626 | lr 1.507714e-03\n",
      "step 36200 | train_loss 1.9634 | val_loss 2.0507 | lr 1.467661e-03\n",
      "step 36400 | train_loss 1.9567 | val_loss 2.0540 | lr 1.428033e-03\n",
      "step 36600 | train_loss 1.9667 | val_loss 2.0528 | lr 1.388836e-03\n",
      "step 36800 | train_loss 1.8734 | val_loss 2.0521 | lr 1.350076e-03\n",
      "step 37000 | train_loss 1.8936 | val_loss 2.0548 | lr 1.311760e-03\n",
      "step 37200 | train_loss 1.9125 | val_loss 2.0557 | lr 1.273894e-03\n",
      "step 37400 | train_loss 1.9212 | val_loss 2.0526 | lr 1.236482e-03\n",
      "step 37600 | train_loss 1.9255 | val_loss 2.0553 | lr 1.199533e-03\n",
      "step 37800 | train_loss 1.9273 | val_loss 2.0508 | lr 1.163050e-03\n",
      "step 38000 | train_loss 1.9238 | val_loss 2.0560 | lr 1.127040e-03\n",
      "step 38200 | train_loss 1.9212 | val_loss 2.0546 | lr 1.091510e-03\n",
      "step 38400 | train_loss 1.9276 | val_loss 2.0514 | lr 1.056463e-03\n",
      "step 38600 | train_loss 1.9387 | val_loss 2.0537 | lr 1.021906e-03\n",
      "step 38800 | train_loss 1.9379 | val_loss 2.0494 | lr 9.878446e-04\n",
      "step 39000 | train_loss 1.9450 | val_loss 2.0498 | lr 9.542838e-04\n",
      "step 39200 | train_loss 1.9374 | val_loss 2.0472 | lr 9.212289e-04\n",
      "step 39400 | train_loss 1.9411 | val_loss 2.0498 | lr 8.886852e-04\n",
      "step 39600 | train_loss 1.9444 | val_loss 2.0453 | lr 8.566578e-04\n",
      "step 39800 | train_loss 1.9407 | val_loss 2.0456 | lr 8.251518e-04\n",
      "step 40000 | train_loss 1.9444 | val_loss 2.0510 | lr 7.941721e-04\n",
      "step 40200 | train_loss 1.8877 | val_loss 2.0471 | lr 7.637237e-04\n",
      "step 40400 | train_loss 1.8945 | val_loss 2.0499 | lr 7.338113e-04\n",
      "step 40600 | train_loss 1.9037 | val_loss 2.0487 | lr 7.044397e-04\n",
      "step 40800 | train_loss 1.9076 | val_loss 2.0486 | lr 6.756135e-04\n",
      "step 41000 | train_loss 1.9129 | val_loss 2.0509 | lr 6.473373e-04\n",
      "step 41200 | train_loss 1.9088 | val_loss 2.0456 | lr 6.196155e-04\n",
      "step 41400 | train_loss 1.9126 | val_loss 2.0487 | lr 5.924525e-04\n",
      "step 41600 | train_loss 1.9115 | val_loss 2.0469 | lr 5.658526e-04\n",
      "step 41800 | train_loss 1.9121 | val_loss 2.0446 | lr 5.398200e-04\n",
      "step 42000 | train_loss 1.9179 | val_loss 2.0447 | lr 5.143588e-04\n",
      "step 42200 | train_loss 1.9212 | val_loss 2.0440 | lr 4.894730e-04\n",
      "step 42400 | train_loss 1.9254 | val_loss 2.0425 | lr 4.651666e-04\n",
      "step 42600 | train_loss 1.9216 | val_loss 2.0421 | lr 4.414434e-04\n",
      "step 42800 | train_loss 1.9246 | val_loss 2.0425 | lr 4.183071e-04\n",
      "step 43000 | train_loss 1.9262 | val_loss 2.0416 | lr 3.957614e-04\n",
      "step 43200 | train_loss 1.9266 | val_loss 2.0410 | lr 3.738099e-04\n",
      "step 43400 | train_loss 1.9208 | val_loss 2.0434 | lr 3.524560e-04\n",
      "step 43600 | train_loss 1.8956 | val_loss 2.0427 | lr 3.317030e-04\n",
      "step 43800 | train_loss 1.8993 | val_loss 2.0450 | lr 3.115544e-04\n",
      "step 44000 | train_loss 1.9010 | val_loss 2.0419 | lr 2.920132e-04\n",
      "step 44200 | train_loss 1.9019 | val_loss 2.0420 | lr 2.730825e-04\n",
      "step 44400 | train_loss 1.9041 | val_loss 2.0428 | lr 2.547654e-04\n",
      "step 44600 | train_loss 1.9036 | val_loss 2.0413 | lr 2.370647e-04\n",
      "step 44800 | train_loss 1.9050 | val_loss 2.0417 | lr 2.199832e-04\n",
      "step 45000 | train_loss 1.9066 | val_loss 2.0400 | lr 2.035236e-04\n",
      "step 45200 | train_loss 1.9064 | val_loss 2.0397 | lr 1.876885e-04\n",
      "step 45400 | train_loss 1.9069 | val_loss 2.0398 | lr 1.724805e-04\n",
      "step 45600 | train_loss 1.9096 | val_loss 2.0395 | lr 1.579018e-04\n",
      "step 45800 | train_loss 1.9089 | val_loss 2.0392 | lr 1.439549e-04\n",
      "step 46000 | train_loss 1.9081 | val_loss 2.0395 | lr 1.306419e-04\n",
      "step 46200 | train_loss 1.9098 | val_loss 2.0392 | lr 1.179649e-04\n",
      "step 46400 | train_loss 1.9115 | val_loss 2.0385 | lr 1.059259e-04\n",
      "step 46600 | train_loss 1.9118 | val_loss 2.0385 | lr 9.452686e-05\n",
      "step 46800 | train_loss 1.9042 | val_loss 2.0396 | lr 8.376953e-05\n",
      "step 47000 | train_loss 1.9028 | val_loss 2.0397 | lr 7.365562e-05\n",
      "step 47200 | train_loss 1.9028 | val_loss 2.0403 | lr 6.418673e-05\n",
      "step 47400 | train_loss 1.9032 | val_loss 2.0402 | lr 5.536436e-05\n",
      "step 47600 | train_loss 1.9028 | val_loss 2.0401 | lr 4.718990e-05\n",
      "step 47800 | train_loss 1.9030 | val_loss 2.0403 | lr 3.966463e-05\n",
      "step 48000 | train_loss 1.9031 | val_loss 2.0403 | lr 3.278975e-05\n",
      "step 48200 | train_loss 1.9030 | val_loss 2.0402 | lr 2.656634e-05\n",
      "step 48400 | train_loss 1.9032 | val_loss 2.0397 | lr 2.099538e-05\n",
      "step 48600 | train_loss 1.9032 | val_loss 2.0396 | lr 1.607777e-05\n",
      "step 48800 | train_loss 1.9035 | val_loss 2.0395 | lr 1.181426e-05\n",
      "step 49000 | train_loss 1.9037 | val_loss 2.0394 | lr 8.205533e-06\n",
      "step 49200 | train_loss 1.9037 | val_loss 2.0394 | lr 5.252163e-06\n",
      "step 49400 | train_loss 1.9036 | val_loss 2.0393 | lr 2.954614e-06\n",
      "step 49600 | train_loss 1.9037 | val_loss 2.0393 | lr 1.313248e-06\n",
      "step 49800 | train_loss 1.9037 | val_loss 2.0393 | lr 3.283250e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:12:31,579] Trial 10 finished with value: 2.0393022000789642 and parameters: {'embedding_size': 16, 'hidden_size': 269, 'learning_rate': 0.0083166786208697, 'batch_size': 64, 'context_length': 5}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.9037 | val_loss 2.0393 | lr 8.208233e-12\n",
      "step 0 | train_loss 3.3157 | val_loss 3.3121 | lr 7.811120e-03\n",
      "step 200 | train_loss 2.3547 | val_loss 2.3664 | lr 7.810812e-03\n",
      "step 400 | train_loss 2.3193 | val_loss 2.3167 | lr 7.809887e-03\n",
      "step 600 | train_loss 2.2667 | val_loss 2.2803 | lr 7.808345e-03\n",
      "step 800 | train_loss 2.2691 | val_loss 2.2448 | lr 7.806187e-03\n",
      "step 1000 | train_loss 2.2782 | val_loss 2.2517 | lr 7.803414e-03\n",
      "step 1200 | train_loss 2.2324 | val_loss 2.2499 | lr 7.800024e-03\n",
      "step 1400 | train_loss 2.2479 | val_loss 2.2351 | lr 7.796020e-03\n",
      "step 1600 | train_loss 2.2165 | val_loss 2.2181 | lr 7.791401e-03\n",
      "step 1800 | train_loss 2.2420 | val_loss 2.2155 | lr 7.786169e-03\n",
      "step 2000 | train_loss 2.2334 | val_loss 2.2186 | lr 7.780324e-03\n",
      "step 2200 | train_loss 2.2207 | val_loss 2.2126 | lr 7.773867e-03\n",
      "step 2400 | train_loss 2.2147 | val_loss 2.1984 | lr 7.766799e-03\n",
      "step 2600 | train_loss 2.2268 | val_loss 2.1981 | lr 7.759121e-03\n",
      "step 2800 | train_loss 2.2250 | val_loss 2.2002 | lr 7.750835e-03\n",
      "step 3000 | train_loss 2.1997 | val_loss 2.1826 | lr 7.741942e-03\n",
      "step 3200 | train_loss 2.2299 | val_loss 2.2020 | lr 7.732443e-03\n",
      "step 3400 | train_loss 1.9740 | val_loss 2.1830 | lr 7.722340e-03\n",
      "step 3600 | train_loss 2.1186 | val_loss 2.1869 | lr 7.711633e-03\n",
      "step 3800 | train_loss 2.1363 | val_loss 2.1730 | lr 7.700326e-03\n",
      "step 4000 | train_loss 2.1362 | val_loss 2.1658 | lr 7.688420e-03\n",
      "step 4200 | train_loss 2.1795 | val_loss 2.1958 | lr 7.675916e-03\n",
      "step 4400 | train_loss 2.1505 | val_loss 2.1741 | lr 7.662817e-03\n",
      "step 4600 | train_loss 2.1608 | val_loss 2.1810 | lr 7.649125e-03\n",
      "step 4800 | train_loss 2.1645 | val_loss 2.1540 | lr 7.634841e-03\n",
      "step 5000 | train_loss 2.1859 | val_loss 2.1801 | lr 7.619969e-03\n",
      "step 5200 | train_loss 2.1787 | val_loss 2.1661 | lr 7.604509e-03\n",
      "step 5400 | train_loss 2.1828 | val_loss 2.1570 | lr 7.588466e-03\n",
      "step 5600 | train_loss 2.1895 | val_loss 2.1666 | lr 7.571842e-03\n",
      "step 5800 | train_loss 2.1914 | val_loss 2.1686 | lr 7.554638e-03\n",
      "step 6000 | train_loss 2.2009 | val_loss 2.1640 | lr 7.536858e-03\n",
      "step 6200 | train_loss 2.1811 | val_loss 2.1690 | lr 7.518505e-03\n",
      "step 6400 | train_loss 2.1970 | val_loss 2.1574 | lr 7.499581e-03\n",
      "step 6600 | train_loss 2.1569 | val_loss 2.1537 | lr 7.480090e-03\n",
      "step 6800 | train_loss 1.9848 | val_loss 2.1591 | lr 7.460034e-03\n",
      "step 7000 | train_loss 2.0720 | val_loss 2.1380 | lr 7.439417e-03\n",
      "step 7200 | train_loss 2.1106 | val_loss 2.1489 | lr 7.418241e-03\n",
      "step 7400 | train_loss 2.1082 | val_loss 2.1438 | lr 7.396512e-03\n",
      "step 7600 | train_loss 2.1224 | val_loss 2.1401 | lr 7.374230e-03\n",
      "step 7800 | train_loss 2.1145 | val_loss 2.1424 | lr 7.351402e-03\n",
      "step 8000 | train_loss 2.1153 | val_loss 2.1483 | lr 7.328029e-03\n",
      "step 8200 | train_loss 2.1209 | val_loss 2.1309 | lr 7.304115e-03\n",
      "step 8400 | train_loss 2.1374 | val_loss 2.1285 | lr 7.279665e-03\n",
      "step 8600 | train_loss 2.1143 | val_loss 2.1294 | lr 7.254682e-03\n",
      "step 8800 | train_loss 2.1160 | val_loss 2.1253 | lr 7.229170e-03\n",
      "step 9000 | train_loss 2.1472 | val_loss 2.1305 | lr 7.203134e-03\n",
      "step 9200 | train_loss 2.1672 | val_loss 2.1387 | lr 7.176576e-03\n",
      "step 9400 | train_loss 2.1544 | val_loss 2.1405 | lr 7.149502e-03\n",
      "step 9600 | train_loss 2.1539 | val_loss 2.1497 | lr 7.121916e-03\n",
      "step 9800 | train_loss 2.1447 | val_loss 2.1351 | lr 7.093822e-03\n",
      "step 10000 | train_loss 2.1561 | val_loss 2.1343 | lr 7.065225e-03\n",
      "step 10200 | train_loss 2.0133 | val_loss 2.1272 | lr 7.036128e-03\n",
      "step 10400 | train_loss 2.0234 | val_loss 2.1111 | lr 7.006537e-03\n",
      "step 10600 | train_loss 2.0825 | val_loss 2.1085 | lr 6.976457e-03\n",
      "step 10800 | train_loss 2.0745 | val_loss 2.1110 | lr 6.945891e-03\n",
      "step 11000 | train_loss 2.0853 | val_loss 2.1195 | lr 6.914846e-03\n",
      "step 11200 | train_loss 2.0848 | val_loss 2.1281 | lr 6.883325e-03\n",
      "step 11400 | train_loss 2.0965 | val_loss 2.1115 | lr 6.851334e-03\n",
      "step 11600 | train_loss 2.1155 | val_loss 2.1318 | lr 6.818878e-03\n",
      "step 11800 | train_loss 2.0966 | val_loss 2.1259 | lr 6.785962e-03\n",
      "step 12000 | train_loss 2.1149 | val_loss 2.1210 | lr 6.752591e-03\n",
      "step 12200 | train_loss 2.0983 | val_loss 2.1273 | lr 6.718770e-03\n",
      "step 12400 | train_loss 2.1444 | val_loss 2.1241 | lr 6.684505e-03\n",
      "step 12600 | train_loss 2.1124 | val_loss 2.1152 | lr 6.649802e-03\n",
      "step 12800 | train_loss 2.1119 | val_loss 2.1048 | lr 6.614665e-03\n",
      "step 13000 | train_loss 2.1000 | val_loss 2.1128 | lr 6.579100e-03\n",
      "step 13200 | train_loss 2.0988 | val_loss 2.1051 | lr 6.543113e-03\n",
      "step 13400 | train_loss 1.8662 | val_loss 2.1130 | lr 6.506710e-03\n",
      "step 13600 | train_loss 1.9928 | val_loss 2.1115 | lr 6.469895e-03\n",
      "step 13800 | train_loss 2.0488 | val_loss 2.1107 | lr 6.432676e-03\n",
      "step 14000 | train_loss 2.0606 | val_loss 2.1086 | lr 6.395058e-03\n",
      "step 14200 | train_loss 2.0491 | val_loss 2.1159 | lr 6.357046e-03\n",
      "step 14400 | train_loss 2.0670 | val_loss 2.1048 | lr 6.318648e-03\n",
      "step 14600 | train_loss 2.0519 | val_loss 2.1065 | lr 6.279868e-03\n",
      "step 14800 | train_loss 2.0677 | val_loss 2.1033 | lr 6.240714e-03\n",
      "step 15000 | train_loss 2.0670 | val_loss 2.1032 | lr 6.201191e-03\n",
      "step 15200 | train_loss 2.0927 | val_loss 2.1176 | lr 6.161305e-03\n",
      "step 15400 | train_loss 2.0821 | val_loss 2.1165 | lr 6.121063e-03\n",
      "step 15600 | train_loss 2.0689 | val_loss 2.1093 | lr 6.080471e-03\n",
      "step 15800 | train_loss 2.0847 | val_loss 2.0975 | lr 6.039536e-03\n",
      "step 16000 | train_loss 2.1009 | val_loss 2.1075 | lr 5.998264e-03\n",
      "step 16200 | train_loss 2.0737 | val_loss 2.1008 | lr 5.956661e-03\n",
      "step 16400 | train_loss 2.0755 | val_loss 2.0963 | lr 5.914735e-03\n",
      "step 16600 | train_loss 2.1066 | val_loss 2.1029 | lr 5.872491e-03\n",
      "step 16800 | train_loss 1.9183 | val_loss 2.1236 | lr 5.829936e-03\n",
      "step 17000 | train_loss 1.9768 | val_loss 2.1021 | lr 5.787078e-03\n",
      "step 17200 | train_loss 2.0206 | val_loss 2.0994 | lr 5.743923e-03\n",
      "step 17400 | train_loss 2.0267 | val_loss 2.1074 | lr 5.700477e-03\n",
      "step 17600 | train_loss 2.0367 | val_loss 2.1034 | lr 5.656748e-03\n",
      "step 17800 | train_loss 2.0132 | val_loss 2.0876 | lr 5.612742e-03\n",
      "step 18000 | train_loss 2.0285 | val_loss 2.0865 | lr 5.568467e-03\n",
      "step 18200 | train_loss 2.0372 | val_loss 2.0956 | lr 5.523929e-03\n",
      "step 18400 | train_loss 2.0505 | val_loss 2.0928 | lr 5.479135e-03\n",
      "step 18600 | train_loss 2.0433 | val_loss 2.0966 | lr 5.434094e-03\n",
      "step 18800 | train_loss 2.0652 | val_loss 2.0881 | lr 5.388810e-03\n",
      "step 19000 | train_loss 2.0676 | val_loss 2.0831 | lr 5.343293e-03\n",
      "step 19200 | train_loss 2.0679 | val_loss 2.0867 | lr 5.297548e-03\n",
      "step 19400 | train_loss 2.0713 | val_loss 2.0934 | lr 5.251584e-03\n",
      "step 19600 | train_loss 2.0733 | val_loss 2.1005 | lr 5.205407e-03\n",
      "step 19800 | train_loss 2.0705 | val_loss 2.0835 | lr 5.159025e-03\n",
      "step 20000 | train_loss 2.0533 | val_loss 2.0807 | lr 5.112445e-03\n",
      "step 20200 | train_loss 1.9177 | val_loss 2.0903 | lr 5.065674e-03\n",
      "step 20400 | train_loss 1.9559 | val_loss 2.0873 | lr 5.018720e-03\n",
      "step 20600 | train_loss 1.9929 | val_loss 2.0766 | lr 4.971590e-03\n",
      "step 20800 | train_loss 1.9931 | val_loss 2.0771 | lr 4.924292e-03\n",
      "step 21000 | train_loss 2.0031 | val_loss 2.0768 | lr 4.876833e-03\n",
      "step 21200 | train_loss 1.9998 | val_loss 2.0725 | lr 4.829221e-03\n",
      "step 21400 | train_loss 2.0183 | val_loss 2.0858 | lr 4.781463e-03\n",
      "step 21600 | train_loss 2.0325 | val_loss 2.0703 | lr 4.733567e-03\n",
      "step 21800 | train_loss 2.0212 | val_loss 2.0764 | lr 4.685539e-03\n",
      "step 22000 | train_loss 2.0138 | val_loss 2.0789 | lr 4.637389e-03\n",
      "step 22200 | train_loss 2.0165 | val_loss 2.0800 | lr 4.589123e-03\n",
      "step 22400 | train_loss 2.0486 | val_loss 2.0827 | lr 4.540749e-03\n",
      "step 22600 | train_loss 2.0421 | val_loss 2.0760 | lr 4.492275e-03\n",
      "step 22800 | train_loss 2.0180 | val_loss 2.0819 | lr 4.443708e-03\n",
      "step 23000 | train_loss 2.0192 | val_loss 2.0731 | lr 4.395057e-03\n",
      "step 23200 | train_loss 2.0226 | val_loss 2.0648 | lr 4.346328e-03\n",
      "step 23400 | train_loss 1.8949 | val_loss 2.0686 | lr 4.297529e-03\n",
      "step 23600 | train_loss 1.9268 | val_loss 2.0754 | lr 4.248668e-03\n",
      "step 23800 | train_loss 1.9510 | val_loss 2.0672 | lr 4.199753e-03\n",
      "step 24000 | train_loss 1.9656 | val_loss 2.0657 | lr 4.150792e-03\n",
      "step 24200 | train_loss 1.9849 | val_loss 2.0704 | lr 4.101792e-03\n",
      "step 24400 | train_loss 1.9789 | val_loss 2.0665 | lr 4.052761e-03\n",
      "step 24600 | train_loss 1.9768 | val_loss 2.0713 | lr 4.003707e-03\n",
      "step 24800 | train_loss 1.9821 | val_loss 2.0609 | lr 3.954638e-03\n",
      "step 25000 | train_loss 2.0018 | val_loss 2.0761 | lr 3.905560e-03\n",
      "step 25200 | train_loss 2.0100 | val_loss 2.0643 | lr 3.856483e-03\n",
      "step 25400 | train_loss 2.0222 | val_loss 2.0697 | lr 3.807413e-03\n",
      "step 25600 | train_loss 2.0174 | val_loss 2.0732 | lr 3.758359e-03\n",
      "step 25800 | train_loss 2.0187 | val_loss 2.0731 | lr 3.709328e-03\n",
      "step 26000 | train_loss 2.0194 | val_loss 2.0695 | lr 3.660328e-03\n",
      "step 26200 | train_loss 2.0160 | val_loss 2.0713 | lr 3.611367e-03\n",
      "step 26400 | train_loss 1.9938 | val_loss 2.0674 | lr 3.562452e-03\n",
      "step 26600 | train_loss 2.0015 | val_loss 2.0712 | lr 3.513591e-03\n",
      "step 26800 | train_loss 1.8472 | val_loss 2.0648 | lr 3.464793e-03\n",
      "step 27000 | train_loss 1.9128 | val_loss 2.0581 | lr 3.416064e-03\n",
      "step 27200 | train_loss 1.9329 | val_loss 2.0654 | lr 3.367412e-03\n",
      "step 27400 | train_loss 1.9481 | val_loss 2.0712 | lr 3.318845e-03\n",
      "step 27600 | train_loss 1.9615 | val_loss 2.0667 | lr 3.270371e-03\n",
      "step 27800 | train_loss 1.9511 | val_loss 2.0640 | lr 3.221997e-03\n",
      "step 28000 | train_loss 1.9590 | val_loss 2.0564 | lr 3.173731e-03\n",
      "step 28200 | train_loss 1.9536 | val_loss 2.0591 | lr 3.125581e-03\n",
      "step 28400 | train_loss 1.9801 | val_loss 2.0617 | lr 3.077554e-03\n",
      "step 28600 | train_loss 1.9856 | val_loss 2.0539 | lr 3.029657e-03\n",
      "step 28800 | train_loss 1.9839 | val_loss 2.0562 | lr 2.981899e-03\n",
      "step 29000 | train_loss 1.9929 | val_loss 2.0637 | lr 2.934287e-03\n",
      "step 29200 | train_loss 1.9940 | val_loss 2.0632 | lr 2.886828e-03\n",
      "step 29400 | train_loss 1.9957 | val_loss 2.0673 | lr 2.839530e-03\n",
      "step 29600 | train_loss 1.9847 | val_loss 2.0650 | lr 2.792400e-03\n",
      "step 29800 | train_loss 1.9823 | val_loss 2.0632 | lr 2.745446e-03\n",
      "step 30000 | train_loss 1.9646 | val_loss 2.0603 | lr 2.698676e-03\n",
      "step 30200 | train_loss 1.8583 | val_loss 2.0544 | lr 2.652096e-03\n",
      "step 30400 | train_loss 1.9031 | val_loss 2.0541 | lr 2.605713e-03\n",
      "step 30600 | train_loss 1.9221 | val_loss 2.0558 | lr 2.559536e-03\n",
      "step 30800 | train_loss 1.9225 | val_loss 2.0542 | lr 2.513572e-03\n",
      "step 31000 | train_loss 1.9399 | val_loss 2.0587 | lr 2.467828e-03\n",
      "step 31200 | train_loss 1.9360 | val_loss 2.0569 | lr 2.422310e-03\n",
      "step 31400 | train_loss 1.9363 | val_loss 2.0501 | lr 2.377027e-03\n",
      "step 31600 | train_loss 1.9479 | val_loss 2.0483 | lr 2.331985e-03\n",
      "step 31800 | train_loss 1.9472 | val_loss 2.0488 | lr 2.287191e-03\n",
      "step 32000 | train_loss 1.9616 | val_loss 2.0528 | lr 2.242653e-03\n",
      "step 32200 | train_loss 1.9610 | val_loss 2.0424 | lr 2.198378e-03\n",
      "step 32400 | train_loss 1.9571 | val_loss 2.0473 | lr 2.154373e-03\n",
      "step 32600 | train_loss 1.9648 | val_loss 2.0533 | lr 2.110643e-03\n",
      "step 32800 | train_loss 1.9680 | val_loss 2.0522 | lr 2.067198e-03\n",
      "step 33000 | train_loss 1.9599 | val_loss 2.0555 | lr 2.024042e-03\n",
      "step 33200 | train_loss 1.9504 | val_loss 2.0530 | lr 1.981184e-03\n",
      "step 33400 | train_loss 1.9405 | val_loss 2.0443 | lr 1.938629e-03\n",
      "step 33600 | train_loss 1.8708 | val_loss 2.0438 | lr 1.896386e-03\n",
      "step 33800 | train_loss 1.8855 | val_loss 2.0466 | lr 1.854459e-03\n",
      "step 34000 | train_loss 1.9066 | val_loss 2.0480 | lr 1.812856e-03\n",
      "step 34200 | train_loss 1.9100 | val_loss 2.0481 | lr 1.771584e-03\n",
      "step 34400 | train_loss 1.9116 | val_loss 2.0481 | lr 1.730649e-03\n",
      "step 34600 | train_loss 1.9222 | val_loss 2.0460 | lr 1.690057e-03\n",
      "step 34800 | train_loss 1.9070 | val_loss 2.0404 | lr 1.649815e-03\n",
      "step 35000 | train_loss 1.9297 | val_loss 2.0485 | lr 1.609929e-03\n",
      "step 35200 | train_loss 1.9212 | val_loss 2.0459 | lr 1.570406e-03\n",
      "step 35400 | train_loss 1.9331 | val_loss 2.0392 | lr 1.531252e-03\n",
      "step 35600 | train_loss 1.9388 | val_loss 2.0455 | lr 1.492472e-03\n",
      "step 35800 | train_loss 1.9456 | val_loss 2.0409 | lr 1.454074e-03\n",
      "step 36000 | train_loss 1.9388 | val_loss 2.0440 | lr 1.416062e-03\n",
      "step 36200 | train_loss 1.9462 | val_loss 2.0404 | lr 1.378444e-03\n",
      "step 36400 | train_loss 1.9282 | val_loss 2.0432 | lr 1.341225e-03\n",
      "step 36600 | train_loss 1.9259 | val_loss 2.0400 | lr 1.304411e-03\n",
      "step 36800 | train_loss 1.8394 | val_loss 2.0390 | lr 1.268007e-03\n",
      "step 37000 | train_loss 1.8655 | val_loss 2.0370 | lr 1.232020e-03\n",
      "step 37200 | train_loss 1.8838 | val_loss 2.0382 | lr 1.196455e-03\n",
      "step 37400 | train_loss 1.8983 | val_loss 2.0418 | lr 1.161318e-03\n",
      "step 37600 | train_loss 1.9033 | val_loss 2.0425 | lr 1.126615e-03\n",
      "step 37800 | train_loss 1.9003 | val_loss 2.0405 | lr 1.092350e-03\n",
      "step 38000 | train_loss 1.8955 | val_loss 2.0399 | lr 1.058529e-03\n",
      "step 38200 | train_loss 1.8927 | val_loss 2.0353 | lr 1.025158e-03\n",
      "step 38400 | train_loss 1.9081 | val_loss 2.0388 | lr 9.922421e-04\n",
      "step 38600 | train_loss 1.9176 | val_loss 2.0359 | lr 9.597860e-04\n",
      "step 38800 | train_loss 1.9160 | val_loss 2.0304 | lr 9.277951e-04\n",
      "step 39000 | train_loss 1.9181 | val_loss 2.0362 | lr 8.962743e-04\n",
      "step 39200 | train_loss 1.9150 | val_loss 2.0317 | lr 8.652288e-04\n",
      "step 39400 | train_loss 1.9204 | val_loss 2.0352 | lr 8.346634e-04\n",
      "step 39600 | train_loss 1.9196 | val_loss 2.0353 | lr 8.045829e-04\n",
      "step 39800 | train_loss 1.9135 | val_loss 2.0352 | lr 7.749921e-04\n",
      "step 40000 | train_loss 1.9142 | val_loss 2.0356 | lr 7.458956e-04\n",
      "step 40200 | train_loss 1.8550 | val_loss 2.0340 | lr 7.172981e-04\n",
      "step 40400 | train_loss 1.8664 | val_loss 2.0320 | lr 6.892040e-04\n",
      "step 40600 | train_loss 1.8741 | val_loss 2.0335 | lr 6.616179e-04\n",
      "step 40800 | train_loss 1.8812 | val_loss 2.0349 | lr 6.345440e-04\n",
      "step 41000 | train_loss 1.8872 | val_loss 2.0346 | lr 6.079866e-04\n",
      "step 41200 | train_loss 1.8836 | val_loss 2.0342 | lr 5.819500e-04\n",
      "step 41400 | train_loss 1.8884 | val_loss 2.0311 | lr 5.564382e-04\n",
      "step 41600 | train_loss 1.8862 | val_loss 2.0308 | lr 5.314553e-04\n",
      "step 41800 | train_loss 1.8928 | val_loss 2.0317 | lr 5.070052e-04\n",
      "step 42000 | train_loss 1.8931 | val_loss 2.0295 | lr 4.830917e-04\n",
      "step 42200 | train_loss 1.8968 | val_loss 2.0268 | lr 4.597187e-04\n",
      "step 42400 | train_loss 1.8959 | val_loss 2.0283 | lr 4.368898e-04\n",
      "step 42600 | train_loss 1.8973 | val_loss 2.0275 | lr 4.146087e-04\n",
      "step 42800 | train_loss 1.9000 | val_loss 2.0298 | lr 3.928788e-04\n",
      "step 43000 | train_loss 1.9007 | val_loss 2.0304 | lr 3.717037e-04\n",
      "step 43200 | train_loss 1.9007 | val_loss 2.0296 | lr 3.510865e-04\n",
      "step 43400 | train_loss 1.8957 | val_loss 2.0292 | lr 3.310307e-04\n",
      "step 43600 | train_loss 1.8700 | val_loss 2.0288 | lr 3.115393e-04\n",
      "step 43800 | train_loss 1.8738 | val_loss 2.0304 | lr 2.926155e-04\n",
      "step 44000 | train_loss 1.8752 | val_loss 2.0273 | lr 2.742622e-04\n",
      "step 44200 | train_loss 1.8764 | val_loss 2.0286 | lr 2.564823e-04\n",
      "step 44400 | train_loss 1.8797 | val_loss 2.0284 | lr 2.392786e-04\n",
      "step 44600 | train_loss 1.8788 | val_loss 2.0285 | lr 2.226539e-04\n",
      "step 44800 | train_loss 1.8799 | val_loss 2.0276 | lr 2.066108e-04\n",
      "step 45000 | train_loss 1.8810 | val_loss 2.0271 | lr 1.911517e-04\n",
      "step 45200 | train_loss 1.8821 | val_loss 2.0267 | lr 1.762792e-04\n",
      "step 45400 | train_loss 1.8819 | val_loss 2.0258 | lr 1.619956e-04\n",
      "step 45600 | train_loss 1.8842 | val_loss 2.0251 | lr 1.483032e-04\n",
      "step 45800 | train_loss 1.8830 | val_loss 2.0254 | lr 1.352041e-04\n",
      "step 46000 | train_loss 1.8835 | val_loss 2.0254 | lr 1.227004e-04\n",
      "step 46200 | train_loss 1.8852 | val_loss 2.0261 | lr 1.107940e-04\n",
      "step 46400 | train_loss 1.8863 | val_loss 2.0261 | lr 9.948683e-05\n",
      "step 46600 | train_loss 1.8866 | val_loss 2.0258 | lr 8.878071e-05\n",
      "step 46800 | train_loss 1.8802 | val_loss 2.0257 | lr 7.867730e-05\n",
      "step 47000 | train_loss 1.8786 | val_loss 2.0259 | lr 6.917821e-05\n",
      "step 47200 | train_loss 1.8786 | val_loss 2.0261 | lr 6.028492e-05\n",
      "step 47400 | train_loss 1.8793 | val_loss 2.0259 | lr 5.199885e-05\n",
      "step 47600 | train_loss 1.8790 | val_loss 2.0262 | lr 4.432129e-05\n",
      "step 47800 | train_loss 1.8793 | val_loss 2.0261 | lr 3.725348e-05\n",
      "step 48000 | train_loss 1.8793 | val_loss 2.0264 | lr 3.079651e-05\n",
      "step 48200 | train_loss 1.8790 | val_loss 2.0262 | lr 2.495141e-05\n",
      "step 48400 | train_loss 1.8790 | val_loss 2.0261 | lr 1.971911e-05\n",
      "step 48600 | train_loss 1.8791 | val_loss 2.0259 | lr 1.510042e-05\n",
      "step 48800 | train_loss 1.8792 | val_loss 2.0258 | lr 1.109609e-05\n",
      "step 49000 | train_loss 1.8794 | val_loss 2.0258 | lr 7.706731e-06\n",
      "step 49200 | train_loss 1.8794 | val_loss 2.0257 | lr 4.932892e-06\n",
      "step 49400 | train_loss 1.8794 | val_loss 2.0257 | lr 2.775007e-06\n",
      "step 49600 | train_loss 1.8794 | val_loss 2.0257 | lr 1.233418e-06\n",
      "step 49800 | train_loss 1.8794 | val_loss 2.0257 | lr 3.083666e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:13:51,205] Trial 11 finished with value: 2.0257206293089047 and parameters: {'embedding_size': 16, 'hidden_size': 263, 'learning_rate': 0.007811120232798292, 'batch_size': 64, 'context_length': 5}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8794 | val_loss 2.0257 | lr 7.709267e-12\n",
      "step 0 | train_loss 3.3017 | val_loss 3.3048 | lr 2.966448e-03\n",
      "step 200 | train_loss 2.3543 | val_loss 2.3886 | lr 2.966331e-03\n",
      "step 400 | train_loss 2.2859 | val_loss 2.3050 | lr 2.965979e-03\n",
      "step 600 | train_loss 2.2594 | val_loss 2.2654 | lr 2.965394e-03\n",
      "step 800 | train_loss 2.2540 | val_loss 2.2435 | lr 2.964574e-03\n",
      "step 1000 | train_loss 2.2489 | val_loss 2.2297 | lr 2.963521e-03\n",
      "step 1200 | train_loss 2.2134 | val_loss 2.2170 | lr 2.962234e-03\n",
      "step 1400 | train_loss 2.2176 | val_loss 2.2052 | lr 2.960713e-03\n",
      "step 1600 | train_loss 2.1991 | val_loss 2.1893 | lr 2.958959e-03\n",
      "step 1800 | train_loss 2.2084 | val_loss 2.1808 | lr 2.956972e-03\n",
      "step 2000 | train_loss 2.1943 | val_loss 2.1746 | lr 2.954752e-03\n",
      "step 2200 | train_loss 2.1863 | val_loss 2.1703 | lr 2.952300e-03\n",
      "step 2400 | train_loss 2.1838 | val_loss 2.1550 | lr 2.949616e-03\n",
      "step 2600 | train_loss 2.1642 | val_loss 2.1530 | lr 2.946700e-03\n",
      "step 2800 | train_loss 2.1543 | val_loss 2.1482 | lr 2.943553e-03\n",
      "step 3000 | train_loss 2.1567 | val_loss 2.1318 | lr 2.940176e-03\n",
      "step 3200 | train_loss 2.1556 | val_loss 2.1332 | lr 2.936568e-03\n",
      "step 3400 | train_loss 1.9396 | val_loss 2.1316 | lr 2.932731e-03\n",
      "step 3600 | train_loss 2.0530 | val_loss 2.1300 | lr 2.928665e-03\n",
      "step 3800 | train_loss 2.0707 | val_loss 2.1199 | lr 2.924371e-03\n",
      "step 4000 | train_loss 2.0741 | val_loss 2.1167 | lr 2.919850e-03\n",
      "step 4200 | train_loss 2.1027 | val_loss 2.1237 | lr 2.915101e-03\n",
      "step 4400 | train_loss 2.0852 | val_loss 2.1107 | lr 2.910126e-03\n",
      "step 4600 | train_loss 2.0834 | val_loss 2.1183 | lr 2.904926e-03\n",
      "step 4800 | train_loss 2.0908 | val_loss 2.1142 | lr 2.899502e-03\n",
      "step 5000 | train_loss 2.0975 | val_loss 2.1150 | lr 2.893854e-03\n",
      "step 5200 | train_loss 2.1126 | val_loss 2.1075 | lr 2.887983e-03\n",
      "step 5400 | train_loss 2.1034 | val_loss 2.1021 | lr 2.881890e-03\n",
      "step 5600 | train_loss 2.0911 | val_loss 2.1054 | lr 2.875576e-03\n",
      "step 5800 | train_loss 2.1015 | val_loss 2.1067 | lr 2.869043e-03\n",
      "step 6000 | train_loss 2.1155 | val_loss 2.1105 | lr 2.862291e-03\n",
      "step 6200 | train_loss 2.0881 | val_loss 2.1028 | lr 2.855321e-03\n",
      "step 6400 | train_loss 2.1210 | val_loss 2.1009 | lr 2.848134e-03\n",
      "step 6600 | train_loss 2.0851 | val_loss 2.0938 | lr 2.840731e-03\n",
      "step 6800 | train_loss 1.9002 | val_loss 2.0925 | lr 2.833115e-03\n",
      "step 7000 | train_loss 1.9792 | val_loss 2.0790 | lr 2.825285e-03\n",
      "step 7200 | train_loss 2.0235 | val_loss 2.0929 | lr 2.817243e-03\n",
      "step 7400 | train_loss 2.0254 | val_loss 2.0823 | lr 2.808991e-03\n",
      "step 7600 | train_loss 2.0474 | val_loss 2.0850 | lr 2.800529e-03\n",
      "step 7800 | train_loss 2.0373 | val_loss 2.0865 | lr 2.791859e-03\n",
      "step 8000 | train_loss 2.0443 | val_loss 2.0866 | lr 2.782983e-03\n",
      "step 8200 | train_loss 2.0530 | val_loss 2.0834 | lr 2.773901e-03\n",
      "step 8400 | train_loss 2.0664 | val_loss 2.0823 | lr 2.764616e-03\n",
      "step 8600 | train_loss 2.0510 | val_loss 2.0852 | lr 2.755128e-03\n",
      "step 8800 | train_loss 2.0547 | val_loss 2.0786 | lr 2.745439e-03\n",
      "step 9000 | train_loss 2.0693 | val_loss 2.0807 | lr 2.735551e-03\n",
      "step 9200 | train_loss 2.0781 | val_loss 2.0851 | lr 2.725465e-03\n",
      "step 9400 | train_loss 2.0618 | val_loss 2.0887 | lr 2.715184e-03\n",
      "step 9600 | train_loss 2.0634 | val_loss 2.0847 | lr 2.704707e-03\n",
      "step 9800 | train_loss 2.0795 | val_loss 2.0758 | lr 2.694038e-03\n",
      "step 10000 | train_loss 2.0424 | val_loss 2.0690 | lr 2.683177e-03\n",
      "step 10200 | train_loss 1.9054 | val_loss 2.0789 | lr 2.672127e-03\n",
      "step 10400 | train_loss 1.9567 | val_loss 2.0815 | lr 2.660889e-03\n",
      "step 10600 | train_loss 1.9598 | val_loss 2.0739 | lr 2.649466e-03\n",
      "step 10800 | train_loss 2.0073 | val_loss 2.0748 | lr 2.637858e-03\n",
      "step 11000 | train_loss 2.0003 | val_loss 2.0692 | lr 2.626068e-03\n",
      "step 11200 | train_loss 1.9972 | val_loss 2.0778 | lr 2.614097e-03\n",
      "step 11400 | train_loss 2.0136 | val_loss 2.0726 | lr 2.601948e-03\n",
      "step 11600 | train_loss 2.0236 | val_loss 2.0768 | lr 2.589622e-03\n",
      "step 11800 | train_loss 2.0126 | val_loss 2.0678 | lr 2.577121e-03\n",
      "step 12000 | train_loss 2.0303 | val_loss 2.0740 | lr 2.564448e-03\n",
      "step 12200 | train_loss 2.0313 | val_loss 2.0763 | lr 2.551603e-03\n",
      "step 12400 | train_loss 2.0520 | val_loss 2.0672 | lr 2.538591e-03\n",
      "step 12600 | train_loss 2.0541 | val_loss 2.0794 | lr 2.525411e-03\n",
      "step 12800 | train_loss 2.0439 | val_loss 2.0750 | lr 2.512067e-03\n",
      "step 13000 | train_loss 2.0426 | val_loss 2.0654 | lr 2.498561e-03\n",
      "step 13200 | train_loss 2.0289 | val_loss 2.0679 | lr 2.484894e-03\n",
      "step 13400 | train_loss 1.7875 | val_loss 2.0633 | lr 2.471069e-03\n",
      "step 13600 | train_loss 1.8915 | val_loss 2.0618 | lr 2.457088e-03\n",
      "step 13800 | train_loss 1.9378 | val_loss 2.0653 | lr 2.442953e-03\n",
      "step 14000 | train_loss 1.9554 | val_loss 2.0612 | lr 2.428666e-03\n",
      "step 14200 | train_loss 1.9848 | val_loss 2.0710 | lr 2.414231e-03\n",
      "step 14400 | train_loss 1.9842 | val_loss 2.0582 | lr 2.399648e-03\n",
      "step 14600 | train_loss 1.9793 | val_loss 2.0633 | lr 2.384921e-03\n",
      "step 14800 | train_loss 1.9916 | val_loss 2.0594 | lr 2.370051e-03\n",
      "step 15000 | train_loss 2.0029 | val_loss 2.0684 | lr 2.355041e-03\n",
      "step 15200 | train_loss 2.0193 | val_loss 2.0633 | lr 2.339894e-03\n",
      "step 15400 | train_loss 2.0154 | val_loss 2.0634 | lr 2.324611e-03\n",
      "step 15600 | train_loss 2.0115 | val_loss 2.0735 | lr 2.309195e-03\n",
      "step 15800 | train_loss 2.0160 | val_loss 2.0662 | lr 2.293649e-03\n",
      "step 16000 | train_loss 2.0396 | val_loss 2.0751 | lr 2.277975e-03\n",
      "step 16200 | train_loss 2.0168 | val_loss 2.0741 | lr 2.262175e-03\n",
      "step 16400 | train_loss 2.0209 | val_loss 2.0594 | lr 2.246253e-03\n",
      "step 16600 | train_loss 2.0118 | val_loss 2.0632 | lr 2.230210e-03\n",
      "step 16800 | train_loss 1.8124 | val_loss 2.0643 | lr 2.214049e-03\n",
      "step 17000 | train_loss 1.8942 | val_loss 2.0615 | lr 2.197772e-03\n",
      "step 17200 | train_loss 1.9149 | val_loss 2.0595 | lr 2.181383e-03\n",
      "step 17400 | train_loss 1.9311 | val_loss 2.0609 | lr 2.164884e-03\n",
      "step 17600 | train_loss 1.9709 | val_loss 2.0605 | lr 2.148277e-03\n",
      "step 17800 | train_loss 1.9550 | val_loss 2.0599 | lr 2.131564e-03\n",
      "step 18000 | train_loss 1.9686 | val_loss 2.0563 | lr 2.114750e-03\n",
      "step 18200 | train_loss 1.9693 | val_loss 2.0605 | lr 2.097836e-03\n",
      "step 18400 | train_loss 1.9905 | val_loss 2.0589 | lr 2.080824e-03\n",
      "step 18600 | train_loss 1.9850 | val_loss 2.0629 | lr 2.063719e-03\n",
      "step 18800 | train_loss 1.9971 | val_loss 2.0636 | lr 2.046521e-03\n",
      "step 19000 | train_loss 1.9976 | val_loss 2.0565 | lr 2.029235e-03\n",
      "step 19200 | train_loss 2.0062 | val_loss 2.0635 | lr 2.011863e-03\n",
      "step 19400 | train_loss 2.0152 | val_loss 2.0658 | lr 1.994407e-03\n",
      "step 19600 | train_loss 1.9958 | val_loss 2.0656 | lr 1.976870e-03\n",
      "step 19800 | train_loss 2.0118 | val_loss 2.0617 | lr 1.959255e-03\n",
      "step 20000 | train_loss 1.9756 | val_loss 2.0524 | lr 1.941565e-03\n",
      "step 20200 | train_loss 1.8363 | val_loss 2.0613 | lr 1.923803e-03\n",
      "step 20400 | train_loss 1.8914 | val_loss 2.0540 | lr 1.905971e-03\n",
      "step 20600 | train_loss 1.8970 | val_loss 2.0522 | lr 1.888073e-03\n",
      "step 20800 | train_loss 1.9302 | val_loss 2.0524 | lr 1.870110e-03\n",
      "step 21000 | train_loss 1.9308 | val_loss 2.0519 | lr 1.852087e-03\n",
      "step 21200 | train_loss 1.9284 | val_loss 2.0499 | lr 1.834005e-03\n",
      "step 21400 | train_loss 1.9479 | val_loss 2.0527 | lr 1.815868e-03\n",
      "step 21600 | train_loss 1.9538 | val_loss 2.0538 | lr 1.797678e-03\n",
      "step 21800 | train_loss 1.9618 | val_loss 2.0529 | lr 1.779439e-03\n",
      "step 22000 | train_loss 1.9555 | val_loss 2.0556 | lr 1.761152e-03\n",
      "step 22200 | train_loss 1.9792 | val_loss 2.0599 | lr 1.742822e-03\n",
      "step 22400 | train_loss 1.9813 | val_loss 2.0592 | lr 1.724451e-03\n",
      "step 22600 | train_loss 1.9958 | val_loss 2.0657 | lr 1.706042e-03\n",
      "step 22800 | train_loss 1.9792 | val_loss 2.0626 | lr 1.687598e-03\n",
      "step 23000 | train_loss 1.9845 | val_loss 2.0591 | lr 1.669121e-03\n",
      "step 23200 | train_loss 1.9777 | val_loss 2.0587 | lr 1.650615e-03\n",
      "step 23400 | train_loss 1.8592 | val_loss 2.0509 | lr 1.632083e-03\n",
      "step 23600 | train_loss 1.8415 | val_loss 2.0573 | lr 1.613527e-03\n",
      "step 23800 | train_loss 1.8807 | val_loss 2.0546 | lr 1.594950e-03\n",
      "step 24000 | train_loss 1.8883 | val_loss 2.0518 | lr 1.576356e-03\n",
      "step 24200 | train_loss 1.9167 | val_loss 2.0512 | lr 1.557747e-03\n",
      "step 24400 | train_loss 1.9181 | val_loss 2.0459 | lr 1.539127e-03\n",
      "step 24600 | train_loss 1.9150 | val_loss 2.0503 | lr 1.520497e-03\n",
      "step 24800 | train_loss 1.9215 | val_loss 2.0464 | lr 1.501862e-03\n",
      "step 25000 | train_loss 1.9242 | val_loss 2.0513 | lr 1.483224e-03\n",
      "step 25200 | train_loss 1.9375 | val_loss 2.0471 | lr 1.464586e-03\n",
      "step 25400 | train_loss 1.9409 | val_loss 2.0473 | lr 1.445950e-03\n",
      "step 25600 | train_loss 1.9536 | val_loss 2.0583 | lr 1.427321e-03\n",
      "step 25800 | train_loss 1.9602 | val_loss 2.0541 | lr 1.408700e-03\n",
      "step 26000 | train_loss 1.9691 | val_loss 2.0562 | lr 1.390091e-03\n",
      "step 26200 | train_loss 1.9625 | val_loss 2.0553 | lr 1.371497e-03\n",
      "step 26400 | train_loss 1.9634 | val_loss 2.0495 | lr 1.352921e-03\n",
      "step 26600 | train_loss 1.9612 | val_loss 2.0533 | lr 1.334365e-03\n",
      "step 26800 | train_loss 1.7981 | val_loss 2.0526 | lr 1.315833e-03\n",
      "step 27000 | train_loss 1.8493 | val_loss 2.0506 | lr 1.297327e-03\n",
      "step 27200 | train_loss 1.8657 | val_loss 2.0469 | lr 1.278850e-03\n",
      "step 27400 | train_loss 1.8760 | val_loss 2.0500 | lr 1.260406e-03\n",
      "step 27600 | train_loss 1.9015 | val_loss 2.0510 | lr 1.241997e-03\n",
      "step 27800 | train_loss 1.8884 | val_loss 2.0419 | lr 1.223626e-03\n",
      "step 28000 | train_loss 1.8944 | val_loss 2.0418 | lr 1.205295e-03\n",
      "step 28200 | train_loss 1.8959 | val_loss 2.0445 | lr 1.187009e-03\n",
      "step 28400 | train_loss 1.9125 | val_loss 2.0476 | lr 1.168770e-03\n",
      "step 28600 | train_loss 1.9210 | val_loss 2.0431 | lr 1.150580e-03\n",
      "step 28800 | train_loss 1.9249 | val_loss 2.0443 | lr 1.132443e-03\n",
      "step 29000 | train_loss 1.9281 | val_loss 2.0473 | lr 1.114361e-03\n",
      "step 29200 | train_loss 1.9345 | val_loss 2.0483 | lr 1.096338e-03\n",
      "step 29400 | train_loss 1.9457 | val_loss 2.0510 | lr 1.078375e-03\n",
      "step 29600 | train_loss 1.9354 | val_loss 2.0514 | lr 1.060477e-03\n",
      "step 29800 | train_loss 1.9461 | val_loss 2.0511 | lr 1.042645e-03\n",
      "step 30000 | train_loss 1.9259 | val_loss 2.0454 | lr 1.024883e-03\n",
      "step 30200 | train_loss 1.8162 | val_loss 2.0493 | lr 1.007193e-03\n",
      "step 30400 | train_loss 1.8511 | val_loss 2.0446 | lr 9.895780e-04\n",
      "step 30600 | train_loss 1.8595 | val_loss 2.0439 | lr 9.720413e-04\n",
      "step 30800 | train_loss 1.8713 | val_loss 2.0427 | lr 9.545853e-04\n",
      "step 31000 | train_loss 1.8794 | val_loss 2.0429 | lr 9.372128e-04\n",
      "step 31200 | train_loss 1.8775 | val_loss 2.0453 | lr 9.199265e-04\n",
      "step 31400 | train_loss 1.8845 | val_loss 2.0407 | lr 9.027291e-04\n",
      "step 31600 | train_loss 1.8838 | val_loss 2.0421 | lr 8.856234e-04\n",
      "step 31800 | train_loss 1.8939 | val_loss 2.0403 | lr 8.686121e-04\n",
      "step 32000 | train_loss 1.8964 | val_loss 2.0414 | lr 8.516979e-04\n",
      "step 32200 | train_loss 1.9084 | val_loss 2.0407 | lr 8.348833e-04\n",
      "step 32400 | train_loss 1.9097 | val_loss 2.0430 | lr 8.181712e-04\n",
      "step 32600 | train_loss 1.9158 | val_loss 2.0455 | lr 8.015641e-04\n",
      "step 32800 | train_loss 1.9188 | val_loss 2.0457 | lr 7.850646e-04\n",
      "step 33000 | train_loss 1.9204 | val_loss 2.0474 | lr 7.686753e-04\n",
      "step 33200 | train_loss 1.9237 | val_loss 2.0452 | lr 7.523989e-04\n",
      "step 33400 | train_loss 1.9096 | val_loss 2.0414 | lr 7.362379e-04\n",
      "step 33600 | train_loss 1.8354 | val_loss 2.0442 | lr 7.201949e-04\n",
      "step 33800 | train_loss 1.8517 | val_loss 2.0434 | lr 7.042723e-04\n",
      "step 34000 | train_loss 1.8528 | val_loss 2.0418 | lr 6.884728e-04\n",
      "step 34200 | train_loss 1.8674 | val_loss 2.0410 | lr 6.727987e-04\n",
      "step 34400 | train_loss 1.8699 | val_loss 2.0394 | lr 6.572527e-04\n",
      "step 34600 | train_loss 1.8679 | val_loss 2.0420 | lr 6.418370e-04\n",
      "step 34800 | train_loss 1.8694 | val_loss 2.0376 | lr 6.265543e-04\n",
      "step 35000 | train_loss 1.8740 | val_loss 2.0381 | lr 6.114068e-04\n",
      "step 35200 | train_loss 1.8739 | val_loss 2.0380 | lr 5.963969e-04\n",
      "step 35400 | train_loss 1.8809 | val_loss 2.0369 | lr 5.815271e-04\n",
      "step 35600 | train_loss 1.8896 | val_loss 2.0389 | lr 5.667998e-04\n",
      "step 35800 | train_loss 1.8957 | val_loss 2.0400 | lr 5.522171e-04\n",
      "step 36000 | train_loss 1.8964 | val_loss 2.0416 | lr 5.377814e-04\n",
      "step 36200 | train_loss 1.8980 | val_loss 2.0401 | lr 5.234950e-04\n",
      "step 36400 | train_loss 1.8982 | val_loss 2.0385 | lr 5.093602e-04\n",
      "step 36600 | train_loss 1.9009 | val_loss 2.0384 | lr 4.953792e-04\n",
      "step 36800 | train_loss 1.8281 | val_loss 2.0390 | lr 4.815541e-04\n",
      "step 37000 | train_loss 1.8426 | val_loss 2.0405 | lr 4.678873e-04\n",
      "step 37200 | train_loss 1.8504 | val_loss 2.0383 | lr 4.543807e-04\n",
      "step 37400 | train_loss 1.8520 | val_loss 2.0387 | lr 4.410367e-04\n",
      "step 37600 | train_loss 1.8605 | val_loss 2.0402 | lr 4.278572e-04\n",
      "step 37800 | train_loss 1.8627 | val_loss 2.0374 | lr 4.148443e-04\n",
      "step 38000 | train_loss 1.8587 | val_loss 2.0378 | lr 4.020002e-04\n",
      "step 38200 | train_loss 1.8585 | val_loss 2.0350 | lr 3.893268e-04\n",
      "step 38400 | train_loss 1.8625 | val_loss 2.0357 | lr 3.768262e-04\n",
      "step 38600 | train_loss 1.8687 | val_loss 2.0340 | lr 3.645002e-04\n",
      "step 38800 | train_loss 1.8708 | val_loss 2.0335 | lr 3.523509e-04\n",
      "step 39000 | train_loss 1.8742 | val_loss 2.0372 | lr 3.403802e-04\n",
      "step 39200 | train_loss 1.8760 | val_loss 2.0356 | lr 3.285900e-04\n",
      "step 39400 | train_loss 1.8811 | val_loss 2.0367 | lr 3.169821e-04\n",
      "step 39600 | train_loss 1.8798 | val_loss 2.0364 | lr 3.055584e-04\n",
      "step 39800 | train_loss 1.8837 | val_loss 2.0362 | lr 2.943206e-04\n",
      "step 40000 | train_loss 1.8872 | val_loss 2.0368 | lr 2.832706e-04\n",
      "step 40200 | train_loss 1.8405 | val_loss 2.0368 | lr 2.724100e-04\n",
      "step 40400 | train_loss 1.8467 | val_loss 2.0358 | lr 2.617407e-04\n",
      "step 40600 | train_loss 1.8505 | val_loss 2.0360 | lr 2.512642e-04\n",
      "step 40800 | train_loss 1.8512 | val_loss 2.0363 | lr 2.409823e-04\n",
      "step 41000 | train_loss 1.8551 | val_loss 2.0361 | lr 2.308965e-04\n",
      "step 41200 | train_loss 1.8552 | val_loss 2.0362 | lr 2.210085e-04\n",
      "step 41400 | train_loss 1.8556 | val_loss 2.0352 | lr 2.113199e-04\n",
      "step 41600 | train_loss 1.8535 | val_loss 2.0344 | lr 2.018320e-04\n",
      "step 41800 | train_loss 1.8584 | val_loss 2.0335 | lr 1.925466e-04\n",
      "step 42000 | train_loss 1.8587 | val_loss 2.0333 | lr 1.834649e-04\n",
      "step 42200 | train_loss 1.8618 | val_loss 2.0327 | lr 1.745885e-04\n",
      "step 42400 | train_loss 1.8638 | val_loss 2.0336 | lr 1.659187e-04\n",
      "step 42600 | train_loss 1.8645 | val_loss 2.0335 | lr 1.574569e-04\n",
      "step 42800 | train_loss 1.8665 | val_loss 2.0340 | lr 1.492045e-04\n",
      "step 43000 | train_loss 1.8672 | val_loss 2.0345 | lr 1.411628e-04\n",
      "step 43200 | train_loss 1.8700 | val_loss 2.0343 | lr 1.333330e-04\n",
      "step 43400 | train_loss 1.8702 | val_loss 2.0341 | lr 1.257163e-04\n",
      "step 43600 | train_loss 1.8500 | val_loss 2.0344 | lr 1.183140e-04\n",
      "step 43800 | train_loss 1.8523 | val_loss 2.0343 | lr 1.111273e-04\n",
      "step 44000 | train_loss 1.8524 | val_loss 2.0341 | lr 1.041572e-04\n",
      "step 44200 | train_loss 1.8532 | val_loss 2.0343 | lr 9.740488e-05\n",
      "step 44400 | train_loss 1.8537 | val_loss 2.0341 | lr 9.087141e-05\n",
      "step 44600 | train_loss 1.8536 | val_loss 2.0347 | lr 8.455780e-05\n",
      "step 44800 | train_loss 1.8540 | val_loss 2.0342 | lr 7.846506e-05\n",
      "step 45000 | train_loss 1.8534 | val_loss 2.0337 | lr 7.259414e-05\n",
      "step 45200 | train_loss 1.8543 | val_loss 2.0330 | lr 6.694599e-05\n",
      "step 45400 | train_loss 1.8548 | val_loss 2.0329 | lr 6.152147e-05\n",
      "step 45600 | train_loss 1.8565 | val_loss 2.0327 | lr 5.632146e-05\n",
      "step 45800 | train_loss 1.8569 | val_loss 2.0330 | lr 5.134678e-05\n",
      "step 46000 | train_loss 1.8575 | val_loss 2.0329 | lr 4.659821e-05\n",
      "step 46200 | train_loss 1.8579 | val_loss 2.0330 | lr 4.207649e-05\n",
      "step 46400 | train_loss 1.8589 | val_loss 2.0330 | lr 3.778235e-05\n",
      "step 46600 | train_loss 1.8595 | val_loss 2.0330 | lr 3.371646e-05\n",
      "step 46800 | train_loss 1.8558 | val_loss 2.0330 | lr 2.987947e-05\n",
      "step 47000 | train_loss 1.8550 | val_loss 2.0331 | lr 2.627197e-05\n",
      "step 47200 | train_loss 1.8552 | val_loss 2.0332 | lr 2.289455e-05\n",
      "step 47400 | train_loss 1.8552 | val_loss 2.0332 | lr 1.974773e-05\n",
      "step 47600 | train_loss 1.8553 | val_loss 2.0332 | lr 1.683200e-05\n",
      "step 47800 | train_loss 1.8555 | val_loss 2.0332 | lr 1.414784e-05\n",
      "step 48000 | train_loss 1.8553 | val_loss 2.0334 | lr 1.169566e-05\n",
      "step 48200 | train_loss 1.8552 | val_loss 2.0333 | lr 9.475857e-06\n",
      "step 48400 | train_loss 1.8551 | val_loss 2.0333 | lr 7.488772e-06\n",
      "step 48600 | train_loss 1.8552 | val_loss 2.0332 | lr 5.734723e-06\n",
      "step 48800 | train_loss 1.8552 | val_loss 2.0332 | lr 4.213987e-06\n",
      "step 49000 | train_loss 1.8553 | val_loss 2.0332 | lr 2.926804e-06\n",
      "step 49200 | train_loss 1.8553 | val_loss 2.0332 | lr 1.873376e-06\n",
      "step 49400 | train_loss 1.8553 | val_loss 2.0332 | lr 1.053871e-06\n",
      "step 49600 | train_loss 1.8553 | val_loss 2.0332 | lr 4.684180e-07\n",
      "step 49800 | train_loss 1.8553 | val_loss 2.0332 | lr 1.171091e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:14:55,992] Trial 12 finished with value: 2.0331808466996466 and parameters: {'embedding_size': 16, 'hidden_size': 289, 'learning_rate': 0.0029664477823724117, 'batch_size': 64, 'context_length': 5}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8553 | val_loss 2.0332 | lr 2.927767e-12\n",
      "step 0 | train_loss 3.3180 | val_loss 3.3136 | lr 9.882995e-03\n",
      "step 200 | train_loss 2.4844 | val_loss 2.4737 | lr 9.882605e-03\n",
      "step 400 | train_loss 2.3693 | val_loss 2.3741 | lr 9.881435e-03\n",
      "step 600 | train_loss 2.3862 | val_loss 2.3998 | lr 9.879484e-03\n",
      "step 800 | train_loss 2.4421 | val_loss 2.3848 | lr 9.876754e-03\n",
      "step 1000 | train_loss 2.4268 | val_loss 2.3941 | lr 9.873244e-03\n",
      "step 1200 | train_loss 2.3323 | val_loss 2.3713 | lr 9.868956e-03\n",
      "step 1400 | train_loss 2.3789 | val_loss 2.3669 | lr 9.863889e-03\n",
      "step 1600 | train_loss 2.3712 | val_loss 2.3607 | lr 9.858046e-03\n",
      "step 1800 | train_loss 2.3815 | val_loss 2.3440 | lr 9.851425e-03\n",
      "step 2000 | train_loss 2.3872 | val_loss 2.3859 | lr 9.844030e-03\n",
      "step 2200 | train_loss 2.3955 | val_loss 2.3998 | lr 9.835860e-03\n",
      "step 2400 | train_loss 2.3636 | val_loss 2.3340 | lr 9.826918e-03\n",
      "step 2600 | train_loss 2.3837 | val_loss 2.3684 | lr 9.817204e-03\n",
      "step 2800 | train_loss 2.3471 | val_loss 2.3327 | lr 9.806720e-03\n",
      "step 3000 | train_loss 2.3550 | val_loss 2.3177 | lr 9.795468e-03\n",
      "step 3200 | train_loss 2.3834 | val_loss 2.3506 | lr 9.783449e-03\n",
      "step 3400 | train_loss 2.1476 | val_loss 2.3446 | lr 9.770666e-03\n",
      "step 3600 | train_loss 2.2723 | val_loss 2.3139 | lr 9.757120e-03\n",
      "step 3800 | train_loss 2.3276 | val_loss 2.3337 | lr 9.742813e-03\n",
      "step 4000 | train_loss 2.3119 | val_loss 2.3144 | lr 9.727749e-03\n",
      "step 4200 | train_loss 2.3043 | val_loss 2.3370 | lr 9.711929e-03\n",
      "step 4400 | train_loss 2.3074 | val_loss 2.3421 | lr 9.695355e-03\n",
      "step 4600 | train_loss 2.3005 | val_loss 2.2996 | lr 9.678031e-03\n",
      "step 4800 | train_loss 2.3070 | val_loss 2.3079 | lr 9.659958e-03\n",
      "step 5000 | train_loss 2.3907 | val_loss 2.3521 | lr 9.641141e-03\n",
      "step 5200 | train_loss 2.3073 | val_loss 2.3175 | lr 9.621581e-03\n",
      "step 5400 | train_loss 2.3236 | val_loss 2.3268 | lr 9.601283e-03\n",
      "step 5600 | train_loss 2.3833 | val_loss 2.3523 | lr 9.580249e-03\n",
      "step 5800 | train_loss 2.3212 | val_loss 2.3201 | lr 9.558482e-03\n",
      "step 6000 | train_loss 2.3166 | val_loss 2.3122 | lr 9.535986e-03\n",
      "step 6200 | train_loss 2.3375 | val_loss 2.3109 | lr 9.512764e-03\n",
      "step 6400 | train_loss 2.3024 | val_loss 2.2915 | lr 9.488821e-03\n",
      "step 6600 | train_loss 2.3018 | val_loss 2.2812 | lr 9.464160e-03\n",
      "step 6800 | train_loss 2.1731 | val_loss 2.3120 | lr 9.438784e-03\n",
      "step 7000 | train_loss 2.2692 | val_loss 2.2809 | lr 9.412698e-03\n",
      "step 7200 | train_loss 2.2961 | val_loss 2.2758 | lr 9.385906e-03\n",
      "step 7400 | train_loss 2.3071 | val_loss 2.3004 | lr 9.358413e-03\n",
      "step 7600 | train_loss 2.3173 | val_loss 2.3018 | lr 9.330222e-03\n",
      "step 7800 | train_loss 2.3010 | val_loss 2.3235 | lr 9.301337e-03\n",
      "step 8000 | train_loss 2.2733 | val_loss 2.3131 | lr 9.271765e-03\n",
      "step 8200 | train_loss 2.2835 | val_loss 2.3036 | lr 9.241508e-03\n",
      "step 8400 | train_loss 2.3142 | val_loss 2.2918 | lr 9.210573e-03\n",
      "step 8600 | train_loss 2.2801 | val_loss 2.2854 | lr 9.178963e-03\n",
      "step 8800 | train_loss 2.2693 | val_loss 2.2670 | lr 9.146685e-03\n",
      "step 9000 | train_loss 2.3232 | val_loss 2.3022 | lr 9.113742e-03\n",
      "step 9200 | train_loss 2.3425 | val_loss 2.3117 | lr 9.080140e-03\n",
      "step 9400 | train_loss 2.2951 | val_loss 2.2928 | lr 9.045885e-03\n",
      "step 9600 | train_loss 2.3079 | val_loss 2.2925 | lr 9.010982e-03\n",
      "step 9800 | train_loss 2.3032 | val_loss 2.3186 | lr 8.975436e-03\n",
      "step 10000 | train_loss 2.3198 | val_loss 2.2882 | lr 8.939253e-03\n",
      "step 10200 | train_loss 2.2311 | val_loss 2.2999 | lr 8.902439e-03\n",
      "step 10400 | train_loss 2.2820 | val_loss 2.2848 | lr 8.864999e-03\n",
      "step 10600 | train_loss 2.3264 | val_loss 2.3296 | lr 8.826940e-03\n",
      "step 10800 | train_loss 2.2662 | val_loss 2.2861 | lr 8.788267e-03\n",
      "step 11000 | train_loss 2.3137 | val_loss 2.3092 | lr 8.748987e-03\n",
      "step 11200 | train_loss 2.2800 | val_loss 2.3045 | lr 8.709105e-03\n",
      "step 11400 | train_loss 2.2621 | val_loss 2.2784 | lr 8.668629e-03\n",
      "step 11600 | train_loss 2.3078 | val_loss 2.3060 | lr 8.627564e-03\n",
      "step 11800 | train_loss 2.2824 | val_loss 2.2801 | lr 8.585917e-03\n",
      "step 12000 | train_loss 2.3066 | val_loss 2.2836 | lr 8.543694e-03\n",
      "step 12200 | train_loss 2.3113 | val_loss 2.3188 | lr 8.500903e-03\n",
      "step 12400 | train_loss 2.3338 | val_loss 2.2914 | lr 8.457549e-03\n",
      "step 12600 | train_loss 2.2725 | val_loss 2.2724 | lr 8.413641e-03\n",
      "step 12800 | train_loss 2.2958 | val_loss 2.2580 | lr 8.369184e-03\n",
      "step 13000 | train_loss 2.3097 | val_loss 2.2915 | lr 8.324185e-03\n",
      "step 13200 | train_loss 2.2760 | val_loss 2.2757 | lr 8.278653e-03\n",
      "step 13400 | train_loss 2.0735 | val_loss 2.2672 | lr 8.232594e-03\n",
      "step 13600 | train_loss 2.2314 | val_loss 2.2553 | lr 8.186014e-03\n",
      "step 13800 | train_loss 2.3115 | val_loss 2.3047 | lr 8.138923e-03\n",
      "step 14000 | train_loss 2.2780 | val_loss 2.2731 | lr 8.091327e-03\n",
      "step 14200 | train_loss 2.2782 | val_loss 2.2854 | lr 8.043233e-03\n",
      "step 14400 | train_loss 2.2900 | val_loss 2.2584 | lr 7.994649e-03\n",
      "step 14600 | train_loss 2.2537 | val_loss 2.2507 | lr 7.945584e-03\n",
      "step 14800 | train_loss 2.2446 | val_loss 2.2551 | lr 7.896044e-03\n",
      "step 15000 | train_loss 2.2654 | val_loss 2.2607 | lr 7.846037e-03\n",
      "step 15200 | train_loss 2.2881 | val_loss 2.2623 | lr 7.795572e-03\n",
      "step 15400 | train_loss 2.2892 | val_loss 2.2687 | lr 7.744656e-03\n",
      "step 15600 | train_loss 2.2559 | val_loss 2.2579 | lr 7.693297e-03\n",
      "step 15800 | train_loss 2.2965 | val_loss 2.2914 | lr 7.641504e-03\n",
      "step 16000 | train_loss 2.2981 | val_loss 2.2889 | lr 7.589284e-03\n",
      "step 16200 | train_loss 2.2683 | val_loss 2.2601 | lr 7.536647e-03\n",
      "step 16400 | train_loss 2.2746 | val_loss 2.2567 | lr 7.483599e-03\n",
      "step 16600 | train_loss 2.3131 | val_loss 2.2933 | lr 7.430150e-03\n",
      "step 16800 | train_loss 2.1787 | val_loss 2.2769 | lr 7.376308e-03\n",
      "step 17000 | train_loss 2.2374 | val_loss 2.2683 | lr 7.322082e-03\n",
      "step 17200 | train_loss 2.2493 | val_loss 2.2601 | lr 7.267480e-03\n",
      "step 17400 | train_loss 2.2752 | val_loss 2.2819 | lr 7.212510e-03\n",
      "step 17600 | train_loss 2.2609 | val_loss 2.2600 | lr 7.157182e-03\n",
      "step 17800 | train_loss 2.2625 | val_loss 2.2758 | lr 7.101504e-03\n",
      "step 18000 | train_loss 2.2100 | val_loss 2.2349 | lr 7.045485e-03\n",
      "step 18200 | train_loss 2.2527 | val_loss 2.2467 | lr 6.989133e-03\n",
      "step 18400 | train_loss 2.2675 | val_loss 2.2667 | lr 6.932459e-03\n",
      "step 18600 | train_loss 2.2388 | val_loss 2.2477 | lr 6.875470e-03\n",
      "step 18800 | train_loss 2.2566 | val_loss 2.2343 | lr 6.818175e-03\n",
      "step 19000 | train_loss 2.2718 | val_loss 2.2346 | lr 6.760584e-03\n",
      "step 19200 | train_loss 2.2530 | val_loss 2.2372 | lr 6.702706e-03\n",
      "step 19400 | train_loss 2.2551 | val_loss 2.2345 | lr 6.644550e-03\n",
      "step 19600 | train_loss 2.2479 | val_loss 2.2366 | lr 6.586125e-03\n",
      "step 19800 | train_loss 2.2803 | val_loss 2.2318 | lr 6.527440e-03\n",
      "step 20000 | train_loss 2.2533 | val_loss 2.2489 | lr 6.468504e-03\n",
      "step 20200 | train_loss 2.1735 | val_loss 2.2299 | lr 6.409328e-03\n",
      "step 20400 | train_loss 2.1974 | val_loss 2.2313 | lr 6.349920e-03\n",
      "step 20600 | train_loss 2.2192 | val_loss 2.2301 | lr 6.290289e-03\n",
      "step 20800 | train_loss 2.2086 | val_loss 2.2313 | lr 6.230445e-03\n",
      "step 21000 | train_loss 2.2037 | val_loss 2.2059 | lr 6.170398e-03\n",
      "step 21200 | train_loss 2.2020 | val_loss 2.2128 | lr 6.110157e-03\n",
      "step 21400 | train_loss 2.1818 | val_loss 2.2036 | lr 6.049731e-03\n",
      "step 21600 | train_loss 2.2126 | val_loss 2.2100 | lr 5.989130e-03\n",
      "step 21800 | train_loss 2.2116 | val_loss 2.2119 | lr 5.928364e-03\n",
      "step 22000 | train_loss 2.1971 | val_loss 2.2042 | lr 5.867442e-03\n",
      "step 22200 | train_loss 2.2017 | val_loss 2.2086 | lr 5.806374e-03\n",
      "step 22400 | train_loss 2.2379 | val_loss 2.2164 | lr 5.745169e-03\n",
      "step 22600 | train_loss 2.2197 | val_loss 2.1956 | lr 5.683837e-03\n",
      "step 22800 | train_loss 2.2236 | val_loss 2.2278 | lr 5.622388e-03\n",
      "step 23000 | train_loss 2.2193 | val_loss 2.1884 | lr 5.560831e-03\n",
      "step 23200 | train_loss 2.2366 | val_loss 2.2076 | lr 5.499177e-03\n",
      "step 23400 | train_loss 2.0899 | val_loss 2.2023 | lr 5.437435e-03\n",
      "step 23600 | train_loss 2.1522 | val_loss 2.2220 | lr 5.375614e-03\n",
      "step 23800 | train_loss 2.1683 | val_loss 2.1908 | lr 5.313725e-03\n",
      "step 24000 | train_loss 2.1681 | val_loss 2.1969 | lr 5.251777e-03\n",
      "step 24200 | train_loss 2.1904 | val_loss 2.2154 | lr 5.189780e-03\n",
      "step 24400 | train_loss 2.1946 | val_loss 2.1972 | lr 5.127743e-03\n",
      "step 24600 | train_loss 2.1842 | val_loss 2.1992 | lr 5.065678e-03\n",
      "step 24800 | train_loss 2.2091 | val_loss 2.1949 | lr 5.003593e-03\n",
      "step 25000 | train_loss 2.2014 | val_loss 2.2039 | lr 4.941498e-03\n",
      "step 25200 | train_loss 2.2225 | val_loss 2.1995 | lr 4.879402e-03\n",
      "step 25400 | train_loss 2.1858 | val_loss 2.1766 | lr 4.817317e-03\n",
      "step 25600 | train_loss 2.2254 | val_loss 2.2204 | lr 4.755252e-03\n",
      "step 25800 | train_loss 2.1951 | val_loss 2.1962 | lr 4.693215e-03\n",
      "step 26000 | train_loss 2.2064 | val_loss 2.2083 | lr 4.631218e-03\n",
      "step 26200 | train_loss 2.2008 | val_loss 2.2004 | lr 4.569270e-03\n",
      "step 26400 | train_loss 2.1812 | val_loss 2.1825 | lr 4.507381e-03\n",
      "step 26600 | train_loss 2.1720 | val_loss 2.1807 | lr 4.445560e-03\n",
      "step 26800 | train_loss 2.1024 | val_loss 2.1931 | lr 4.383818e-03\n",
      "step 27000 | train_loss 2.1267 | val_loss 2.1733 | lr 4.322164e-03\n",
      "step 27200 | train_loss 2.1303 | val_loss 2.1795 | lr 4.260607e-03\n",
      "step 27400 | train_loss 2.1329 | val_loss 2.1737 | lr 4.199158e-03\n",
      "step 27600 | train_loss 2.1581 | val_loss 2.1853 | lr 4.137826e-03\n",
      "step 27800 | train_loss 2.1599 | val_loss 2.1896 | lr 4.076622e-03\n",
      "step 28000 | train_loss 2.1451 | val_loss 2.1706 | lr 4.015553e-03\n",
      "step 28200 | train_loss 2.1580 | val_loss 2.1782 | lr 3.954631e-03\n",
      "step 28400 | train_loss 2.1654 | val_loss 2.1826 | lr 3.893865e-03\n",
      "step 28600 | train_loss 2.1810 | val_loss 2.1663 | lr 3.833264e-03\n",
      "step 28800 | train_loss 2.1991 | val_loss 2.1725 | lr 3.772838e-03\n",
      "step 29000 | train_loss 2.1873 | val_loss 2.1718 | lr 3.712597e-03\n",
      "step 29200 | train_loss 2.1879 | val_loss 2.1692 | lr 3.652550e-03\n",
      "step 29400 | train_loss 2.1838 | val_loss 2.1651 | lr 3.592706e-03\n",
      "step 29600 | train_loss 2.1594 | val_loss 2.1532 | lr 3.533076e-03\n",
      "step 29800 | train_loss 2.1728 | val_loss 2.1650 | lr 3.473667e-03\n",
      "step 30000 | train_loss 2.1686 | val_loss 2.1612 | lr 3.414491e-03\n",
      "step 30200 | train_loss 2.0842 | val_loss 2.1491 | lr 3.355555e-03\n",
      "step 30400 | train_loss 2.1083 | val_loss 2.1436 | lr 3.296871e-03\n",
      "step 30600 | train_loss 2.1227 | val_loss 2.1482 | lr 3.238445e-03\n",
      "step 30800 | train_loss 2.1086 | val_loss 2.1432 | lr 3.180289e-03\n",
      "step 31000 | train_loss 2.1328 | val_loss 2.1521 | lr 3.122411e-03\n",
      "step 31200 | train_loss 2.1189 | val_loss 2.1462 | lr 3.064820e-03\n",
      "step 31400 | train_loss 2.1373 | val_loss 2.1465 | lr 3.007526e-03\n",
      "step 31600 | train_loss 2.1532 | val_loss 2.1508 | lr 2.950536e-03\n",
      "step 31800 | train_loss 2.1402 | val_loss 2.1340 | lr 2.893862e-03\n",
      "step 32000 | train_loss 2.1437 | val_loss 2.1481 | lr 2.837510e-03\n",
      "step 32200 | train_loss 2.1437 | val_loss 2.1399 | lr 2.781491e-03\n",
      "step 32400 | train_loss 2.1208 | val_loss 2.1380 | lr 2.725813e-03\n",
      "step 32600 | train_loss 2.1381 | val_loss 2.1454 | lr 2.670485e-03\n",
      "step 32800 | train_loss 2.1258 | val_loss 2.1306 | lr 2.615515e-03\n",
      "step 33000 | train_loss 2.1490 | val_loss 2.1428 | lr 2.560913e-03\n",
      "step 33200 | train_loss 2.1390 | val_loss 2.1317 | lr 2.506687e-03\n",
      "step 33400 | train_loss 2.1452 | val_loss 2.1292 | lr 2.452845e-03\n",
      "step 33600 | train_loss 2.0947 | val_loss 2.1359 | lr 2.399396e-03\n",
      "step 33800 | train_loss 2.0927 | val_loss 2.1225 | lr 2.346348e-03\n",
      "step 34000 | train_loss 2.1126 | val_loss 2.1296 | lr 2.293711e-03\n",
      "step 34200 | train_loss 2.0993 | val_loss 2.1224 | lr 2.241491e-03\n",
      "step 34400 | train_loss 2.1131 | val_loss 2.1307 | lr 2.189698e-03\n",
      "step 34600 | train_loss 2.0975 | val_loss 2.1210 | lr 2.138339e-03\n",
      "step 34800 | train_loss 2.1033 | val_loss 2.1303 | lr 2.087423e-03\n",
      "step 35000 | train_loss 2.1111 | val_loss 2.1324 | lr 2.036958e-03\n",
      "step 35200 | train_loss 2.1154 | val_loss 2.1317 | lr 1.986952e-03\n",
      "step 35400 | train_loss 2.1251 | val_loss 2.1219 | lr 1.937411e-03\n",
      "step 35600 | train_loss 2.1201 | val_loss 2.1310 | lr 1.888346e-03\n",
      "step 35800 | train_loss 2.1182 | val_loss 2.1197 | lr 1.839762e-03\n",
      "step 36000 | train_loss 2.0981 | val_loss 2.1143 | lr 1.791668e-03\n",
      "step 36200 | train_loss 2.1038 | val_loss 2.1156 | lr 1.744072e-03\n",
      "step 36400 | train_loss 2.1000 | val_loss 2.1120 | lr 1.696981e-03\n",
      "step 36600 | train_loss 2.1144 | val_loss 2.1133 | lr 1.650402e-03\n",
      "step 36800 | train_loss 2.0410 | val_loss 2.1078 | lr 1.604342e-03\n",
      "step 37000 | train_loss 2.0616 | val_loss 2.1106 | lr 1.558810e-03\n",
      "step 37200 | train_loss 2.0813 | val_loss 2.1107 | lr 1.513811e-03\n",
      "step 37400 | train_loss 2.0737 | val_loss 2.1032 | lr 1.469354e-03\n",
      "step 37600 | train_loss 2.0795 | val_loss 2.1059 | lr 1.425446e-03\n",
      "step 37800 | train_loss 2.0815 | val_loss 2.1036 | lr 1.382092e-03\n",
      "step 38000 | train_loss 2.0741 | val_loss 2.1106 | lr 1.339301e-03\n",
      "step 38200 | train_loss 2.0601 | val_loss 2.0996 | lr 1.297078e-03\n",
      "step 38400 | train_loss 2.0674 | val_loss 2.1011 | lr 1.255431e-03\n",
      "step 38600 | train_loss 2.0815 | val_loss 2.0984 | lr 1.214366e-03\n",
      "step 38800 | train_loss 2.0844 | val_loss 2.0985 | lr 1.173890e-03\n",
      "step 39000 | train_loss 2.0749 | val_loss 2.1024 | lr 1.134008e-03\n",
      "step 39200 | train_loss 2.0754 | val_loss 2.0953 | lr 1.094728e-03\n",
      "step 39400 | train_loss 2.0786 | val_loss 2.0972 | lr 1.056055e-03\n",
      "step 39600 | train_loss 2.0749 | val_loss 2.0973 | lr 1.017996e-03\n",
      "step 39800 | train_loss 2.0749 | val_loss 2.0911 | lr 9.805563e-04\n",
      "step 40000 | train_loss 2.0731 | val_loss 2.0915 | lr 9.437421e-04\n",
      "step 40200 | train_loss 2.0278 | val_loss 2.0929 | lr 9.075591e-04\n",
      "step 40400 | train_loss 2.0370 | val_loss 2.0908 | lr 8.720132e-04\n",
      "step 40600 | train_loss 2.0449 | val_loss 2.0958 | lr 8.371099e-04\n",
      "step 40800 | train_loss 2.0452 | val_loss 2.0925 | lr 8.028548e-04\n",
      "step 41000 | train_loss 2.0548 | val_loss 2.0918 | lr 7.692532e-04\n",
      "step 41200 | train_loss 2.0475 | val_loss 2.0899 | lr 7.363104e-04\n",
      "step 41400 | train_loss 2.0518 | val_loss 2.0894 | lr 7.040317e-04\n",
      "step 41600 | train_loss 2.0546 | val_loss 2.0887 | lr 6.724221e-04\n",
      "step 41800 | train_loss 2.0511 | val_loss 2.0849 | lr 6.414867e-04\n",
      "step 42000 | train_loss 2.0532 | val_loss 2.0882 | lr 6.112302e-04\n",
      "step 42200 | train_loss 2.0562 | val_loss 2.0842 | lr 5.816576e-04\n",
      "step 42400 | train_loss 2.0522 | val_loss 2.0834 | lr 5.527735e-04\n",
      "step 42600 | train_loss 2.0534 | val_loss 2.0821 | lr 5.245823e-04\n",
      "step 42800 | train_loss 2.0551 | val_loss 2.0828 | lr 4.970887e-04\n",
      "step 43000 | train_loss 2.0572 | val_loss 2.0820 | lr 4.702969e-04\n",
      "step 43200 | train_loss 2.0630 | val_loss 2.0805 | lr 4.442111e-04\n",
      "step 43400 | train_loss 2.0511 | val_loss 2.0797 | lr 4.188355e-04\n",
      "step 43600 | train_loss 2.0268 | val_loss 2.0797 | lr 3.941741e-04\n",
      "step 43800 | train_loss 2.0324 | val_loss 2.0839 | lr 3.702308e-04\n",
      "step 44000 | train_loss 2.0322 | val_loss 2.0785 | lr 3.470093e-04\n",
      "step 44200 | train_loss 2.0334 | val_loss 2.0786 | lr 3.245134e-04\n",
      "step 44400 | train_loss 2.0373 | val_loss 2.0773 | lr 3.027465e-04\n",
      "step 44600 | train_loss 2.0341 | val_loss 2.0785 | lr 2.817121e-04\n",
      "step 44800 | train_loss 2.0343 | val_loss 2.0787 | lr 2.614136e-04\n",
      "step 45000 | train_loss 2.0400 | val_loss 2.0777 | lr 2.418541e-04\n",
      "step 45200 | train_loss 2.0364 | val_loss 2.0747 | lr 2.230367e-04\n",
      "step 45400 | train_loss 2.0364 | val_loss 2.0758 | lr 2.049645e-04\n",
      "step 45600 | train_loss 2.0394 | val_loss 2.0751 | lr 1.876402e-04\n",
      "step 45800 | train_loss 2.0360 | val_loss 2.0755 | lr 1.710665e-04\n",
      "step 46000 | train_loss 2.0363 | val_loss 2.0759 | lr 1.552462e-04\n",
      "step 46200 | train_loss 2.0391 | val_loss 2.0765 | lr 1.401817e-04\n",
      "step 46400 | train_loss 2.0397 | val_loss 2.0748 | lr 1.258754e-04\n",
      "step 46600 | train_loss 2.0405 | val_loss 2.0747 | lr 1.123295e-04\n",
      "step 46800 | train_loss 2.0291 | val_loss 2.0741 | lr 9.954621e-05\n",
      "step 47000 | train_loss 2.0292 | val_loss 2.0746 | lr 8.752751e-05\n",
      "step 47200 | train_loss 2.0291 | val_loss 2.0746 | lr 7.627530e-05\n",
      "step 47400 | train_loss 2.0303 | val_loss 2.0747 | lr 6.579137e-05\n",
      "step 47600 | train_loss 2.0289 | val_loss 2.0747 | lr 5.607738e-05\n",
      "step 47800 | train_loss 2.0301 | val_loss 2.0742 | lr 4.713484e-05\n",
      "step 48000 | train_loss 2.0301 | val_loss 2.0747 | lr 3.896518e-05\n",
      "step 48200 | train_loss 2.0295 | val_loss 2.0746 | lr 3.156969e-05\n",
      "step 48400 | train_loss 2.0293 | val_loss 2.0742 | lr 2.494954e-05\n",
      "step 48600 | train_loss 2.0294 | val_loss 2.0737 | lr 1.910576e-05\n",
      "step 48800 | train_loss 2.0298 | val_loss 2.0737 | lr 1.403929e-05\n",
      "step 49000 | train_loss 2.0300 | val_loss 2.0736 | lr 9.750917e-06\n",
      "step 49200 | train_loss 2.0300 | val_loss 2.0736 | lr 6.241326e-06\n",
      "step 49400 | train_loss 2.0299 | val_loss 2.0736 | lr 3.511069e-06\n",
      "step 49600 | train_loss 2.0300 | val_loss 2.0736 | lr 1.560578e-06\n",
      "step 49800 | train_loss 2.0300 | val_loss 2.0736 | lr 3.901599e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:16:00,435] Trial 13 finished with value: 2.073575678680624 and parameters: {'embedding_size': 60, 'hidden_size': 403, 'learning_rate': 0.009882995088996524, 'batch_size': 64, 'context_length': 4}. Best is trial 2 with value: 2.0138411607061113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 2.0300 | val_loss 2.0736 | lr 9.754125e-12\n",
      "step 0 | train_loss 3.2850 | val_loss 3.2966 | lr 4.788129e-03\n",
      "step 200 | train_loss 2.3539 | val_loss 2.3602 | lr 4.787940e-03\n",
      "step 400 | train_loss 2.2785 | val_loss 2.2931 | lr 4.787373e-03\n",
      "step 600 | train_loss 2.2577 | val_loss 2.2703 | lr 4.786428e-03\n",
      "step 800 | train_loss 2.2625 | val_loss 2.2318 | lr 4.785105e-03\n",
      "step 1000 | train_loss 2.2676 | val_loss 2.2426 | lr 4.783405e-03\n",
      "step 1200 | train_loss 2.2081 | val_loss 2.2350 | lr 4.781327e-03\n",
      "step 1400 | train_loss 2.2294 | val_loss 2.2177 | lr 4.778873e-03\n",
      "step 1600 | train_loss 2.2053 | val_loss 2.1976 | lr 4.776042e-03\n",
      "step 1800 | train_loss 2.2210 | val_loss 2.1945 | lr 4.772834e-03\n",
      "step 2000 | train_loss 2.2087 | val_loss 2.1897 | lr 4.769251e-03\n",
      "step 2200 | train_loss 2.1826 | val_loss 2.1849 | lr 4.765293e-03\n",
      "step 2400 | train_loss 2.1922 | val_loss 2.1717 | lr 4.760961e-03\n",
      "step 2600 | train_loss 2.1933 | val_loss 2.1731 | lr 4.756254e-03\n",
      "step 2800 | train_loss 2.1969 | val_loss 2.1705 | lr 4.751175e-03\n",
      "step 3000 | train_loss 2.1826 | val_loss 2.1578 | lr 4.745724e-03\n",
      "step 3200 | train_loss 2.1826 | val_loss 2.1536 | lr 4.739901e-03\n",
      "step 3400 | train_loss 1.8961 | val_loss 2.1595 | lr 4.733708e-03\n",
      "step 3600 | train_loss 2.0520 | val_loss 2.1584 | lr 4.727145e-03\n",
      "step 3800 | train_loss 2.0808 | val_loss 2.1424 | lr 4.720214e-03\n",
      "step 4000 | train_loss 2.1052 | val_loss 2.1374 | lr 4.712915e-03\n",
      "step 4200 | train_loss 2.1340 | val_loss 2.1607 | lr 4.705251e-03\n",
      "step 4400 | train_loss 2.1210 | val_loss 2.1424 | lr 4.697221e-03\n",
      "step 4600 | train_loss 2.1195 | val_loss 2.1526 | lr 4.688828e-03\n",
      "step 4800 | train_loss 2.1128 | val_loss 2.1332 | lr 4.680072e-03\n",
      "step 5000 | train_loss 2.1216 | val_loss 2.1365 | lr 4.670955e-03\n",
      "step 5200 | train_loss 2.1387 | val_loss 2.1351 | lr 4.661479e-03\n",
      "step 5400 | train_loss 2.1543 | val_loss 2.1287 | lr 4.651645e-03\n",
      "step 5600 | train_loss 2.1469 | val_loss 2.1398 | lr 4.641454e-03\n",
      "step 5800 | train_loss 2.1526 | val_loss 2.1358 | lr 4.630908e-03\n",
      "step 6000 | train_loss 2.1600 | val_loss 2.1398 | lr 4.620009e-03\n",
      "step 6200 | train_loss 2.1379 | val_loss 2.1373 | lr 4.608759e-03\n",
      "step 6400 | train_loss 2.1647 | val_loss 2.1277 | lr 4.597159e-03\n",
      "step 6600 | train_loss 2.1209 | val_loss 2.1140 | lr 4.585211e-03\n",
      "step 6800 | train_loss 1.9046 | val_loss 2.1237 | lr 4.572917e-03\n",
      "step 7000 | train_loss 2.0204 | val_loss 2.1072 | lr 4.560279e-03\n",
      "step 7200 | train_loss 2.0530 | val_loss 2.1198 | lr 4.547299e-03\n",
      "step 7400 | train_loss 2.0696 | val_loss 2.1069 | lr 4.533979e-03\n",
      "step 7600 | train_loss 2.0840 | val_loss 2.1161 | lr 4.520321e-03\n",
      "step 7800 | train_loss 2.0540 | val_loss 2.1100 | lr 4.506327e-03\n",
      "step 8000 | train_loss 2.0632 | val_loss 2.1165 | lr 4.491999e-03\n",
      "step 8200 | train_loss 2.0590 | val_loss 2.1014 | lr 4.477341e-03\n",
      "step 8400 | train_loss 2.0997 | val_loss 2.0968 | lr 4.462353e-03\n",
      "step 8600 | train_loss 2.0863 | val_loss 2.1098 | lr 4.447039e-03\n",
      "step 8800 | train_loss 2.1045 | val_loss 2.1009 | lr 4.431400e-03\n",
      "step 9000 | train_loss 2.1122 | val_loss 2.1032 | lr 4.415440e-03\n",
      "step 9200 | train_loss 2.1099 | val_loss 2.1054 | lr 4.399161e-03\n",
      "step 9400 | train_loss 2.0966 | val_loss 2.1042 | lr 4.382565e-03\n",
      "step 9600 | train_loss 2.1009 | val_loss 2.1120 | lr 4.365655e-03\n",
      "step 9800 | train_loss 2.0998 | val_loss 2.0964 | lr 4.348433e-03\n",
      "step 10000 | train_loss 2.1068 | val_loss 2.0938 | lr 4.330903e-03\n",
      "step 10200 | train_loss 1.9414 | val_loss 2.1014 | lr 4.313068e-03\n",
      "step 10400 | train_loss 1.9860 | val_loss 2.0911 | lr 4.294929e-03\n",
      "step 10600 | train_loss 2.0159 | val_loss 2.0788 | lr 4.276490e-03\n",
      "step 10800 | train_loss 2.0240 | val_loss 2.0797 | lr 4.257754e-03\n",
      "step 11000 | train_loss 2.0262 | val_loss 2.0913 | lr 4.238723e-03\n",
      "step 11200 | train_loss 2.0244 | val_loss 2.0948 | lr 4.219401e-03\n",
      "step 11400 | train_loss 2.0355 | val_loss 2.0926 | lr 4.199791e-03\n",
      "step 11600 | train_loss 2.0495 | val_loss 2.0962 | lr 4.179896e-03\n",
      "step 11800 | train_loss 2.0562 | val_loss 2.0888 | lr 4.159719e-03\n",
      "step 12000 | train_loss 2.0902 | val_loss 2.0939 | lr 4.139263e-03\n",
      "step 12200 | train_loss 2.0681 | val_loss 2.0936 | lr 4.118531e-03\n",
      "step 12400 | train_loss 2.0956 | val_loss 2.0886 | lr 4.097527e-03\n",
      "step 12600 | train_loss 2.0779 | val_loss 2.0968 | lr 4.076254e-03\n",
      "step 12800 | train_loss 2.0679 | val_loss 2.0859 | lr 4.054715e-03\n",
      "step 13000 | train_loss 2.0708 | val_loss 2.0924 | lr 4.032915e-03\n",
      "step 13200 | train_loss 2.0607 | val_loss 2.0830 | lr 4.010855e-03\n",
      "step 13400 | train_loss 1.7771 | val_loss 2.0767 | lr 3.988540e-03\n",
      "step 13600 | train_loss 1.9112 | val_loss 2.0766 | lr 3.965973e-03\n",
      "step 13800 | train_loss 1.9728 | val_loss 2.0733 | lr 3.943158e-03\n",
      "step 14000 | train_loss 1.9849 | val_loss 2.0682 | lr 3.920099e-03\n",
      "step 14200 | train_loss 1.9998 | val_loss 2.0820 | lr 3.896798e-03\n",
      "step 14400 | train_loss 2.0018 | val_loss 2.0648 | lr 3.873260e-03\n",
      "step 14600 | train_loss 1.9892 | val_loss 2.0743 | lr 3.849489e-03\n",
      "step 14800 | train_loss 2.0036 | val_loss 2.0700 | lr 3.825488e-03\n",
      "step 15000 | train_loss 2.0144 | val_loss 2.0690 | lr 3.801260e-03\n",
      "step 15200 | train_loss 2.0594 | val_loss 2.0825 | lr 3.776811e-03\n",
      "step 15400 | train_loss 2.0551 | val_loss 2.0715 | lr 3.752143e-03\n",
      "step 15600 | train_loss 2.0392 | val_loss 2.0776 | lr 3.727261e-03\n",
      "step 15800 | train_loss 2.0392 | val_loss 2.0780 | lr 3.702168e-03\n",
      "step 16000 | train_loss 2.0660 | val_loss 2.0846 | lr 3.676868e-03\n",
      "step 16200 | train_loss 2.0379 | val_loss 2.0789 | lr 3.651367e-03\n",
      "step 16400 | train_loss 2.0347 | val_loss 2.0732 | lr 3.625666e-03\n",
      "step 16600 | train_loss 2.0517 | val_loss 2.0770 | lr 3.599771e-03\n",
      "step 16800 | train_loss 1.8125 | val_loss 2.0822 | lr 3.573686e-03\n",
      "step 17000 | train_loss 1.8994 | val_loss 2.0765 | lr 3.547414e-03\n",
      "step 17200 | train_loss 1.9463 | val_loss 2.0708 | lr 3.520960e-03\n",
      "step 17400 | train_loss 1.9574 | val_loss 2.0753 | lr 3.494328e-03\n",
      "step 17600 | train_loss 1.9793 | val_loss 2.0764 | lr 3.467523e-03\n",
      "step 17800 | train_loss 1.9638 | val_loss 2.0648 | lr 3.440548e-03\n",
      "step 18000 | train_loss 1.9741 | val_loss 2.0626 | lr 3.413408e-03\n",
      "step 18200 | train_loss 1.9834 | val_loss 2.0688 | lr 3.386106e-03\n",
      "step 18400 | train_loss 2.0011 | val_loss 2.0604 | lr 3.358649e-03\n",
      "step 18600 | train_loss 2.0001 | val_loss 2.0714 | lr 3.331038e-03\n",
      "step 18800 | train_loss 2.0394 | val_loss 2.0671 | lr 3.303280e-03\n",
      "step 19000 | train_loss 2.0282 | val_loss 2.0577 | lr 3.275379e-03\n",
      "step 19200 | train_loss 2.0326 | val_loss 2.0666 | lr 3.247338e-03\n",
      "step 19400 | train_loss 2.0411 | val_loss 2.0661 | lr 3.219162e-03\n",
      "step 19600 | train_loss 2.0358 | val_loss 2.0788 | lr 3.190856e-03\n",
      "step 19800 | train_loss 2.0335 | val_loss 2.0644 | lr 3.162424e-03\n",
      "step 20000 | train_loss 2.0005 | val_loss 2.0512 | lr 3.133871e-03\n",
      "step 20200 | train_loss 1.8323 | val_loss 2.0648 | lr 3.105201e-03\n",
      "step 20400 | train_loss 1.8955 | val_loss 2.0620 | lr 3.076419e-03\n",
      "step 20600 | train_loss 1.9198 | val_loss 2.0573 | lr 3.047529e-03\n",
      "step 20800 | train_loss 1.9368 | val_loss 2.0574 | lr 3.018536e-03\n",
      "step 21000 | train_loss 1.9467 | val_loss 2.0613 | lr 2.989444e-03\n",
      "step 21200 | train_loss 1.9540 | val_loss 2.0575 | lr 2.960258e-03\n",
      "step 21400 | train_loss 1.9574 | val_loss 2.0613 | lr 2.930983e-03\n",
      "step 21600 | train_loss 1.9706 | val_loss 2.0537 | lr 2.901623e-03\n",
      "step 21800 | train_loss 1.9959 | val_loss 2.0591 | lr 2.872183e-03\n",
      "step 22000 | train_loss 1.9712 | val_loss 2.0608 | lr 2.842668e-03\n",
      "step 22200 | train_loss 1.9985 | val_loss 2.0645 | lr 2.813081e-03\n",
      "step 22400 | train_loss 1.9957 | val_loss 2.0647 | lr 2.783428e-03\n",
      "step 22600 | train_loss 2.0046 | val_loss 2.0584 | lr 2.753714e-03\n",
      "step 22800 | train_loss 1.9853 | val_loss 2.0598 | lr 2.723943e-03\n",
      "step 23000 | train_loss 1.9874 | val_loss 2.0563 | lr 2.694120e-03\n",
      "step 23200 | train_loss 1.9812 | val_loss 2.0508 | lr 2.664250e-03\n",
      "step 23400 | train_loss 1.8336 | val_loss 2.0506 | lr 2.634337e-03\n",
      "step 23600 | train_loss 1.8479 | val_loss 2.0569 | lr 2.604386e-03\n",
      "step 23800 | train_loss 1.8870 | val_loss 2.0487 | lr 2.574402e-03\n",
      "step 24000 | train_loss 1.8935 | val_loss 2.0444 | lr 2.544389e-03\n",
      "step 24200 | train_loss 1.9255 | val_loss 2.0523 | lr 2.514353e-03\n",
      "step 24400 | train_loss 1.9380 | val_loss 2.0439 | lr 2.484297e-03\n",
      "step 24600 | train_loss 1.9245 | val_loss 2.0504 | lr 2.454228e-03\n",
      "step 24800 | train_loss 1.9356 | val_loss 2.0457 | lr 2.424148e-03\n",
      "step 25000 | train_loss 1.9387 | val_loss 2.0488 | lr 2.394065e-03\n",
      "step 25200 | train_loss 1.9653 | val_loss 2.0446 | lr 2.363981e-03\n",
      "step 25400 | train_loss 1.9657 | val_loss 2.0494 | lr 2.333901e-03\n",
      "step 25600 | train_loss 1.9596 | val_loss 2.0543 | lr 2.303832e-03\n",
      "step 25800 | train_loss 1.9679 | val_loss 2.0483 | lr 2.273776e-03\n",
      "step 26000 | train_loss 1.9584 | val_loss 2.0453 | lr 2.243740e-03\n",
      "step 26200 | train_loss 1.9707 | val_loss 2.0508 | lr 2.213727e-03\n",
      "step 26400 | train_loss 1.9660 | val_loss 2.0477 | lr 2.183743e-03\n",
      "step 26600 | train_loss 1.9618 | val_loss 2.0490 | lr 2.153792e-03\n",
      "step 26800 | train_loss 1.7676 | val_loss 2.0441 | lr 2.123879e-03\n",
      "step 27000 | train_loss 1.8415 | val_loss 2.0394 | lr 2.094009e-03\n",
      "step 27200 | train_loss 1.8638 | val_loss 2.0401 | lr 2.064186e-03\n",
      "step 27400 | train_loss 1.8842 | val_loss 2.0414 | lr 2.034415e-03\n",
      "step 27600 | train_loss 1.9030 | val_loss 2.0459 | lr 2.004701e-03\n",
      "step 27800 | train_loss 1.9034 | val_loss 2.0339 | lr 1.975048e-03\n",
      "step 28000 | train_loss 1.8950 | val_loss 2.0376 | lr 1.945462e-03\n",
      "step 28200 | train_loss 1.9093 | val_loss 2.0396 | lr 1.915946e-03\n",
      "step 28400 | train_loss 1.9237 | val_loss 2.0377 | lr 1.886506e-03\n",
      "step 28600 | train_loss 1.9407 | val_loss 2.0379 | lr 1.857146e-03\n",
      "step 28800 | train_loss 1.9370 | val_loss 2.0386 | lr 1.827871e-03\n",
      "step 29000 | train_loss 1.9423 | val_loss 2.0456 | lr 1.798685e-03\n",
      "step 29200 | train_loss 1.9407 | val_loss 2.0399 | lr 1.769593e-03\n",
      "step 29400 | train_loss 1.9436 | val_loss 2.0450 | lr 1.740600e-03\n",
      "step 29600 | train_loss 1.9459 | val_loss 2.0453 | lr 1.711710e-03\n",
      "step 29800 | train_loss 1.9459 | val_loss 2.0426 | lr 1.682928e-03\n",
      "step 30000 | train_loss 1.9099 | val_loss 2.0372 | lr 1.654258e-03\n",
      "step 30200 | train_loss 1.7818 | val_loss 2.0355 | lr 1.625705e-03\n",
      "step 30400 | train_loss 1.8450 | val_loss 2.0323 | lr 1.597273e-03\n",
      "step 30600 | train_loss 1.8666 | val_loss 2.0334 | lr 1.568967e-03\n",
      "step 30800 | train_loss 1.8739 | val_loss 2.0291 | lr 1.540792e-03\n",
      "step 31000 | train_loss 1.8864 | val_loss 2.0366 | lr 1.512751e-03\n",
      "step 31200 | train_loss 1.8837 | val_loss 2.0343 | lr 1.484849e-03\n",
      "step 31400 | train_loss 1.8858 | val_loss 2.0346 | lr 1.457091e-03\n",
      "step 31600 | train_loss 1.8883 | val_loss 2.0337 | lr 1.429481e-03\n",
      "step 31800 | train_loss 1.9007 | val_loss 2.0306 | lr 1.402023e-03\n",
      "step 32000 | train_loss 1.9173 | val_loss 2.0341 | lr 1.374721e-03\n",
      "step 32200 | train_loss 1.9147 | val_loss 2.0304 | lr 1.347581e-03\n",
      "step 32400 | train_loss 1.9122 | val_loss 2.0324 | lr 1.320606e-03\n",
      "step 32600 | train_loss 1.9148 | val_loss 2.0328 | lr 1.293801e-03\n",
      "step 32800 | train_loss 1.9116 | val_loss 2.0287 | lr 1.267169e-03\n",
      "step 33000 | train_loss 1.9249 | val_loss 2.0380 | lr 1.240715e-03\n",
      "step 33200 | train_loss 1.9117 | val_loss 2.0329 | lr 1.214443e-03\n",
      "step 33400 | train_loss 1.8939 | val_loss 2.0261 | lr 1.188358e-03\n",
      "step 33600 | train_loss 1.8064 | val_loss 2.0308 | lr 1.162463e-03\n",
      "step 33800 | train_loss 1.8306 | val_loss 2.0275 | lr 1.136763e-03\n",
      "step 34000 | train_loss 1.8514 | val_loss 2.0237 | lr 1.111261e-03\n",
      "step 34200 | train_loss 1.8602 | val_loss 2.0278 | lr 1.085961e-03\n",
      "step 34400 | train_loss 1.8619 | val_loss 2.0255 | lr 1.060868e-03\n",
      "step 34600 | train_loss 1.8671 | val_loss 2.0275 | lr 1.035986e-03\n",
      "step 34800 | train_loss 1.8590 | val_loss 2.0254 | lr 1.011318e-03\n",
      "step 35000 | train_loss 1.8704 | val_loss 2.0277 | lr 9.868687e-04\n",
      "step 35200 | train_loss 1.8746 | val_loss 2.0273 | lr 9.626414e-04\n",
      "step 35400 | train_loss 1.8864 | val_loss 2.0215 | lr 9.386402e-04\n",
      "step 35600 | train_loss 1.8854 | val_loss 2.0288 | lr 9.148688e-04\n",
      "step 35800 | train_loss 1.8937 | val_loss 2.0243 | lr 8.913309e-04\n",
      "step 36000 | train_loss 1.8870 | val_loss 2.0252 | lr 8.680304e-04\n",
      "step 36200 | train_loss 1.8907 | val_loss 2.0210 | lr 8.449708e-04\n",
      "step 36400 | train_loss 1.8882 | val_loss 2.0278 | lr 8.221559e-04\n",
      "step 36600 | train_loss 1.8808 | val_loss 2.0214 | lr 7.995892e-04\n",
      "step 36800 | train_loss 1.7834 | val_loss 2.0213 | lr 7.772742e-04\n",
      "step 37000 | train_loss 1.8069 | val_loss 2.0226 | lr 7.552146e-04\n",
      "step 37200 | train_loss 1.8266 | val_loss 2.0191 | lr 7.334138e-04\n",
      "step 37400 | train_loss 1.8337 | val_loss 2.0172 | lr 7.118752e-04\n",
      "step 37600 | train_loss 1.8440 | val_loss 2.0200 | lr 6.906022e-04\n",
      "step 37800 | train_loss 1.8504 | val_loss 2.0182 | lr 6.695983e-04\n",
      "step 38000 | train_loss 1.8430 | val_loss 2.0185 | lr 6.488666e-04\n",
      "step 38200 | train_loss 1.8438 | val_loss 2.0179 | lr 6.284105e-04\n",
      "step 38400 | train_loss 1.8488 | val_loss 2.0176 | lr 6.082333e-04\n",
      "step 38600 | train_loss 1.8622 | val_loss 2.0175 | lr 5.883381e-04\n",
      "step 38800 | train_loss 1.8630 | val_loss 2.0145 | lr 5.687280e-04\n",
      "step 39000 | train_loss 1.8644 | val_loss 2.0199 | lr 5.494061e-04\n",
      "step 39200 | train_loss 1.8643 | val_loss 2.0155 | lr 5.303756e-04\n",
      "step 39400 | train_loss 1.8672 | val_loss 2.0170 | lr 5.116393e-04\n",
      "step 39600 | train_loss 1.8667 | val_loss 2.0165 | lr 4.932003e-04\n",
      "step 39800 | train_loss 1.8660 | val_loss 2.0181 | lr 4.750614e-04\n",
      "step 40000 | train_loss 1.8632 | val_loss 2.0174 | lr 4.572256e-04\n",
      "step 40200 | train_loss 1.8008 | val_loss 2.0159 | lr 4.396957e-04\n",
      "step 40400 | train_loss 1.8138 | val_loss 2.0149 | lr 4.224744e-04\n",
      "step 40600 | train_loss 1.8204 | val_loss 2.0153 | lr 4.055644e-04\n",
      "step 40800 | train_loss 1.8226 | val_loss 2.0136 | lr 3.889684e-04\n",
      "step 41000 | train_loss 1.8271 | val_loss 2.0138 | lr 3.726890e-04\n",
      "step 41200 | train_loss 1.8252 | val_loss 2.0118 | lr 3.567288e-04\n",
      "step 41400 | train_loss 1.8293 | val_loss 2.0117 | lr 3.410904e-04\n",
      "step 41600 | train_loss 1.8291 | val_loss 2.0119 | lr 3.257761e-04\n",
      "step 41800 | train_loss 1.8349 | val_loss 2.0104 | lr 3.107885e-04\n",
      "step 42000 | train_loss 1.8388 | val_loss 2.0113 | lr 2.961298e-04\n",
      "step 42200 | train_loss 1.8423 | val_loss 2.0110 | lr 2.818024e-04\n",
      "step 42400 | train_loss 1.8414 | val_loss 2.0114 | lr 2.678086e-04\n",
      "step 42600 | train_loss 1.8434 | val_loss 2.0104 | lr 2.541505e-04\n",
      "step 42800 | train_loss 1.8457 | val_loss 2.0114 | lr 2.408303e-04\n",
      "step 43000 | train_loss 1.8477 | val_loss 2.0111 | lr 2.278502e-04\n",
      "step 43200 | train_loss 1.8474 | val_loss 2.0108 | lr 2.152121e-04\n",
      "step 43400 | train_loss 1.8413 | val_loss 2.0107 | lr 2.029181e-04\n",
      "step 43600 | train_loss 1.8142 | val_loss 2.0106 | lr 1.909701e-04\n",
      "step 43800 | train_loss 1.8209 | val_loss 2.0115 | lr 1.793700e-04\n",
      "step 44000 | train_loss 1.8214 | val_loss 2.0099 | lr 1.681196e-04\n",
      "step 44200 | train_loss 1.8210 | val_loss 2.0098 | lr 1.572208e-04\n",
      "step 44400 | train_loss 1.8226 | val_loss 2.0093 | lr 1.466751e-04\n",
      "step 44600 | train_loss 1.8208 | val_loss 2.0089 | lr 1.364843e-04\n",
      "step 44800 | train_loss 1.8223 | val_loss 2.0086 | lr 1.266501e-04\n",
      "step 45000 | train_loss 1.8233 | val_loss 2.0081 | lr 1.171739e-04\n",
      "step 45200 | train_loss 1.8255 | val_loss 2.0069 | lr 1.080572e-04\n",
      "step 45400 | train_loss 1.8267 | val_loss 2.0073 | lr 9.930151e-05\n",
      "step 45600 | train_loss 1.8285 | val_loss 2.0077 | lr 9.090821e-05\n",
      "step 45800 | train_loss 1.8278 | val_loss 2.0080 | lr 8.287859e-05\n",
      "step 46000 | train_loss 1.8285 | val_loss 2.0077 | lr 7.521394e-05\n",
      "step 46200 | train_loss 1.8296 | val_loss 2.0081 | lr 6.791546e-05\n",
      "step 46400 | train_loss 1.8309 | val_loss 2.0078 | lr 6.098431e-05\n",
      "step 46600 | train_loss 1.8310 | val_loss 2.0077 | lr 5.442158e-05\n",
      "step 46800 | train_loss 1.8246 | val_loss 2.0076 | lr 4.822831e-05\n",
      "step 47000 | train_loss 1.8231 | val_loss 2.0078 | lr 4.240547e-05\n",
      "step 47200 | train_loss 1.8239 | val_loss 2.0081 | lr 3.695398e-05\n",
      "step 47400 | train_loss 1.8242 | val_loss 2.0081 | lr 3.187471e-05\n",
      "step 47600 | train_loss 1.8240 | val_loss 2.0081 | lr 2.716846e-05\n",
      "step 47800 | train_loss 1.8241 | val_loss 2.0080 | lr 2.283596e-05\n",
      "step 48000 | train_loss 1.8239 | val_loss 2.0080 | lr 1.887791e-05\n",
      "step 48200 | train_loss 1.8237 | val_loss 2.0080 | lr 1.529494e-05\n",
      "step 48400 | train_loss 1.8237 | val_loss 2.0079 | lr 1.208759e-05\n",
      "step 48600 | train_loss 1.8238 | val_loss 2.0077 | lr 9.256389e-06\n",
      "step 48800 | train_loss 1.8241 | val_loss 2.0076 | lr 6.801776e-06\n",
      "step 49000 | train_loss 1.8242 | val_loss 2.0077 | lr 4.724140e-06\n",
      "step 49200 | train_loss 1.8242 | val_loss 2.0076 | lr 3.023807e-06\n",
      "step 49400 | train_loss 1.8242 | val_loss 2.0076 | lr 1.701048e-06\n",
      "step 49600 | train_loss 1.8242 | val_loss 2.0076 | lr 7.560712e-07\n",
      "step 49800 | train_loss 1.8243 | val_loss 2.0076 | lr 1.890253e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:17:05,754] Trial 14 finished with value: 2.0076147115656306 and parameters: {'embedding_size': 25, 'hidden_size': 382, 'learning_rate': 0.00478812911907272, 'batch_size': 64, 'context_length': 4}. Best is trial 14 with value: 2.0076147115656306.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8243 | val_loss 2.0076 | lr 4.725694e-12\n",
      "step 0 | train_loss 3.3177 | val_loss 3.3187 | lr 1.165369e-03\n",
      "step 200 | train_loss 2.3780 | val_loss 2.4129 | lr 1.165323e-03\n",
      "step 400 | train_loss 2.3014 | val_loss 2.3313 | lr 1.165185e-03\n",
      "step 600 | train_loss 2.2724 | val_loss 2.2910 | lr 1.164955e-03\n",
      "step 800 | train_loss 2.2503 | val_loss 2.2607 | lr 1.164633e-03\n",
      "step 1000 | train_loss 2.2482 | val_loss 2.2466 | lr 1.164219e-03\n",
      "step 1200 | train_loss 2.2179 | val_loss 2.2328 | lr 1.163713e-03\n",
      "step 1400 | train_loss 2.2083 | val_loss 2.2108 | lr 1.163116e-03\n",
      "step 1600 | train_loss 2.2064 | val_loss 2.2075 | lr 1.162427e-03\n",
      "step 1800 | train_loss 2.1930 | val_loss 2.1971 | lr 1.161646e-03\n",
      "step 2000 | train_loss 2.1866 | val_loss 2.1785 | lr 1.160774e-03\n",
      "step 2200 | train_loss 2.1790 | val_loss 2.1820 | lr 1.159811e-03\n",
      "step 2400 | train_loss 2.1789 | val_loss 2.1722 | lr 1.158756e-03\n",
      "step 2600 | train_loss 2.1648 | val_loss 2.1667 | lr 1.157611e-03\n",
      "step 2800 | train_loss 2.1616 | val_loss 2.1616 | lr 1.156374e-03\n",
      "step 3000 | train_loss 2.1598 | val_loss 2.1475 | lr 1.155048e-03\n",
      "step 3200 | train_loss 2.1601 | val_loss 2.1368 | lr 1.153630e-03\n",
      "step 3400 | train_loss 2.0164 | val_loss 2.1402 | lr 1.152123e-03\n",
      "step 3600 | train_loss 2.0875 | val_loss 2.1415 | lr 1.150526e-03\n",
      "step 3800 | train_loss 2.0852 | val_loss 2.1298 | lr 1.148839e-03\n",
      "step 4000 | train_loss 2.0960 | val_loss 2.1303 | lr 1.147062e-03\n",
      "step 4200 | train_loss 2.1065 | val_loss 2.1374 | lr 1.145197e-03\n",
      "step 4400 | train_loss 2.0892 | val_loss 2.1205 | lr 1.143243e-03\n",
      "step 4600 | train_loss 2.0822 | val_loss 2.1274 | lr 1.141200e-03\n",
      "step 4800 | train_loss 2.0778 | val_loss 2.1187 | lr 1.139069e-03\n",
      "step 5000 | train_loss 2.1003 | val_loss 2.1189 | lr 1.136850e-03\n",
      "step 5200 | train_loss 2.1031 | val_loss 2.1126 | lr 1.134544e-03\n",
      "step 5400 | train_loss 2.1059 | val_loss 2.1035 | lr 1.132150e-03\n",
      "step 5600 | train_loss 2.0877 | val_loss 2.1064 | lr 1.129670e-03\n",
      "step 5800 | train_loss 2.0986 | val_loss 2.1125 | lr 1.127103e-03\n",
      "step 6000 | train_loss 2.1076 | val_loss 2.1074 | lr 1.124450e-03\n",
      "step 6200 | train_loss 2.0861 | val_loss 2.1014 | lr 1.121712e-03\n",
      "step 6400 | train_loss 2.1072 | val_loss 2.1047 | lr 1.118889e-03\n",
      "step 6600 | train_loss 2.0867 | val_loss 2.0923 | lr 1.115981e-03\n",
      "step 6800 | train_loss 1.9518 | val_loss 2.0926 | lr 1.112989e-03\n",
      "step 7000 | train_loss 2.0017 | val_loss 2.0864 | lr 1.109913e-03\n",
      "step 7200 | train_loss 2.0166 | val_loss 2.0880 | lr 1.106754e-03\n",
      "step 7400 | train_loss 2.0385 | val_loss 2.0888 | lr 1.103512e-03\n",
      "step 7600 | train_loss 2.0409 | val_loss 2.0885 | lr 1.100187e-03\n",
      "step 7800 | train_loss 2.0279 | val_loss 2.0866 | lr 1.096782e-03\n",
      "step 8000 | train_loss 2.0287 | val_loss 2.0875 | lr 1.093294e-03\n",
      "step 8200 | train_loss 2.0415 | val_loss 2.0801 | lr 1.089727e-03\n",
      "step 8400 | train_loss 2.0520 | val_loss 2.0756 | lr 1.086079e-03\n",
      "step 8600 | train_loss 2.0315 | val_loss 2.0814 | lr 1.082352e-03\n",
      "step 8800 | train_loss 2.0527 | val_loss 2.0729 | lr 1.078545e-03\n",
      "step 9000 | train_loss 2.0470 | val_loss 2.0758 | lr 1.074661e-03\n",
      "step 9200 | train_loss 2.0600 | val_loss 2.0806 | lr 1.070699e-03\n",
      "step 9400 | train_loss 2.0485 | val_loss 2.0778 | lr 1.066659e-03\n",
      "step 9600 | train_loss 2.0567 | val_loss 2.0809 | lr 1.062544e-03\n",
      "step 9800 | train_loss 2.0695 | val_loss 2.0712 | lr 1.058352e-03\n",
      "step 10000 | train_loss 2.0522 | val_loss 2.0644 | lr 1.054086e-03\n",
      "step 10200 | train_loss 1.9329 | val_loss 2.0719 | lr 1.049745e-03\n",
      "step 10400 | train_loss 1.9614 | val_loss 2.0718 | lr 1.045330e-03\n",
      "step 10600 | train_loss 1.9717 | val_loss 2.0613 | lr 1.040842e-03\n",
      "step 10800 | train_loss 1.9924 | val_loss 2.0680 | lr 1.036282e-03\n",
      "step 11000 | train_loss 1.9884 | val_loss 2.0625 | lr 1.031650e-03\n",
      "step 11200 | train_loss 1.9911 | val_loss 2.0710 | lr 1.026948e-03\n",
      "step 11400 | train_loss 1.9884 | val_loss 2.0622 | lr 1.022175e-03\n",
      "step 11600 | train_loss 2.0081 | val_loss 2.0613 | lr 1.017332e-03\n",
      "step 11800 | train_loss 2.0060 | val_loss 2.0586 | lr 1.012422e-03\n",
      "step 12000 | train_loss 2.0083 | val_loss 2.0549 | lr 1.007443e-03\n",
      "step 12200 | train_loss 2.0123 | val_loss 2.0636 | lr 1.002397e-03\n",
      "step 12400 | train_loss 2.0123 | val_loss 2.0574 | lr 9.972850e-04\n",
      "step 12600 | train_loss 2.0164 | val_loss 2.0639 | lr 9.921074e-04\n",
      "step 12800 | train_loss 2.0190 | val_loss 2.0594 | lr 9.868652e-04\n",
      "step 13000 | train_loss 2.0209 | val_loss 2.0575 | lr 9.815591e-04\n",
      "step 13200 | train_loss 2.0270 | val_loss 2.0574 | lr 9.761901e-04\n",
      "step 13400 | train_loss 1.8478 | val_loss 2.0516 | lr 9.707590e-04\n",
      "step 13600 | train_loss 1.9190 | val_loss 2.0562 | lr 9.652665e-04\n",
      "step 13800 | train_loss 1.9326 | val_loss 2.0533 | lr 9.597136e-04\n",
      "step 14000 | train_loss 1.9577 | val_loss 2.0507 | lr 9.541012e-04\n",
      "step 14200 | train_loss 1.9614 | val_loss 2.0561 | lr 9.484302e-04\n",
      "step 14400 | train_loss 1.9654 | val_loss 2.0479 | lr 9.427014e-04\n",
      "step 14600 | train_loss 1.9645 | val_loss 2.0533 | lr 9.369157e-04\n",
      "step 14800 | train_loss 1.9638 | val_loss 2.0440 | lr 9.310741e-04\n",
      "step 15000 | train_loss 1.9773 | val_loss 2.0487 | lr 9.251775e-04\n",
      "step 15200 | train_loss 1.9811 | val_loss 2.0451 | lr 9.192268e-04\n",
      "step 15400 | train_loss 1.9862 | val_loss 2.0436 | lr 9.132230e-04\n",
      "step 15600 | train_loss 1.9814 | val_loss 2.0566 | lr 9.071670e-04\n",
      "step 15800 | train_loss 1.9830 | val_loss 2.0525 | lr 9.010597e-04\n",
      "step 16000 | train_loss 1.9980 | val_loss 2.0533 | lr 8.949022e-04\n",
      "step 16200 | train_loss 1.9867 | val_loss 2.0520 | lr 8.886953e-04\n",
      "step 16400 | train_loss 1.9966 | val_loss 2.0446 | lr 8.824401e-04\n",
      "step 16600 | train_loss 2.0081 | val_loss 2.0463 | lr 8.761376e-04\n",
      "step 16800 | train_loss 1.8545 | val_loss 2.0428 | lr 8.697888e-04\n",
      "step 17000 | train_loss 1.9026 | val_loss 2.0470 | lr 8.633946e-04\n",
      "step 17200 | train_loss 1.9067 | val_loss 2.0413 | lr 8.569561e-04\n",
      "step 17400 | train_loss 1.9275 | val_loss 2.0458 | lr 8.504743e-04\n",
      "step 17600 | train_loss 1.9440 | val_loss 2.0464 | lr 8.439502e-04\n",
      "step 17800 | train_loss 1.9343 | val_loss 2.0410 | lr 8.373848e-04\n",
      "step 18000 | train_loss 1.9443 | val_loss 2.0398 | lr 8.307792e-04\n",
      "step 18200 | train_loss 1.9462 | val_loss 2.0389 | lr 8.241344e-04\n",
      "step 18400 | train_loss 1.9645 | val_loss 2.0399 | lr 8.174516e-04\n",
      "step 18600 | train_loss 1.9548 | val_loss 2.0415 | lr 8.107316e-04\n",
      "step 18800 | train_loss 1.9649 | val_loss 2.0410 | lr 8.039756e-04\n",
      "step 19000 | train_loss 1.9649 | val_loss 2.0414 | lr 7.971847e-04\n",
      "step 19200 | train_loss 1.9706 | val_loss 2.0437 | lr 7.903599e-04\n",
      "step 19400 | train_loss 1.9733 | val_loss 2.0425 | lr 7.835023e-04\n",
      "step 19600 | train_loss 1.9742 | val_loss 2.0450 | lr 7.766130e-04\n",
      "step 19800 | train_loss 1.9876 | val_loss 2.0416 | lr 7.696931e-04\n",
      "step 20000 | train_loss 1.9731 | val_loss 2.0318 | lr 7.627436e-04\n",
      "step 20200 | train_loss 1.8558 | val_loss 2.0369 | lr 7.557658e-04\n",
      "step 20400 | train_loss 1.8913 | val_loss 2.0349 | lr 7.487605e-04\n",
      "step 20600 | train_loss 1.9001 | val_loss 2.0351 | lr 7.417291e-04\n",
      "step 20800 | train_loss 1.9164 | val_loss 2.0344 | lr 7.346725e-04\n",
      "step 21000 | train_loss 1.9180 | val_loss 2.0348 | lr 7.275920e-04\n",
      "step 21200 | train_loss 1.9209 | val_loss 2.0350 | lr 7.204885e-04\n",
      "step 21400 | train_loss 1.9197 | val_loss 2.0332 | lr 7.133633e-04\n",
      "step 21600 | train_loss 1.9307 | val_loss 2.0300 | lr 7.062175e-04\n",
      "step 21800 | train_loss 1.9407 | val_loss 2.0320 | lr 6.990522e-04\n",
      "step 22000 | train_loss 1.9290 | val_loss 2.0330 | lr 6.918684e-04\n",
      "step 22200 | train_loss 1.9458 | val_loss 2.0360 | lr 6.846675e-04\n",
      "step 22400 | train_loss 1.9374 | val_loss 2.0368 | lr 6.774504e-04\n",
      "step 22600 | train_loss 1.9569 | val_loss 2.0379 | lr 6.702184e-04\n",
      "step 22800 | train_loss 1.9486 | val_loss 2.0368 | lr 6.629725e-04\n",
      "step 23000 | train_loss 1.9591 | val_loss 2.0371 | lr 6.557140e-04\n",
      "step 23200 | train_loss 1.9665 | val_loss 2.0351 | lr 6.484439e-04\n",
      "step 23400 | train_loss 1.8898 | val_loss 2.0279 | lr 6.411635e-04\n",
      "step 23600 | train_loss 1.8640 | val_loss 2.0336 | lr 6.338738e-04\n",
      "step 23800 | train_loss 1.8774 | val_loss 2.0310 | lr 6.265760e-04\n",
      "step 24000 | train_loss 1.8906 | val_loss 2.0323 | lr 6.192713e-04\n",
      "step 24200 | train_loss 1.9006 | val_loss 2.0300 | lr 6.119609e-04\n",
      "step 24400 | train_loss 1.9102 | val_loss 2.0290 | lr 6.046458e-04\n",
      "step 24600 | train_loss 1.9098 | val_loss 2.0333 | lr 5.973272e-04\n",
      "step 24800 | train_loss 1.9072 | val_loss 2.0245 | lr 5.900063e-04\n",
      "step 25000 | train_loss 1.9113 | val_loss 2.0263 | lr 5.826843e-04\n",
      "step 25200 | train_loss 1.9157 | val_loss 2.0264 | lr 5.753623e-04\n",
      "step 25400 | train_loss 1.9212 | val_loss 2.0264 | lr 5.680414e-04\n",
      "step 25600 | train_loss 1.9208 | val_loss 2.0323 | lr 5.607228e-04\n",
      "step 25800 | train_loss 1.9261 | val_loss 2.0337 | lr 5.534077e-04\n",
      "step 26000 | train_loss 1.9320 | val_loss 2.0320 | lr 5.460972e-04\n",
      "step 26200 | train_loss 1.9303 | val_loss 2.0316 | lr 5.387926e-04\n",
      "step 26400 | train_loss 1.9337 | val_loss 2.0268 | lr 5.314948e-04\n",
      "step 26600 | train_loss 1.9429 | val_loss 2.0263 | lr 5.242051e-04\n",
      "step 26800 | train_loss 1.8305 | val_loss 2.0249 | lr 5.169247e-04\n",
      "step 27000 | train_loss 1.8640 | val_loss 2.0258 | lr 5.096546e-04\n",
      "step 27200 | train_loss 1.8686 | val_loss 2.0240 | lr 5.023961e-04\n",
      "step 27400 | train_loss 1.8816 | val_loss 2.0246 | lr 4.951502e-04\n",
      "step 27600 | train_loss 1.8898 | val_loss 2.0278 | lr 4.879182e-04\n",
      "step 27800 | train_loss 1.8850 | val_loss 2.0226 | lr 4.807011e-04\n",
      "step 28000 | train_loss 1.8917 | val_loss 2.0239 | lr 4.735001e-04\n",
      "step 28200 | train_loss 1.8907 | val_loss 2.0220 | lr 4.663164e-04\n",
      "step 28400 | train_loss 1.9030 | val_loss 2.0234 | lr 4.591511e-04\n",
      "step 28600 | train_loss 1.9049 | val_loss 2.0203 | lr 4.520052e-04\n",
      "step 28800 | train_loss 1.9087 | val_loss 2.0214 | lr 4.448800e-04\n",
      "step 29000 | train_loss 1.9037 | val_loss 2.0240 | lr 4.377766e-04\n",
      "step 29200 | train_loss 1.9090 | val_loss 2.0251 | lr 4.306960e-04\n",
      "step 29400 | train_loss 1.9142 | val_loss 2.0266 | lr 4.236395e-04\n",
      "step 29600 | train_loss 1.9132 | val_loss 2.0266 | lr 4.166080e-04\n",
      "step 29800 | train_loss 1.9231 | val_loss 2.0251 | lr 4.096028e-04\n",
      "step 30000 | train_loss 1.9163 | val_loss 2.0198 | lr 4.026249e-04\n",
      "step 30200 | train_loss 1.8376 | val_loss 2.0207 | lr 3.956755e-04\n",
      "step 30400 | train_loss 1.8605 | val_loss 2.0178 | lr 3.887556e-04\n",
      "step 30600 | train_loss 1.8641 | val_loss 2.0190 | lr 3.818663e-04\n",
      "step 30800 | train_loss 1.8732 | val_loss 2.0204 | lr 3.750087e-04\n",
      "step 31000 | train_loss 1.8777 | val_loss 2.0208 | lr 3.681839e-04\n",
      "step 31200 | train_loss 1.8786 | val_loss 2.0212 | lr 3.613930e-04\n",
      "step 31400 | train_loss 1.8824 | val_loss 2.0212 | lr 3.546370e-04\n",
      "step 31600 | train_loss 1.8817 | val_loss 2.0185 | lr 3.479170e-04\n",
      "step 31800 | train_loss 1.8907 | val_loss 2.0164 | lr 3.412342e-04\n",
      "step 32000 | train_loss 1.8879 | val_loss 2.0185 | lr 3.345894e-04\n",
      "step 32200 | train_loss 1.8980 | val_loss 2.0177 | lr 3.279838e-04\n",
      "step 32400 | train_loss 1.8897 | val_loss 2.0201 | lr 3.214184e-04\n",
      "step 32600 | train_loss 1.8966 | val_loss 2.0202 | lr 3.148943e-04\n",
      "step 32800 | train_loss 1.8979 | val_loss 2.0215 | lr 3.084125e-04\n",
      "step 33000 | train_loss 1.9030 | val_loss 2.0231 | lr 3.019740e-04\n",
      "step 33200 | train_loss 1.9091 | val_loss 2.0208 | lr 2.955798e-04\n",
      "step 33400 | train_loss 1.9060 | val_loss 2.0152 | lr 2.892310e-04\n",
      "step 33600 | train_loss 1.8471 | val_loss 2.0169 | lr 2.829285e-04\n",
      "step 33800 | train_loss 1.8598 | val_loss 2.0176 | lr 2.766733e-04\n",
      "step 34000 | train_loss 1.8612 | val_loss 2.0164 | lr 2.704664e-04\n",
      "step 34200 | train_loss 1.8673 | val_loss 2.0162 | lr 2.643089e-04\n",
      "step 34400 | train_loss 1.8701 | val_loss 2.0160 | lr 2.582016e-04\n",
      "step 34600 | train_loss 1.8712 | val_loss 2.0177 | lr 2.521456e-04\n",
      "step 34800 | train_loss 1.8699 | val_loss 2.0154 | lr 2.461418e-04\n",
      "step 35000 | train_loss 1.8739 | val_loss 2.0149 | lr 2.401911e-04\n",
      "step 35200 | train_loss 1.8759 | val_loss 2.0135 | lr 2.342945e-04\n",
      "step 35400 | train_loss 1.8774 | val_loss 2.0130 | lr 2.284529e-04\n",
      "step 35600 | train_loss 1.8814 | val_loss 2.0158 | lr 2.226672e-04\n",
      "step 35800 | train_loss 1.8813 | val_loss 2.0166 | lr 2.169384e-04\n",
      "step 36000 | train_loss 1.8826 | val_loss 2.0167 | lr 2.112673e-04\n",
      "step 36200 | train_loss 1.8850 | val_loss 2.0164 | lr 2.056549e-04\n",
      "step 36400 | train_loss 1.8869 | val_loss 2.0163 | lr 2.001021e-04\n",
      "step 36600 | train_loss 1.8924 | val_loss 2.0152 | lr 1.946096e-04\n",
      "step 36800 | train_loss 1.8446 | val_loss 2.0138 | lr 1.891785e-04\n",
      "step 37000 | train_loss 1.8531 | val_loss 2.0143 | lr 1.838094e-04\n",
      "step 37200 | train_loss 1.8582 | val_loss 2.0139 | lr 1.785034e-04\n",
      "step 37400 | train_loss 1.8613 | val_loss 2.0139 | lr 1.732612e-04\n",
      "step 37600 | train_loss 1.8632 | val_loss 2.0146 | lr 1.680836e-04\n",
      "step 37800 | train_loss 1.8662 | val_loss 2.0135 | lr 1.629715e-04\n",
      "step 38000 | train_loss 1.8648 | val_loss 2.0141 | lr 1.579257e-04\n",
      "step 38200 | train_loss 1.8635 | val_loss 2.0131 | lr 1.529470e-04\n",
      "step 38400 | train_loss 1.8664 | val_loss 2.0121 | lr 1.480361e-04\n",
      "step 38600 | train_loss 1.8697 | val_loss 2.0100 | lr 1.431939e-04\n",
      "step 38800 | train_loss 1.8714 | val_loss 2.0104 | lr 1.384210e-04\n",
      "step 39000 | train_loss 1.8711 | val_loss 2.0133 | lr 1.337183e-04\n",
      "step 39200 | train_loss 1.8712 | val_loss 2.0129 | lr 1.290865e-04\n",
      "step 39400 | train_loss 1.8737 | val_loss 2.0137 | lr 1.245264e-04\n",
      "step 39600 | train_loss 1.8736 | val_loss 2.0138 | lr 1.200386e-04\n",
      "step 39800 | train_loss 1.8768 | val_loss 2.0131 | lr 1.156238e-04\n",
      "step 40000 | train_loss 1.8813 | val_loss 2.0130 | lr 1.112828e-04\n",
      "step 40200 | train_loss 1.8513 | val_loss 2.0120 | lr 1.070162e-04\n",
      "step 40400 | train_loss 1.8554 | val_loss 2.0113 | lr 1.028248e-04\n",
      "step 40600 | train_loss 1.8576 | val_loss 2.0115 | lr 9.870911e-05\n",
      "step 40800 | train_loss 1.8597 | val_loss 2.0120 | lr 9.466986e-05\n",
      "step 41000 | train_loss 1.8611 | val_loss 2.0123 | lr 9.070767e-05\n",
      "step 41200 | train_loss 1.8616 | val_loss 2.0119 | lr 8.682318e-05\n",
      "step 41400 | train_loss 1.8634 | val_loss 2.0120 | lr 8.301698e-05\n",
      "step 41600 | train_loss 1.8612 | val_loss 2.0113 | lr 7.928969e-05\n",
      "step 41800 | train_loss 1.8634 | val_loss 2.0104 | lr 7.564189e-05\n",
      "step 42000 | train_loss 1.8636 | val_loss 2.0101 | lr 7.207416e-05\n",
      "step 42200 | train_loss 1.8659 | val_loss 2.0098 | lr 6.858705e-05\n",
      "step 42400 | train_loss 1.8654 | val_loss 2.0108 | lr 6.518113e-05\n",
      "step 42600 | train_loss 1.8651 | val_loss 2.0108 | lr 6.185694e-05\n",
      "step 42800 | train_loss 1.8660 | val_loss 2.0113 | lr 5.861498e-05\n",
      "step 43000 | train_loss 1.8669 | val_loss 2.0117 | lr 5.545578e-05\n",
      "step 43200 | train_loss 1.8696 | val_loss 2.0114 | lr 5.237984e-05\n",
      "step 43400 | train_loss 1.8705 | val_loss 2.0109 | lr 4.938764e-05\n",
      "step 43600 | train_loss 1.8570 | val_loss 2.0109 | lr 4.647965e-05\n",
      "step 43800 | train_loss 1.8589 | val_loss 2.0107 | lr 4.365634e-05\n",
      "step 44000 | train_loss 1.8592 | val_loss 2.0105 | lr 4.091814e-05\n",
      "step 44200 | train_loss 1.8599 | val_loss 2.0107 | lr 3.826549e-05\n",
      "step 44400 | train_loss 1.8602 | val_loss 2.0107 | lr 3.569882e-05\n",
      "step 44600 | train_loss 1.8608 | val_loss 2.0108 | lr 3.321852e-05\n",
      "step 44800 | train_loss 1.8611 | val_loss 2.0109 | lr 3.082499e-05\n",
      "step 45000 | train_loss 1.8605 | val_loss 2.0105 | lr 2.851860e-05\n",
      "step 45200 | train_loss 1.8609 | val_loss 2.0099 | lr 2.629972e-05\n",
      "step 45400 | train_loss 1.8610 | val_loss 2.0098 | lr 2.416870e-05\n",
      "step 45600 | train_loss 1.8620 | val_loss 2.0098 | lr 2.212588e-05\n",
      "step 45800 | train_loss 1.8616 | val_loss 2.0101 | lr 2.017158e-05\n",
      "step 46000 | train_loss 1.8618 | val_loss 2.0101 | lr 1.830610e-05\n",
      "step 46200 | train_loss 1.8620 | val_loss 2.0102 | lr 1.652974e-05\n",
      "step 46400 | train_loss 1.8624 | val_loss 2.0103 | lr 1.484279e-05\n",
      "step 46600 | train_loss 1.8632 | val_loss 2.0103 | lr 1.324551e-05\n",
      "step 46800 | train_loss 1.8609 | val_loss 2.0102 | lr 1.173814e-05\n",
      "step 47000 | train_loss 1.8604 | val_loss 2.0102 | lr 1.032094e-05\n",
      "step 47200 | train_loss 1.8606 | val_loss 2.0102 | lr 8.994120e-06\n",
      "step 47400 | train_loss 1.8608 | val_loss 2.0102 | lr 7.757891e-06\n",
      "step 47600 | train_loss 1.8608 | val_loss 2.0102 | lr 6.612450e-06\n",
      "step 47800 | train_loss 1.8610 | val_loss 2.0102 | lr 5.557977e-06\n",
      "step 48000 | train_loss 1.8611 | val_loss 2.0103 | lr 4.594640e-06\n",
      "step 48200 | train_loss 1.8611 | val_loss 2.0102 | lr 3.722589e-06\n",
      "step 48400 | train_loss 1.8610 | val_loss 2.0102 | lr 2.941963e-06\n",
      "step 48600 | train_loss 1.8610 | val_loss 2.0101 | lr 2.252885e-06\n",
      "step 48800 | train_loss 1.8610 | val_loss 2.0101 | lr 1.655464e-06\n",
      "step 49000 | train_loss 1.8610 | val_loss 2.0101 | lr 1.149794e-06\n",
      "step 49200 | train_loss 1.8610 | val_loss 2.0101 | lr 7.359556e-07\n",
      "step 49400 | train_loss 1.8610 | val_loss 2.0101 | lr 4.140131e-07\n",
      "step 49600 | train_loss 1.8610 | val_loss 2.0101 | lr 1.840179e-07\n",
      "step 49800 | train_loss 1.8610 | val_loss 2.0101 | lr 4.600630e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:18:09,515] Trial 15 finished with value: 2.0101490222982 and parameters: {'embedding_size': 25, 'hidden_size': 394, 'learning_rate': 0.0011653685943772332, 'batch_size': 64, 'context_length': 4}. Best is trial 14 with value: 2.0076147115656306.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8610 | val_loss 2.0101 | lr 1.150173e-12\n",
      "step 0 | train_loss 3.3083 | val_loss 3.3139 | lr 1.311942e-03\n",
      "step 200 | train_loss 2.3511 | val_loss 2.4263 | lr 1.311890e-03\n",
      "step 400 | train_loss 2.3095 | val_loss 2.3589 | lr 1.311735e-03\n",
      "step 600 | train_loss 2.3005 | val_loss 2.3356 | lr 1.311476e-03\n",
      "step 800 | train_loss 2.2704 | val_loss 2.3042 | lr 1.311114e-03\n",
      "step 1000 | train_loss 2.2630 | val_loss 2.2761 | lr 1.310648e-03\n",
      "step 1200 | train_loss 2.2461 | val_loss 2.2759 | lr 1.310078e-03\n",
      "step 1400 | train_loss 2.2656 | val_loss 2.2679 | lr 1.309406e-03\n",
      "step 1600 | train_loss 2.2526 | val_loss 2.2428 | lr 1.308630e-03\n",
      "step 1800 | train_loss 2.2504 | val_loss 2.2438 | lr 1.307751e-03\n",
      "step 2000 | train_loss 2.2443 | val_loss 2.2361 | lr 1.306770e-03\n",
      "step 2200 | train_loss 2.2479 | val_loss 2.2295 | lr 1.305685e-03\n",
      "step 2400 | train_loss 2.1858 | val_loss 2.2214 | lr 1.304498e-03\n",
      "step 2600 | train_loss 2.2339 | val_loss 2.2074 | lr 1.303208e-03\n",
      "step 2800 | train_loss 2.2200 | val_loss 2.2041 | lr 1.301817e-03\n",
      "step 3000 | train_loss 2.2094 | val_loss 2.2006 | lr 1.300323e-03\n",
      "step 3200 | train_loss 2.1970 | val_loss 2.1954 | lr 1.298728e-03\n",
      "step 3400 | train_loss 2.1913 | val_loss 2.1863 | lr 1.297031e-03\n",
      "step 3600 | train_loss 2.2070 | val_loss 2.1870 | lr 1.295232e-03\n",
      "step 3800 | train_loss 2.1589 | val_loss 2.1941 | lr 1.293333e-03\n",
      "step 4000 | train_loss 2.1763 | val_loss 2.1761 | lr 1.291334e-03\n",
      "step 4200 | train_loss 2.1821 | val_loss 2.1710 | lr 1.289233e-03\n",
      "step 4400 | train_loss 2.1808 | val_loss 2.1734 | lr 1.287033e-03\n",
      "step 4600 | train_loss 2.1750 | val_loss 2.1547 | lr 1.284734e-03\n",
      "step 4800 | train_loss 2.1873 | val_loss 2.1619 | lr 1.282335e-03\n",
      "step 5000 | train_loss 2.1826 | val_loss 2.1580 | lr 1.279837e-03\n",
      "step 5200 | train_loss 2.1506 | val_loss 2.1573 | lr 1.277240e-03\n",
      "step 5400 | train_loss 2.1837 | val_loss 2.1522 | lr 1.274546e-03\n",
      "step 5600 | train_loss 2.1555 | val_loss 2.1443 | lr 1.271753e-03\n",
      "step 5800 | train_loss 2.1658 | val_loss 2.1517 | lr 1.268864e-03\n",
      "step 6000 | train_loss 2.1640 | val_loss 2.1350 | lr 1.265878e-03\n",
      "step 6200 | train_loss 2.1728 | val_loss 2.1342 | lr 1.262795e-03\n",
      "step 6400 | train_loss 2.1447 | val_loss 2.1240 | lr 1.259617e-03\n",
      "step 6600 | train_loss 2.1464 | val_loss 2.1188 | lr 1.256343e-03\n",
      "step 6800 | train_loss 1.9392 | val_loss 2.1285 | lr 1.252974e-03\n",
      "step 7000 | train_loss 2.0166 | val_loss 2.1305 | lr 1.249511e-03\n",
      "step 7200 | train_loss 2.0693 | val_loss 2.1304 | lr 1.245955e-03\n",
      "step 7400 | train_loss 2.0778 | val_loss 2.1299 | lr 1.242305e-03\n",
      "step 7600 | train_loss 2.0774 | val_loss 2.1129 | lr 1.238563e-03\n",
      "step 7800 | train_loss 2.0554 | val_loss 2.1138 | lr 1.234729e-03\n",
      "step 8000 | train_loss 2.0634 | val_loss 2.1112 | lr 1.230803e-03\n",
      "step 8200 | train_loss 2.0822 | val_loss 2.1106 | lr 1.226786e-03\n",
      "step 8400 | train_loss 2.0945 | val_loss 2.1265 | lr 1.222680e-03\n",
      "step 8600 | train_loss 2.0852 | val_loss 2.0985 | lr 1.218484e-03\n",
      "step 8800 | train_loss 2.0806 | val_loss 2.1057 | lr 1.214199e-03\n",
      "step 9000 | train_loss 2.0807 | val_loss 2.1094 | lr 1.209826e-03\n",
      "step 9200 | train_loss 2.0753 | val_loss 2.1110 | lr 1.205365e-03\n",
      "step 9400 | train_loss 2.0995 | val_loss 2.1046 | lr 1.200818e-03\n",
      "step 9600 | train_loss 2.0872 | val_loss 2.1001 | lr 1.196185e-03\n",
      "step 9800 | train_loss 2.1034 | val_loss 2.0978 | lr 1.191466e-03\n",
      "step 10000 | train_loss 2.0954 | val_loss 2.1062 | lr 1.186663e-03\n",
      "step 10200 | train_loss 2.0989 | val_loss 2.0988 | lr 1.181776e-03\n",
      "step 10400 | train_loss 2.0938 | val_loss 2.0999 | lr 1.176806e-03\n",
      "step 10600 | train_loss 2.0587 | val_loss 2.0892 | lr 1.171753e-03\n",
      "step 10800 | train_loss 2.0814 | val_loss 2.0913 | lr 1.166620e-03\n",
      "step 11000 | train_loss 2.0689 | val_loss 2.0957 | lr 1.161405e-03\n",
      "step 11200 | train_loss 2.0717 | val_loss 2.0925 | lr 1.156111e-03\n",
      "step 11400 | train_loss 2.1009 | val_loss 2.0915 | lr 1.150738e-03\n",
      "step 11600 | train_loss 2.0875 | val_loss 2.0921 | lr 1.145287e-03\n",
      "step 11800 | train_loss 2.0931 | val_loss 2.0843 | lr 1.139758e-03\n",
      "step 12000 | train_loss 2.0949 | val_loss 2.0912 | lr 1.134153e-03\n",
      "step 12200 | train_loss 2.0746 | val_loss 2.0834 | lr 1.128473e-03\n",
      "step 12400 | train_loss 2.0855 | val_loss 2.0864 | lr 1.122718e-03\n",
      "step 12600 | train_loss 2.0742 | val_loss 2.0771 | lr 1.116889e-03\n",
      "step 12800 | train_loss 2.1163 | val_loss 2.0837 | lr 1.110988e-03\n",
      "step 13000 | train_loss 2.0932 | val_loss 2.0811 | lr 1.105014e-03\n",
      "step 13200 | train_loss 2.0695 | val_loss 2.0759 | lr 1.098970e-03\n",
      "step 13400 | train_loss 1.8227 | val_loss 2.0739 | lr 1.092856e-03\n",
      "step 13600 | train_loss 1.8992 | val_loss 2.0739 | lr 1.086672e-03\n",
      "step 13800 | train_loss 1.9496 | val_loss 2.0798 | lr 1.080421e-03\n",
      "step 14000 | train_loss 1.9629 | val_loss 2.0586 | lr 1.074103e-03\n",
      "step 14200 | train_loss 1.9825 | val_loss 2.0702 | lr 1.067718e-03\n",
      "step 14400 | train_loss 2.0021 | val_loss 2.0690 | lr 1.061269e-03\n",
      "step 14600 | train_loss 1.9782 | val_loss 2.0699 | lr 1.054756e-03\n",
      "step 14800 | train_loss 2.0107 | val_loss 2.0618 | lr 1.048179e-03\n",
      "step 15000 | train_loss 2.0177 | val_loss 2.0681 | lr 1.041541e-03\n",
      "step 15200 | train_loss 2.0247 | val_loss 2.0620 | lr 1.034842e-03\n",
      "step 15400 | train_loss 2.0221 | val_loss 2.0560 | lr 1.028083e-03\n",
      "step 15600 | train_loss 2.0143 | val_loss 2.0633 | lr 1.021265e-03\n",
      "step 15800 | train_loss 2.0119 | val_loss 2.0656 | lr 1.014390e-03\n",
      "step 16000 | train_loss 2.0317 | val_loss 2.0643 | lr 1.007458e-03\n",
      "step 16200 | train_loss 2.0241 | val_loss 2.0563 | lr 1.000470e-03\n",
      "step 16400 | train_loss 2.0317 | val_loss 2.0559 | lr 9.934285e-04\n",
      "step 16600 | train_loss 2.0275 | val_loss 2.0619 | lr 9.863333e-04\n",
      "step 16800 | train_loss 2.0497 | val_loss 2.0538 | lr 9.791859e-04\n",
      "step 17000 | train_loss 2.0340 | val_loss 2.0605 | lr 9.719875e-04\n",
      "step 17200 | train_loss 1.9953 | val_loss 2.0571 | lr 9.647392e-04\n",
      "step 17400 | train_loss 2.0136 | val_loss 2.0549 | lr 9.574421e-04\n",
      "step 17600 | train_loss 2.0160 | val_loss 2.0517 | lr 9.500975e-04\n",
      "step 17800 | train_loss 2.0155 | val_loss 2.0646 | lr 9.427063e-04\n",
      "step 18000 | train_loss 2.0283 | val_loss 2.0586 | lr 9.352699e-04\n",
      "step 18200 | train_loss 2.0362 | val_loss 2.0589 | lr 9.277894e-04\n",
      "step 18400 | train_loss 2.0364 | val_loss 2.0560 | lr 9.202660e-04\n",
      "step 18600 | train_loss 2.0316 | val_loss 2.0554 | lr 9.127008e-04\n",
      "step 18800 | train_loss 2.0235 | val_loss 2.0566 | lr 9.050951e-04\n",
      "step 19000 | train_loss 2.0287 | val_loss 2.0563 | lr 8.974501e-04\n",
      "step 19200 | train_loss 2.0240 | val_loss 2.0574 | lr 8.897669e-04\n",
      "step 19400 | train_loss 2.0246 | val_loss 2.0475 | lr 8.820468e-04\n",
      "step 19600 | train_loss 2.0311 | val_loss 2.0464 | lr 8.742910e-04\n",
      "step 19800 | train_loss 2.0195 | val_loss 2.0479 | lr 8.665008e-04\n",
      "step 20000 | train_loss 2.0254 | val_loss 2.0425 | lr 8.586773e-04\n",
      "step 20200 | train_loss 1.8450 | val_loss 2.0475 | lr 8.508217e-04\n",
      "step 20400 | train_loss 1.8808 | val_loss 2.0454 | lr 8.429354e-04\n",
      "step 20600 | train_loss 1.9112 | val_loss 2.0420 | lr 8.350196e-04\n",
      "step 20800 | train_loss 1.9267 | val_loss 2.0458 | lr 8.270755e-04\n",
      "step 21000 | train_loss 1.9244 | val_loss 2.0354 | lr 8.191044e-04\n",
      "step 21200 | train_loss 1.9161 | val_loss 2.0354 | lr 8.111075e-04\n",
      "step 21400 | train_loss 1.9294 | val_loss 2.0407 | lr 8.030862e-04\n",
      "step 21600 | train_loss 1.9571 | val_loss 2.0356 | lr 7.950416e-04\n",
      "step 21800 | train_loss 1.9623 | val_loss 2.0428 | lr 7.869750e-04\n",
      "step 22000 | train_loss 1.9639 | val_loss 2.0329 | lr 7.788878e-04\n",
      "step 22200 | train_loss 1.9541 | val_loss 2.0336 | lr 7.707811e-04\n",
      "step 22400 | train_loss 1.9661 | val_loss 2.0451 | lr 7.626563e-04\n",
      "step 22600 | train_loss 1.9532 | val_loss 2.0318 | lr 7.545147e-04\n",
      "step 22800 | train_loss 1.9654 | val_loss 2.0369 | lr 7.463575e-04\n",
      "step 23000 | train_loss 1.9770 | val_loss 2.0339 | lr 7.381860e-04\n",
      "step 23200 | train_loss 1.9847 | val_loss 2.0366 | lr 7.300016e-04\n",
      "step 23400 | train_loss 1.9842 | val_loss 2.0367 | lr 7.218054e-04\n",
      "step 23600 | train_loss 1.9829 | val_loss 2.0335 | lr 7.135989e-04\n",
      "step 23800 | train_loss 1.9712 | val_loss 2.0346 | lr 7.053833e-04\n",
      "step 24000 | train_loss 1.9620 | val_loss 2.0330 | lr 6.971598e-04\n",
      "step 24200 | train_loss 1.9668 | val_loss 2.0307 | lr 6.889299e-04\n",
      "step 24400 | train_loss 1.9634 | val_loss 2.0383 | lr 6.806947e-04\n",
      "step 24600 | train_loss 1.9606 | val_loss 2.0312 | lr 6.724557e-04\n",
      "step 24800 | train_loss 1.9757 | val_loss 2.0317 | lr 6.642140e-04\n",
      "step 25000 | train_loss 1.9729 | val_loss 2.0327 | lr 6.559710e-04\n",
      "step 25200 | train_loss 1.9912 | val_loss 2.0336 | lr 6.477281e-04\n",
      "step 25400 | train_loss 1.9810 | val_loss 2.0336 | lr 6.394864e-04\n",
      "step 25600 | train_loss 1.9661 | val_loss 2.0276 | lr 6.312474e-04\n",
      "step 25800 | train_loss 1.9792 | val_loss 2.0350 | lr 6.230122e-04\n",
      "step 26000 | train_loss 1.9815 | val_loss 2.0275 | lr 6.147823e-04\n",
      "step 26200 | train_loss 1.9939 | val_loss 2.0303 | lr 6.065588e-04\n",
      "step 26400 | train_loss 1.9839 | val_loss 2.0278 | lr 5.983432e-04\n",
      "step 26600 | train_loss 1.9600 | val_loss 2.0214 | lr 5.901367e-04\n",
      "step 26800 | train_loss 1.8072 | val_loss 2.0220 | lr 5.819405e-04\n",
      "step 27000 | train_loss 1.8456 | val_loss 2.0242 | lr 5.737561e-04\n",
      "step 27200 | train_loss 1.8723 | val_loss 2.0230 | lr 5.655846e-04\n",
      "step 27400 | train_loss 1.8760 | val_loss 2.0163 | lr 5.574274e-04\n",
      "step 27600 | train_loss 1.8838 | val_loss 2.0177 | lr 5.492858e-04\n",
      "step 27800 | train_loss 1.8912 | val_loss 2.0160 | lr 5.411610e-04\n",
      "step 28000 | train_loss 1.8966 | val_loss 2.0164 | lr 5.330543e-04\n",
      "step 28200 | train_loss 1.9111 | val_loss 2.0139 | lr 5.249671e-04\n",
      "step 28400 | train_loss 1.9187 | val_loss 2.0186 | lr 5.169005e-04\n",
      "step 28600 | train_loss 1.9192 | val_loss 2.0166 | lr 5.088559e-04\n",
      "step 28800 | train_loss 1.9195 | val_loss 2.0111 | lr 5.008346e-04\n",
      "step 29000 | train_loss 1.9191 | val_loss 2.0175 | lr 4.928377e-04\n",
      "step 29200 | train_loss 1.9136 | val_loss 2.0177 | lr 4.848666e-04\n",
      "step 29400 | train_loss 1.9269 | val_loss 2.0171 | lr 4.769225e-04\n",
      "step 29600 | train_loss 1.9196 | val_loss 2.0132 | lr 4.690067e-04\n",
      "step 29800 | train_loss 1.9346 | val_loss 2.0145 | lr 4.611204e-04\n",
      "step 30000 | train_loss 1.9309 | val_loss 2.0135 | lr 4.532648e-04\n",
      "step 30200 | train_loss 1.9384 | val_loss 2.0108 | lr 4.454413e-04\n",
      "step 30400 | train_loss 1.9404 | val_loss 2.0116 | lr 4.376511e-04\n",
      "step 30600 | train_loss 1.9230 | val_loss 2.0149 | lr 4.298953e-04\n",
      "step 30800 | train_loss 1.9290 | val_loss 2.0102 | lr 4.221752e-04\n",
      "step 31000 | train_loss 1.9343 | val_loss 2.0118 | lr 4.144920e-04\n",
      "step 31200 | train_loss 1.9278 | val_loss 2.0202 | lr 4.068470e-04\n",
      "step 31400 | train_loss 1.9227 | val_loss 2.0142 | lr 3.992413e-04\n",
      "step 31600 | train_loss 1.9300 | val_loss 2.0129 | lr 3.916761e-04\n",
      "step 31800 | train_loss 1.9337 | val_loss 2.0115 | lr 3.841527e-04\n",
      "step 32000 | train_loss 1.9326 | val_loss 2.0140 | lr 3.766722e-04\n",
      "step 32200 | train_loss 1.9303 | val_loss 2.0122 | lr 3.692358e-04\n",
      "step 32400 | train_loss 1.9323 | val_loss 2.0128 | lr 3.618446e-04\n",
      "step 32600 | train_loss 1.9315 | val_loss 2.0141 | lr 3.545000e-04\n",
      "step 32800 | train_loss 1.9311 | val_loss 2.0102 | lr 3.472029e-04\n",
      "step 33000 | train_loss 1.9410 | val_loss 2.0098 | lr 3.399546e-04\n",
      "step 33200 | train_loss 1.9332 | val_loss 2.0097 | lr 3.327562e-04\n",
      "step 33400 | train_loss 1.9321 | val_loss 2.0056 | lr 3.256088e-04\n",
      "step 33600 | train_loss 1.8390 | val_loss 2.0071 | lr 3.185136e-04\n",
      "step 33800 | train_loss 1.8494 | val_loss 2.0061 | lr 3.114717e-04\n",
      "step 34000 | train_loss 1.8631 | val_loss 2.0056 | lr 3.044842e-04\n",
      "step 34200 | train_loss 1.8676 | val_loss 2.0055 | lr 2.975522e-04\n",
      "step 34400 | train_loss 1.8730 | val_loss 2.0023 | lr 2.906768e-04\n",
      "step 34600 | train_loss 1.8633 | val_loss 2.0019 | lr 2.838590e-04\n",
      "step 34800 | train_loss 1.8705 | val_loss 2.0029 | lr 2.771001e-04\n",
      "step 35000 | train_loss 1.8824 | val_loss 1.9991 | lr 2.704009e-04\n",
      "step 35200 | train_loss 1.8828 | val_loss 2.0025 | lr 2.637627e-04\n",
      "step 35400 | train_loss 1.8821 | val_loss 2.0001 | lr 2.571864e-04\n",
      "step 35600 | train_loss 1.8826 | val_loss 1.9999 | lr 2.506730e-04\n",
      "step 35800 | train_loss 1.8829 | val_loss 2.0014 | lr 2.442237e-04\n",
      "step 36000 | train_loss 1.8857 | val_loss 1.9990 | lr 2.378394e-04\n",
      "step 36200 | train_loss 1.8903 | val_loss 1.9989 | lr 2.315211e-04\n",
      "step 36400 | train_loss 1.8903 | val_loss 1.9993 | lr 2.252698e-04\n",
      "step 36600 | train_loss 1.8938 | val_loss 1.9985 | lr 2.190865e-04\n",
      "step 36800 | train_loss 1.8927 | val_loss 1.9971 | lr 2.129723e-04\n",
      "step 37000 | train_loss 1.8922 | val_loss 1.9970 | lr 2.069280e-04\n",
      "step 37200 | train_loss 1.8927 | val_loss 1.9989 | lr 2.009546e-04\n",
      "step 37400 | train_loss 1.8914 | val_loss 1.9969 | lr 1.950530e-04\n",
      "step 37600 | train_loss 1.8969 | val_loss 1.9971 | lr 1.892242e-04\n",
      "step 37800 | train_loss 1.8924 | val_loss 1.9996 | lr 1.834692e-04\n",
      "step 38000 | train_loss 1.8877 | val_loss 1.9979 | lr 1.777887e-04\n",
      "step 38200 | train_loss 1.8957 | val_loss 1.9989 | lr 1.721838e-04\n",
      "step 38400 | train_loss 1.8889 | val_loss 1.9976 | lr 1.666553e-04\n",
      "step 38600 | train_loss 1.8933 | val_loss 1.9981 | lr 1.612040e-04\n",
      "step 38800 | train_loss 1.8993 | val_loss 1.9977 | lr 1.558308e-04\n",
      "step 39000 | train_loss 1.8929 | val_loss 1.9966 | lr 1.505367e-04\n",
      "step 39200 | train_loss 1.8989 | val_loss 1.9988 | lr 1.453223e-04\n",
      "step 39400 | train_loss 1.8982 | val_loss 1.9966 | lr 1.401886e-04\n",
      "step 39600 | train_loss 1.9038 | val_loss 1.9975 | lr 1.351363e-04\n",
      "step 39800 | train_loss 1.9000 | val_loss 1.9959 | lr 1.301663e-04\n",
      "step 40000 | train_loss 1.8972 | val_loss 1.9953 | lr 1.252793e-04\n",
      "step 40200 | train_loss 1.8537 | val_loss 1.9954 | lr 1.204761e-04\n",
      "step 40400 | train_loss 1.8581 | val_loss 1.9962 | lr 1.157575e-04\n",
      "step 40600 | train_loss 1.8642 | val_loss 1.9958 | lr 1.111242e-04\n",
      "step 40800 | train_loss 1.8663 | val_loss 1.9948 | lr 1.065769e-04\n",
      "step 41000 | train_loss 1.8673 | val_loss 1.9940 | lr 1.021164e-04\n",
      "step 41200 | train_loss 1.8682 | val_loss 1.9940 | lr 9.774331e-05\n",
      "step 41400 | train_loss 1.8661 | val_loss 1.9937 | lr 9.345839e-05\n",
      "step 41600 | train_loss 1.8677 | val_loss 1.9932 | lr 8.926230e-05\n",
      "step 41800 | train_loss 1.8677 | val_loss 1.9936 | lr 8.515570e-05\n",
      "step 42000 | train_loss 1.8691 | val_loss 1.9927 | lr 8.113924e-05\n",
      "step 42200 | train_loss 1.8695 | val_loss 1.9923 | lr 7.721355e-05\n",
      "step 42400 | train_loss 1.8686 | val_loss 1.9926 | lr 7.337925e-05\n",
      "step 42600 | train_loss 1.8669 | val_loss 1.9926 | lr 6.963695e-05\n",
      "step 42800 | train_loss 1.8703 | val_loss 1.9922 | lr 6.598724e-05\n",
      "step 43000 | train_loss 1.8696 | val_loss 1.9918 | lr 6.243070e-05\n",
      "step 43200 | train_loss 1.8711 | val_loss 1.9914 | lr 5.896788e-05\n",
      "step 43400 | train_loss 1.8699 | val_loss 1.9910 | lr 5.559934e-05\n",
      "step 43600 | train_loss 1.8709 | val_loss 1.9901 | lr 5.232560e-05\n",
      "step 43800 | train_loss 1.8720 | val_loss 1.9897 | lr 4.914718e-05\n",
      "step 44000 | train_loss 1.8701 | val_loss 1.9904 | lr 4.606459e-05\n",
      "step 44200 | train_loss 1.8722 | val_loss 1.9898 | lr 4.307831e-05\n",
      "step 44400 | train_loss 1.8725 | val_loss 1.9901 | lr 4.018882e-05\n",
      "step 44600 | train_loss 1.8714 | val_loss 1.9909 | lr 3.739656e-05\n",
      "step 44800 | train_loss 1.8710 | val_loss 1.9907 | lr 3.470198e-05\n",
      "step 45000 | train_loss 1.8713 | val_loss 1.9905 | lr 3.210551e-05\n",
      "step 45200 | train_loss 1.8715 | val_loss 1.9904 | lr 2.960755e-05\n",
      "step 45400 | train_loss 1.8720 | val_loss 1.9907 | lr 2.720851e-05\n",
      "step 45600 | train_loss 1.8723 | val_loss 1.9908 | lr 2.490875e-05\n",
      "step 45800 | train_loss 1.8729 | val_loss 1.9908 | lr 2.270864e-05\n",
      "step 46000 | train_loss 1.8734 | val_loss 1.9908 | lr 2.060854e-05\n",
      "step 46200 | train_loss 1.8747 | val_loss 1.9907 | lr 1.860876e-05\n",
      "step 46400 | train_loss 1.8753 | val_loss 1.9907 | lr 1.670963e-05\n",
      "step 46600 | train_loss 1.8751 | val_loss 1.9906 | lr 1.491145e-05\n",
      "step 46800 | train_loss 1.8710 | val_loss 1.9905 | lr 1.321450e-05\n",
      "step 47000 | train_loss 1.8701 | val_loss 1.9906 | lr 1.161905e-05\n",
      "step 47200 | train_loss 1.8701 | val_loss 1.9906 | lr 1.012535e-05\n",
      "step 47400 | train_loss 1.8704 | val_loss 1.9905 | lr 8.733635e-06\n",
      "step 47600 | train_loss 1.8707 | val_loss 1.9906 | lr 7.444127e-06\n",
      "step 47800 | train_loss 1.8710 | val_loss 1.9905 | lr 6.257029e-06\n",
      "step 48000 | train_loss 1.8707 | val_loss 1.9905 | lr 5.172528e-06\n",
      "step 48200 | train_loss 1.8708 | val_loss 1.9905 | lr 4.190795e-06\n",
      "step 48400 | train_loss 1.8708 | val_loss 1.9905 | lr 3.311987e-06\n",
      "step 48600 | train_loss 1.8708 | val_loss 1.9905 | lr 2.536240e-06\n",
      "step 48800 | train_loss 1.8708 | val_loss 1.9905 | lr 1.863679e-06\n",
      "step 49000 | train_loss 1.8708 | val_loss 1.9905 | lr 1.294409e-06\n",
      "step 49200 | train_loss 1.8707 | val_loss 1.9905 | lr 8.285199e-07\n",
      "step 49400 | train_loss 1.8708 | val_loss 1.9905 | lr 4.660854e-07\n",
      "step 49600 | train_loss 1.8708 | val_loss 1.9905 | lr 2.071627e-07\n",
      "step 49800 | train_loss 1.8708 | val_loss 1.9905 | lr 5.179272e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:19:13,058] Trial 16 finished with value: 1.9904905743896961 and parameters: {'embedding_size': 25, 'hidden_size': 534, 'learning_rate': 0.0013119420989330382, 'batch_size': 32, 'context_length': 4}. Best is trial 16 with value: 1.9904905743896961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8708 | val_loss 1.9905 | lr 1.294835e-12\n",
      "step 0 | train_loss 3.2958 | val_loss 3.2995 | lr 2.201489e-03\n",
      "step 200 | train_loss 2.3566 | val_loss 2.4277 | lr 2.201402e-03\n",
      "step 400 | train_loss 2.3331 | val_loss 2.3725 | lr 2.201141e-03\n",
      "step 600 | train_loss 2.3119 | val_loss 2.3287 | lr 2.200707e-03\n",
      "step 800 | train_loss 2.2604 | val_loss 2.2894 | lr 2.200098e-03\n",
      "step 1000 | train_loss 2.2604 | val_loss 2.2645 | lr 2.199317e-03\n",
      "step 1200 | train_loss 2.2550 | val_loss 2.2760 | lr 2.198361e-03\n",
      "step 1400 | train_loss 2.3023 | val_loss 2.2722 | lr 2.197233e-03\n",
      "step 1600 | train_loss 2.2674 | val_loss 2.2370 | lr 2.195931e-03\n",
      "step 1800 | train_loss 2.2702 | val_loss 2.2390 | lr 2.194456e-03\n",
      "step 2000 | train_loss 2.2599 | val_loss 2.2330 | lr 2.192809e-03\n",
      "step 2200 | train_loss 2.2354 | val_loss 2.2202 | lr 2.190989e-03\n",
      "step 2400 | train_loss 2.1933 | val_loss 2.2208 | lr 2.188997e-03\n",
      "step 2600 | train_loss 2.2247 | val_loss 2.1989 | lr 2.186833e-03\n",
      "step 2800 | train_loss 2.2455 | val_loss 2.2073 | lr 2.184498e-03\n",
      "step 3000 | train_loss 2.2181 | val_loss 2.2055 | lr 2.181992e-03\n",
      "step 3200 | train_loss 2.1937 | val_loss 2.1934 | lr 2.179314e-03\n",
      "step 3400 | train_loss 2.2089 | val_loss 2.1841 | lr 2.176467e-03\n",
      "step 3600 | train_loss 2.2181 | val_loss 2.1856 | lr 2.173449e-03\n",
      "step 3800 | train_loss 2.1737 | val_loss 2.2087 | lr 2.170263e-03\n",
      "step 4000 | train_loss 2.2043 | val_loss 2.1830 | lr 2.166907e-03\n",
      "step 4200 | train_loss 2.2086 | val_loss 2.1813 | lr 2.163383e-03\n",
      "step 4400 | train_loss 2.1986 | val_loss 2.1814 | lr 2.159691e-03\n",
      "step 4600 | train_loss 2.1830 | val_loss 2.1542 | lr 2.155832e-03\n",
      "step 4800 | train_loss 2.2066 | val_loss 2.1704 | lr 2.151806e-03\n",
      "step 5000 | train_loss 2.2134 | val_loss 2.1689 | lr 2.147614e-03\n",
      "step 5200 | train_loss 2.1751 | val_loss 2.1720 | lr 2.143257e-03\n",
      "step 5400 | train_loss 2.2099 | val_loss 2.1545 | lr 2.138736e-03\n",
      "step 5600 | train_loss 2.1853 | val_loss 2.1571 | lr 2.134050e-03\n",
      "step 5800 | train_loss 2.1802 | val_loss 2.1595 | lr 2.129202e-03\n",
      "step 6000 | train_loss 2.1836 | val_loss 2.1398 | lr 2.124191e-03\n",
      "step 6200 | train_loss 2.2024 | val_loss 2.1427 | lr 2.119018e-03\n",
      "step 6400 | train_loss 2.1968 | val_loss 2.1435 | lr 2.113684e-03\n",
      "step 6600 | train_loss 2.1975 | val_loss 2.1430 | lr 2.108191e-03\n",
      "step 6800 | train_loss 1.8864 | val_loss 2.1488 | lr 2.102538e-03\n",
      "step 7000 | train_loss 1.9878 | val_loss 2.1395 | lr 2.096728e-03\n",
      "step 7200 | train_loss 2.0551 | val_loss 2.1331 | lr 2.090760e-03\n",
      "step 7400 | train_loss 2.0931 | val_loss 2.1409 | lr 2.084635e-03\n",
      "step 7600 | train_loss 2.0904 | val_loss 2.1318 | lr 2.078356e-03\n",
      "step 7800 | train_loss 2.0482 | val_loss 2.1318 | lr 2.071921e-03\n",
      "step 8000 | train_loss 2.0606 | val_loss 2.1245 | lr 2.065334e-03\n",
      "step 8200 | train_loss 2.1166 | val_loss 2.1383 | lr 2.058594e-03\n",
      "step 8400 | train_loss 2.1296 | val_loss 2.1527 | lr 2.051703e-03\n",
      "step 8600 | train_loss 2.0941 | val_loss 2.1131 | lr 2.044662e-03\n",
      "step 8800 | train_loss 2.1025 | val_loss 2.1283 | lr 2.037472e-03\n",
      "step 9000 | train_loss 2.0999 | val_loss 2.1186 | lr 2.030134e-03\n",
      "step 9200 | train_loss 2.0898 | val_loss 2.1372 | lr 2.022649e-03\n",
      "step 9400 | train_loss 2.1205 | val_loss 2.1237 | lr 2.015018e-03\n",
      "step 9600 | train_loss 2.1180 | val_loss 2.1215 | lr 2.007243e-03\n",
      "step 9800 | train_loss 2.1327 | val_loss 2.1147 | lr 1.999325e-03\n",
      "step 10000 | train_loss 2.1345 | val_loss 2.1298 | lr 1.991265e-03\n",
      "step 10200 | train_loss 2.1289 | val_loss 2.1187 | lr 1.983065e-03\n",
      "step 10400 | train_loss 2.1217 | val_loss 2.1232 | lr 1.974725e-03\n",
      "step 10600 | train_loss 2.0954 | val_loss 2.1050 | lr 1.966247e-03\n",
      "step 10800 | train_loss 2.1322 | val_loss 2.1141 | lr 1.957632e-03\n",
      "step 11000 | train_loss 2.1177 | val_loss 2.1258 | lr 1.948882e-03\n",
      "step 11200 | train_loss 2.1065 | val_loss 2.1198 | lr 1.939999e-03\n",
      "step 11400 | train_loss 2.1319 | val_loss 2.1106 | lr 1.930982e-03\n",
      "step 11600 | train_loss 2.1147 | val_loss 2.1136 | lr 1.921835e-03\n",
      "step 11800 | train_loss 2.1224 | val_loss 2.1055 | lr 1.912558e-03\n",
      "step 12000 | train_loss 2.1208 | val_loss 2.1172 | lr 1.903152e-03\n",
      "step 12200 | train_loss 2.1133 | val_loss 2.1192 | lr 1.893620e-03\n",
      "step 12400 | train_loss 2.1205 | val_loss 2.1040 | lr 1.883963e-03\n",
      "step 12600 | train_loss 2.1135 | val_loss 2.1031 | lr 1.874182e-03\n",
      "step 12800 | train_loss 2.1621 | val_loss 2.1031 | lr 1.864279e-03\n",
      "step 13000 | train_loss 2.1271 | val_loss 2.1020 | lr 1.854256e-03\n",
      "step 13200 | train_loss 2.0861 | val_loss 2.0987 | lr 1.844113e-03\n",
      "step 13400 | train_loss 1.7503 | val_loss 2.1036 | lr 1.833853e-03\n",
      "step 13600 | train_loss 1.8640 | val_loss 2.1010 | lr 1.823477e-03\n",
      "step 13800 | train_loss 1.9481 | val_loss 2.1061 | lr 1.812988e-03\n",
      "step 14000 | train_loss 1.9823 | val_loss 2.0822 | lr 1.802385e-03\n",
      "step 14200 | train_loss 1.9941 | val_loss 2.0911 | lr 1.791672e-03\n",
      "step 14400 | train_loss 2.0162 | val_loss 2.0917 | lr 1.780850e-03\n",
      "step 14600 | train_loss 1.9875 | val_loss 2.0942 | lr 1.769920e-03\n",
      "step 14800 | train_loss 2.0361 | val_loss 2.0865 | lr 1.758885e-03\n",
      "step 15000 | train_loss 2.0394 | val_loss 2.0908 | lr 1.747746e-03\n",
      "step 15200 | train_loss 2.0375 | val_loss 2.0804 | lr 1.736504e-03\n",
      "step 15400 | train_loss 2.0266 | val_loss 2.0724 | lr 1.725162e-03\n",
      "step 15600 | train_loss 2.0163 | val_loss 2.0829 | lr 1.713722e-03\n",
      "step 15800 | train_loss 2.0150 | val_loss 2.0864 | lr 1.702185e-03\n",
      "step 16000 | train_loss 2.0466 | val_loss 2.0893 | lr 1.690553e-03\n",
      "step 16200 | train_loss 2.0452 | val_loss 2.0811 | lr 1.678827e-03\n",
      "step 16400 | train_loss 2.0441 | val_loss 2.0781 | lr 1.667011e-03\n",
      "step 16600 | train_loss 2.0480 | val_loss 2.0812 | lr 1.655105e-03\n",
      "step 16800 | train_loss 2.0797 | val_loss 2.0735 | lr 1.643111e-03\n",
      "step 17000 | train_loss 2.0687 | val_loss 2.0833 | lr 1.631032e-03\n",
      "step 17200 | train_loss 2.0247 | val_loss 2.0750 | lr 1.618869e-03\n",
      "step 17400 | train_loss 2.0525 | val_loss 2.0703 | lr 1.606624e-03\n",
      "step 17600 | train_loss 2.0485 | val_loss 2.0683 | lr 1.594300e-03\n",
      "step 17800 | train_loss 2.0599 | val_loss 2.0899 | lr 1.581897e-03\n",
      "step 18000 | train_loss 2.0580 | val_loss 2.0778 | lr 1.569419e-03\n",
      "step 18200 | train_loss 2.0609 | val_loss 2.0755 | lr 1.556866e-03\n",
      "step 18400 | train_loss 2.0758 | val_loss 2.0794 | lr 1.544241e-03\n",
      "step 18600 | train_loss 2.0594 | val_loss 2.0811 | lr 1.531547e-03\n",
      "step 18800 | train_loss 2.0575 | val_loss 2.0788 | lr 1.518784e-03\n",
      "step 19000 | train_loss 2.0588 | val_loss 2.0780 | lr 1.505955e-03\n",
      "step 19200 | train_loss 2.0567 | val_loss 2.0805 | lr 1.493063e-03\n",
      "step 19400 | train_loss 2.0621 | val_loss 2.0662 | lr 1.480108e-03\n",
      "step 19600 | train_loss 2.0648 | val_loss 2.0625 | lr 1.467094e-03\n",
      "step 19800 | train_loss 2.0304 | val_loss 2.0660 | lr 1.454021e-03\n",
      "step 20000 | train_loss 2.0394 | val_loss 2.0644 | lr 1.440893e-03\n",
      "step 20200 | train_loss 1.8039 | val_loss 2.0709 | lr 1.427711e-03\n",
      "step 20400 | train_loss 1.8631 | val_loss 2.0679 | lr 1.414478e-03\n",
      "step 20600 | train_loss 1.9105 | val_loss 2.0605 | lr 1.401195e-03\n",
      "step 20800 | train_loss 1.9401 | val_loss 2.0636 | lr 1.387864e-03\n",
      "step 21000 | train_loss 1.9379 | val_loss 2.0556 | lr 1.374488e-03\n",
      "step 21200 | train_loss 1.9441 | val_loss 2.0549 | lr 1.361069e-03\n",
      "step 21400 | train_loss 1.9396 | val_loss 2.0573 | lr 1.347609e-03\n",
      "step 21600 | train_loss 1.9597 | val_loss 2.0527 | lr 1.334110e-03\n",
      "step 21800 | train_loss 1.9723 | val_loss 2.0648 | lr 1.320574e-03\n",
      "step 22000 | train_loss 1.9714 | val_loss 2.0496 | lr 1.307003e-03\n",
      "step 22200 | train_loss 1.9698 | val_loss 2.0512 | lr 1.293400e-03\n",
      "step 22400 | train_loss 1.9654 | val_loss 2.0576 | lr 1.279766e-03\n",
      "step 22600 | train_loss 1.9587 | val_loss 2.0495 | lr 1.266104e-03\n",
      "step 22800 | train_loss 1.9777 | val_loss 2.0550 | lr 1.252416e-03\n",
      "step 23000 | train_loss 1.9932 | val_loss 2.0560 | lr 1.238704e-03\n",
      "step 23200 | train_loss 2.0126 | val_loss 2.0599 | lr 1.224970e-03\n",
      "step 23400 | train_loss 2.0099 | val_loss 2.0541 | lr 1.211217e-03\n",
      "step 23600 | train_loss 2.0092 | val_loss 2.0540 | lr 1.197446e-03\n",
      "step 23800 | train_loss 1.9920 | val_loss 2.0495 | lr 1.183660e-03\n",
      "step 24000 | train_loss 1.9918 | val_loss 2.0547 | lr 1.169861e-03\n",
      "step 24200 | train_loss 1.9986 | val_loss 2.0463 | lr 1.156051e-03\n",
      "step 24400 | train_loss 1.9927 | val_loss 2.0572 | lr 1.142232e-03\n",
      "step 24600 | train_loss 1.9934 | val_loss 2.0505 | lr 1.128406e-03\n",
      "step 24800 | train_loss 2.0032 | val_loss 2.0455 | lr 1.114576e-03\n",
      "step 25000 | train_loss 2.0101 | val_loss 2.0483 | lr 1.100744e-03\n",
      "step 25200 | train_loss 2.0219 | val_loss 2.0538 | lr 1.086912e-03\n",
      "step 25400 | train_loss 2.0018 | val_loss 2.0489 | lr 1.073083e-03\n",
      "step 25600 | train_loss 1.9802 | val_loss 2.0432 | lr 1.059257e-03\n",
      "step 25800 | train_loss 1.9925 | val_loss 2.0490 | lr 1.045438e-03\n",
      "step 26000 | train_loss 2.0059 | val_loss 2.0465 | lr 1.031628e-03\n",
      "step 26200 | train_loss 2.0125 | val_loss 2.0461 | lr 1.017829e-03\n",
      "step 26400 | train_loss 1.9926 | val_loss 2.0410 | lr 1.004043e-03\n",
      "step 26600 | train_loss 1.9722 | val_loss 2.0415 | lr 9.902718e-04\n",
      "step 26800 | train_loss 1.7477 | val_loss 2.0400 | lr 9.765183e-04\n",
      "step 27000 | train_loss 1.8042 | val_loss 2.0409 | lr 9.627845e-04\n",
      "step 27200 | train_loss 1.8470 | val_loss 2.0363 | lr 9.490725e-04\n",
      "step 27400 | train_loss 1.8785 | val_loss 2.0332 | lr 9.353844e-04\n",
      "step 27600 | train_loss 1.8778 | val_loss 2.0367 | lr 9.217224e-04\n",
      "step 27800 | train_loss 1.8902 | val_loss 2.0357 | lr 9.080887e-04\n",
      "step 28000 | train_loss 1.8989 | val_loss 2.0337 | lr 8.944854e-04\n",
      "step 28200 | train_loss 1.9013 | val_loss 2.0318 | lr 8.809147e-04\n",
      "step 28400 | train_loss 1.9077 | val_loss 2.0393 | lr 8.673787e-04\n",
      "step 28600 | train_loss 1.9183 | val_loss 2.0320 | lr 8.538796e-04\n",
      "step 28800 | train_loss 1.9220 | val_loss 2.0241 | lr 8.404194e-04\n",
      "step 29000 | train_loss 1.9136 | val_loss 2.0331 | lr 8.270004e-04\n",
      "step 29200 | train_loss 1.9045 | val_loss 2.0325 | lr 8.136245e-04\n",
      "step 29400 | train_loss 1.9219 | val_loss 2.0344 | lr 8.002941e-04\n",
      "step 29600 | train_loss 1.9229 | val_loss 2.0273 | lr 7.870110e-04\n",
      "step 29800 | train_loss 1.9502 | val_loss 2.0302 | lr 7.737775e-04\n",
      "step 30000 | train_loss 1.9466 | val_loss 2.0307 | lr 7.605956e-04\n",
      "step 30200 | train_loss 1.9569 | val_loss 2.0288 | lr 7.474675e-04\n",
      "step 30400 | train_loss 1.9639 | val_loss 2.0303 | lr 7.343951e-04\n",
      "step 30600 | train_loss 1.9412 | val_loss 2.0315 | lr 7.213806e-04\n",
      "step 30800 | train_loss 1.9457 | val_loss 2.0252 | lr 7.084260e-04\n",
      "step 31000 | train_loss 1.9521 | val_loss 2.0255 | lr 6.955333e-04\n",
      "step 31200 | train_loss 1.9414 | val_loss 2.0356 | lr 6.827047e-04\n",
      "step 31400 | train_loss 1.9391 | val_loss 2.0293 | lr 6.699420e-04\n",
      "step 31600 | train_loss 1.9477 | val_loss 2.0250 | lr 6.572474e-04\n",
      "step 31800 | train_loss 1.9547 | val_loss 2.0254 | lr 6.446228e-04\n",
      "step 32000 | train_loss 1.9407 | val_loss 2.0278 | lr 6.320702e-04\n",
      "step 32200 | train_loss 1.9413 | val_loss 2.0235 | lr 6.195916e-04\n",
      "step 32400 | train_loss 1.9346 | val_loss 2.0248 | lr 6.071891e-04\n",
      "step 32600 | train_loss 1.9449 | val_loss 2.0280 | lr 5.948644e-04\n",
      "step 32800 | train_loss 1.9452 | val_loss 2.0213 | lr 5.826197e-04\n",
      "step 33000 | train_loss 1.9469 | val_loss 2.0218 | lr 5.704567e-04\n",
      "step 33200 | train_loss 1.9320 | val_loss 2.0225 | lr 5.583775e-04\n",
      "step 33400 | train_loss 1.9267 | val_loss 2.0187 | lr 5.463840e-04\n",
      "step 33600 | train_loss 1.7964 | val_loss 2.0228 | lr 5.344779e-04\n",
      "step 33800 | train_loss 1.8143 | val_loss 2.0211 | lr 5.226613e-04\n",
      "step 34000 | train_loss 1.8355 | val_loss 2.0210 | lr 5.109360e-04\n",
      "step 34200 | train_loss 1.8442 | val_loss 2.0192 | lr 4.993039e-04\n",
      "step 34400 | train_loss 1.8551 | val_loss 2.0187 | lr 4.877667e-04\n",
      "step 34600 | train_loss 1.8496 | val_loss 2.0170 | lr 4.763263e-04\n",
      "step 34800 | train_loss 1.8547 | val_loss 2.0192 | lr 4.649845e-04\n",
      "step 35000 | train_loss 1.8650 | val_loss 2.0155 | lr 4.537431e-04\n",
      "step 35200 | train_loss 1.8637 | val_loss 2.0186 | lr 4.426038e-04\n",
      "step 35400 | train_loss 1.8662 | val_loss 2.0131 | lr 4.315685e-04\n",
      "step 35600 | train_loss 1.8645 | val_loss 2.0120 | lr 4.206389e-04\n",
      "step 35800 | train_loss 1.8695 | val_loss 2.0125 | lr 4.098166e-04\n",
      "step 36000 | train_loss 1.8704 | val_loss 2.0106 | lr 3.991035e-04\n",
      "step 36200 | train_loss 1.8759 | val_loss 2.0121 | lr 3.885012e-04\n",
      "step 36400 | train_loss 1.8788 | val_loss 2.0119 | lr 3.780113e-04\n",
      "step 36600 | train_loss 1.8914 | val_loss 2.0116 | lr 3.676356e-04\n",
      "step 36800 | train_loss 1.8883 | val_loss 2.0099 | lr 3.573756e-04\n",
      "step 37000 | train_loss 1.8912 | val_loss 2.0112 | lr 3.472330e-04\n",
      "step 37200 | train_loss 1.8933 | val_loss 2.0124 | lr 3.372094e-04\n",
      "step 37400 | train_loss 1.8897 | val_loss 2.0094 | lr 3.273064e-04\n",
      "step 37600 | train_loss 1.8966 | val_loss 2.0097 | lr 3.175255e-04\n",
      "step 37800 | train_loss 1.8955 | val_loss 2.0109 | lr 3.078683e-04\n",
      "step 38000 | train_loss 1.8913 | val_loss 2.0096 | lr 2.983363e-04\n",
      "step 38200 | train_loss 1.8974 | val_loss 2.0101 | lr 2.889310e-04\n",
      "step 38400 | train_loss 1.8962 | val_loss 2.0079 | lr 2.796539e-04\n",
      "step 38600 | train_loss 1.8913 | val_loss 2.0097 | lr 2.705064e-04\n",
      "step 38800 | train_loss 1.8991 | val_loss 2.0085 | lr 2.614901e-04\n",
      "step 39000 | train_loss 1.8909 | val_loss 2.0063 | lr 2.526063e-04\n",
      "step 39200 | train_loss 1.8966 | val_loss 2.0092 | lr 2.438564e-04\n",
      "step 39400 | train_loss 1.8939 | val_loss 2.0069 | lr 2.352418e-04\n",
      "step 39600 | train_loss 1.9006 | val_loss 2.0067 | lr 2.267639e-04\n",
      "step 39800 | train_loss 1.8956 | val_loss 2.0040 | lr 2.184240e-04\n",
      "step 40000 | train_loss 1.8880 | val_loss 2.0038 | lr 2.102235e-04\n",
      "step 40200 | train_loss 1.8233 | val_loss 2.0048 | lr 2.021635e-04\n",
      "step 40400 | train_loss 1.8289 | val_loss 2.0051 | lr 1.942455e-04\n",
      "step 40600 | train_loss 1.8368 | val_loss 2.0047 | lr 1.864706e-04\n",
      "step 40800 | train_loss 1.8442 | val_loss 2.0052 | lr 1.788401e-04\n",
      "step 41000 | train_loss 1.8421 | val_loss 2.0036 | lr 1.713552e-04\n",
      "step 41200 | train_loss 1.8457 | val_loss 2.0048 | lr 1.640170e-04\n",
      "step 41400 | train_loss 1.8434 | val_loss 2.0040 | lr 1.568267e-04\n",
      "step 41600 | train_loss 1.8443 | val_loss 2.0044 | lr 1.497855e-04\n",
      "step 41800 | train_loss 1.8433 | val_loss 2.0050 | lr 1.428945e-04\n",
      "step 42000 | train_loss 1.8457 | val_loss 2.0028 | lr 1.361547e-04\n",
      "step 42200 | train_loss 1.8476 | val_loss 2.0023 | lr 1.295673e-04\n",
      "step 42400 | train_loss 1.8444 | val_loss 2.0026 | lr 1.231332e-04\n",
      "step 42600 | train_loss 1.8433 | val_loss 2.0021 | lr 1.168535e-04\n",
      "step 42800 | train_loss 1.8470 | val_loss 2.0021 | lr 1.107291e-04\n",
      "step 43000 | train_loss 1.8448 | val_loss 2.0010 | lr 1.047611e-04\n",
      "step 43200 | train_loss 1.8488 | val_loss 2.0002 | lr 9.895035e-05\n",
      "step 43400 | train_loss 1.8493 | val_loss 2.0000 | lr 9.329780e-05\n",
      "step 43600 | train_loss 1.8505 | val_loss 1.9989 | lr 8.780434e-05\n",
      "step 43800 | train_loss 1.8538 | val_loss 1.9989 | lr 8.247084e-05\n",
      "step 44000 | train_loss 1.8516 | val_loss 1.9995 | lr 7.729814e-05\n",
      "step 44200 | train_loss 1.8526 | val_loss 1.9984 | lr 7.228705e-05\n",
      "step 44400 | train_loss 1.8538 | val_loss 1.9984 | lr 6.743836e-05\n",
      "step 44600 | train_loss 1.8539 | val_loss 1.9994 | lr 6.275285e-05\n",
      "step 44800 | train_loss 1.8526 | val_loss 1.9990 | lr 5.823124e-05\n",
      "step 45000 | train_loss 1.8543 | val_loss 1.9986 | lr 5.387426e-05\n",
      "step 45200 | train_loss 1.8550 | val_loss 1.9986 | lr 4.968260e-05\n",
      "step 45400 | train_loss 1.8540 | val_loss 1.9991 | lr 4.565691e-05\n",
      "step 45600 | train_loss 1.8554 | val_loss 1.9991 | lr 4.179783e-05\n",
      "step 45800 | train_loss 1.8559 | val_loss 1.9991 | lr 3.810597e-05\n",
      "step 46000 | train_loss 1.8566 | val_loss 1.9989 | lr 3.458191e-05\n",
      "step 46200 | train_loss 1.8578 | val_loss 1.9985 | lr 3.122621e-05\n",
      "step 46400 | train_loss 1.8581 | val_loss 1.9984 | lr 2.803940e-05\n",
      "step 46600 | train_loss 1.8580 | val_loss 1.9984 | lr 2.502198e-05\n",
      "step 46800 | train_loss 1.8512 | val_loss 1.9983 | lr 2.217444e-05\n",
      "step 47000 | train_loss 1.8497 | val_loss 1.9984 | lr 1.949721e-05\n",
      "step 47200 | train_loss 1.8496 | val_loss 1.9983 | lr 1.699072e-05\n",
      "step 47400 | train_loss 1.8500 | val_loss 1.9982 | lr 1.465537e-05\n",
      "step 47600 | train_loss 1.8507 | val_loss 1.9984 | lr 1.249153e-05\n",
      "step 47800 | train_loss 1.8508 | val_loss 1.9984 | lr 1.049953e-05\n",
      "step 48000 | train_loss 1.8505 | val_loss 1.9984 | lr 8.679698e-06\n",
      "step 48200 | train_loss 1.8506 | val_loss 1.9985 | lr 7.032314e-06\n",
      "step 48400 | train_loss 1.8507 | val_loss 1.9985 | lr 5.557640e-06\n",
      "step 48600 | train_loss 1.8505 | val_loss 1.9984 | lr 4.255908e-06\n",
      "step 48800 | train_loss 1.8505 | val_loss 1.9984 | lr 3.127325e-06\n",
      "step 49000 | train_loss 1.8504 | val_loss 1.9984 | lr 2.172068e-06\n",
      "step 49200 | train_loss 1.8504 | val_loss 1.9984 | lr 1.390288e-06\n",
      "step 49400 | train_loss 1.8504 | val_loss 1.9984 | lr 7.821090e-07\n",
      "step 49600 | train_loss 1.8504 | val_loss 1.9984 | lr 3.476269e-07\n",
      "step 49800 | train_loss 1.8504 | val_loss 1.9984 | lr 8.691015e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:20:17,489] Trial 17 finished with value: 1.9984359991337572 and parameters: {'embedding_size': 30, 'hidden_size': 557, 'learning_rate': 0.0022014887199505615, 'batch_size': 32, 'context_length': 4}. Best is trial 16 with value: 1.9904905743896961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8504 | val_loss 1.9984 | lr 2.172782e-12\n",
      "step 0 | train_loss 3.2977 | val_loss 3.2999 | lr 2.073719e-03\n",
      "step 200 | train_loss 2.3489 | val_loss 2.4343 | lr 2.073637e-03\n",
      "step 400 | train_loss 2.3527 | val_loss 2.3716 | lr 2.073392e-03\n",
      "step 600 | train_loss 2.3371 | val_loss 2.3365 | lr 2.072983e-03\n",
      "step 800 | train_loss 2.2761 | val_loss 2.2845 | lr 2.072410e-03\n",
      "step 1000 | train_loss 2.2823 | val_loss 2.2704 | lr 2.071673e-03\n",
      "step 1200 | train_loss 2.2636 | val_loss 2.2744 | lr 2.070774e-03\n",
      "step 1400 | train_loss 2.2958 | val_loss 2.2651 | lr 2.069710e-03\n",
      "step 1600 | train_loss 2.2756 | val_loss 2.2370 | lr 2.068484e-03\n",
      "step 1800 | train_loss 2.2896 | val_loss 2.2393 | lr 2.067095e-03\n",
      "step 2000 | train_loss 2.2630 | val_loss 2.2288 | lr 2.065543e-03\n",
      "step 2200 | train_loss 2.2557 | val_loss 2.2242 | lr 2.063829e-03\n",
      "step 2400 | train_loss 2.1993 | val_loss 2.2229 | lr 2.061953e-03\n",
      "step 2600 | train_loss 2.2053 | val_loss 2.2006 | lr 2.059915e-03\n",
      "step 2800 | train_loss 2.2206 | val_loss 2.1977 | lr 2.057715e-03\n",
      "step 3000 | train_loss 2.2048 | val_loss 2.1894 | lr 2.055354e-03\n",
      "step 3200 | train_loss 2.2000 | val_loss 2.1907 | lr 2.052832e-03\n",
      "step 3400 | train_loss 2.1847 | val_loss 2.1826 | lr 2.050150e-03\n",
      "step 3600 | train_loss 2.1954 | val_loss 2.1762 | lr 2.047307e-03\n",
      "step 3800 | train_loss 2.1574 | val_loss 2.2071 | lr 2.044305e-03\n",
      "step 4000 | train_loss 2.1794 | val_loss 2.1741 | lr 2.041145e-03\n",
      "step 4200 | train_loss 2.1882 | val_loss 2.1691 | lr 2.037825e-03\n",
      "step 4400 | train_loss 2.1846 | val_loss 2.1763 | lr 2.034347e-03\n",
      "step 4600 | train_loss 2.1704 | val_loss 2.1620 | lr 2.030712e-03\n",
      "step 4800 | train_loss 2.1884 | val_loss 2.1623 | lr 2.026920e-03\n",
      "step 5000 | train_loss 2.1907 | val_loss 2.1658 | lr 2.022972e-03\n",
      "step 5200 | train_loss 2.1475 | val_loss 2.1614 | lr 2.018868e-03\n",
      "step 5400 | train_loss 2.1672 | val_loss 2.1514 | lr 2.014609e-03\n",
      "step 5600 | train_loss 2.1455 | val_loss 2.1564 | lr 2.010195e-03\n",
      "step 5800 | train_loss 2.1546 | val_loss 2.1578 | lr 2.005628e-03\n",
      "step 6000 | train_loss 2.1670 | val_loss 2.1448 | lr 2.000907e-03\n",
      "step 6200 | train_loss 2.1651 | val_loss 2.1376 | lr 1.996035e-03\n",
      "step 6400 | train_loss 2.1554 | val_loss 2.1383 | lr 1.991011e-03\n",
      "step 6600 | train_loss 2.1614 | val_loss 2.1325 | lr 1.985836e-03\n",
      "step 6800 | train_loss 1.8634 | val_loss 2.1417 | lr 1.980512e-03\n",
      "step 7000 | train_loss 1.9706 | val_loss 2.1431 | lr 1.975038e-03\n",
      "step 7200 | train_loss 2.0275 | val_loss 2.1319 | lr 1.969417e-03\n",
      "step 7400 | train_loss 2.0515 | val_loss 2.1410 | lr 1.963648e-03\n",
      "step 7600 | train_loss 2.0678 | val_loss 2.1269 | lr 1.957733e-03\n",
      "step 7800 | train_loss 2.0333 | val_loss 2.1192 | lr 1.951672e-03\n",
      "step 8000 | train_loss 2.0498 | val_loss 2.1187 | lr 1.945467e-03\n",
      "step 8200 | train_loss 2.0948 | val_loss 2.1291 | lr 1.939118e-03\n",
      "step 8400 | train_loss 2.0958 | val_loss 2.1363 | lr 1.932627e-03\n",
      "step 8600 | train_loss 2.0739 | val_loss 2.1031 | lr 1.925994e-03\n",
      "step 8800 | train_loss 2.0822 | val_loss 2.1163 | lr 1.919222e-03\n",
      "step 9000 | train_loss 2.0773 | val_loss 2.1096 | lr 1.912309e-03\n",
      "step 9200 | train_loss 2.0614 | val_loss 2.1241 | lr 1.905259e-03\n",
      "step 9400 | train_loss 2.0768 | val_loss 2.1139 | lr 1.898071e-03\n",
      "step 9600 | train_loss 2.0759 | val_loss 2.1005 | lr 1.890747e-03\n",
      "step 9800 | train_loss 2.1009 | val_loss 2.1016 | lr 1.883289e-03\n",
      "step 10000 | train_loss 2.0870 | val_loss 2.1214 | lr 1.875697e-03\n",
      "step 10200 | train_loss 2.0745 | val_loss 2.1083 | lr 1.867972e-03\n",
      "step 10400 | train_loss 2.0842 | val_loss 2.1155 | lr 1.860116e-03\n",
      "step 10600 | train_loss 2.0739 | val_loss 2.1003 | lr 1.852130e-03\n",
      "step 10800 | train_loss 2.0937 | val_loss 2.1064 | lr 1.844016e-03\n",
      "step 11000 | train_loss 2.0868 | val_loss 2.1032 | lr 1.835774e-03\n",
      "step 11200 | train_loss 2.0910 | val_loss 2.1057 | lr 1.827406e-03\n",
      "step 11400 | train_loss 2.1136 | val_loss 2.1025 | lr 1.818913e-03\n",
      "step 11600 | train_loss 2.1025 | val_loss 2.1084 | lr 1.810296e-03\n",
      "step 11800 | train_loss 2.0874 | val_loss 2.0982 | lr 1.801557e-03\n",
      "step 12000 | train_loss 2.0808 | val_loss 2.1038 | lr 1.792698e-03\n",
      "step 12200 | train_loss 2.0736 | val_loss 2.1044 | lr 1.783719e-03\n",
      "step 12400 | train_loss 2.0831 | val_loss 2.0988 | lr 1.774622e-03\n",
      "step 12600 | train_loss 2.0885 | val_loss 2.0919 | lr 1.765409e-03\n",
      "step 12800 | train_loss 2.1165 | val_loss 2.0898 | lr 1.756081e-03\n",
      "step 13000 | train_loss 2.0915 | val_loss 2.0939 | lr 1.746639e-03\n",
      "step 13200 | train_loss 2.0761 | val_loss 2.0872 | lr 1.737085e-03\n",
      "step 13400 | train_loss 1.7173 | val_loss 2.0912 | lr 1.727421e-03\n",
      "step 13600 | train_loss 1.8346 | val_loss 2.0926 | lr 1.717647e-03\n",
      "step 13800 | train_loss 1.9217 | val_loss 2.1049 | lr 1.707766e-03\n",
      "step 14000 | train_loss 1.9452 | val_loss 2.0778 | lr 1.697779e-03\n",
      "step 14200 | train_loss 1.9563 | val_loss 2.0893 | lr 1.687688e-03\n",
      "step 14400 | train_loss 1.9873 | val_loss 2.0921 | lr 1.677493e-03\n",
      "step 14600 | train_loss 1.9616 | val_loss 2.0850 | lr 1.667198e-03\n",
      "step 14800 | train_loss 2.0053 | val_loss 2.0791 | lr 1.656803e-03\n",
      "step 15000 | train_loss 2.0093 | val_loss 2.0815 | lr 1.646311e-03\n",
      "step 15200 | train_loss 2.0115 | val_loss 2.0735 | lr 1.635722e-03\n",
      "step 15400 | train_loss 1.9976 | val_loss 2.0697 | lr 1.625038e-03\n",
      "step 15600 | train_loss 2.0007 | val_loss 2.0794 | lr 1.614262e-03\n",
      "step 15800 | train_loss 2.0014 | val_loss 2.0785 | lr 1.603394e-03\n",
      "step 16000 | train_loss 2.0189 | val_loss 2.0830 | lr 1.592437e-03\n",
      "step 16200 | train_loss 2.0115 | val_loss 2.0689 | lr 1.581392e-03\n",
      "step 16400 | train_loss 2.0192 | val_loss 2.0702 | lr 1.570261e-03\n",
      "step 16600 | train_loss 2.0203 | val_loss 2.0772 | lr 1.559046e-03\n",
      "step 16800 | train_loss 2.0532 | val_loss 2.0719 | lr 1.547749e-03\n",
      "step 17000 | train_loss 2.0407 | val_loss 2.0733 | lr 1.536371e-03\n",
      "step 17200 | train_loss 1.9964 | val_loss 2.0722 | lr 1.524914e-03\n",
      "step 17400 | train_loss 2.0146 | val_loss 2.0667 | lr 1.513380e-03\n",
      "step 17600 | train_loss 2.0287 | val_loss 2.0675 | lr 1.501770e-03\n",
      "step 17800 | train_loss 2.0323 | val_loss 2.0772 | lr 1.490087e-03\n",
      "step 18000 | train_loss 2.0225 | val_loss 2.0738 | lr 1.478333e-03\n",
      "step 18200 | train_loss 2.0475 | val_loss 2.0732 | lr 1.466509e-03\n",
      "step 18400 | train_loss 2.0506 | val_loss 2.0690 | lr 1.454617e-03\n",
      "step 18600 | train_loss 2.0414 | val_loss 2.0731 | lr 1.442659e-03\n",
      "step 18800 | train_loss 2.0191 | val_loss 2.0692 | lr 1.430637e-03\n",
      "step 19000 | train_loss 2.0253 | val_loss 2.0773 | lr 1.418553e-03\n",
      "step 19200 | train_loss 2.0158 | val_loss 2.0768 | lr 1.406409e-03\n",
      "step 19400 | train_loss 2.0282 | val_loss 2.0620 | lr 1.394206e-03\n",
      "step 19600 | train_loss 2.0310 | val_loss 2.0622 | lr 1.381947e-03\n",
      "step 19800 | train_loss 2.0126 | val_loss 2.0620 | lr 1.369633e-03\n",
      "step 20000 | train_loss 2.0202 | val_loss 2.0563 | lr 1.357267e-03\n",
      "step 20200 | train_loss 1.7775 | val_loss 2.0668 | lr 1.344850e-03\n",
      "step 20400 | train_loss 1.8352 | val_loss 2.0632 | lr 1.332385e-03\n",
      "step 20600 | train_loss 1.8763 | val_loss 2.0567 | lr 1.319873e-03\n",
      "step 20800 | train_loss 1.8956 | val_loss 2.0595 | lr 1.307316e-03\n",
      "step 21000 | train_loss 1.9087 | val_loss 2.0553 | lr 1.294716e-03\n",
      "step 21200 | train_loss 1.8971 | val_loss 2.0541 | lr 1.282076e-03\n",
      "step 21400 | train_loss 1.9051 | val_loss 2.0562 | lr 1.269397e-03\n",
      "step 21600 | train_loss 1.9478 | val_loss 2.0510 | lr 1.256681e-03\n",
      "step 21800 | train_loss 1.9540 | val_loss 2.0581 | lr 1.243931e-03\n",
      "step 22000 | train_loss 1.9482 | val_loss 2.0505 | lr 1.231148e-03\n",
      "step 22200 | train_loss 1.9400 | val_loss 2.0523 | lr 1.218334e-03\n",
      "step 22400 | train_loss 1.9506 | val_loss 2.0564 | lr 1.205492e-03\n",
      "step 22600 | train_loss 1.9496 | val_loss 2.0444 | lr 1.192623e-03\n",
      "step 22800 | train_loss 1.9518 | val_loss 2.0518 | lr 1.179729e-03\n",
      "step 23000 | train_loss 1.9520 | val_loss 2.0491 | lr 1.166813e-03\n",
      "step 23200 | train_loss 1.9868 | val_loss 2.0519 | lr 1.153876e-03\n",
      "step 23400 | train_loss 1.9789 | val_loss 2.0514 | lr 1.140921e-03\n",
      "step 23600 | train_loss 1.9797 | val_loss 2.0506 | lr 1.127949e-03\n",
      "step 23800 | train_loss 1.9773 | val_loss 2.0501 | lr 1.114963e-03\n",
      "step 24000 | train_loss 1.9795 | val_loss 2.0517 | lr 1.101965e-03\n",
      "step 24200 | train_loss 1.9772 | val_loss 2.0453 | lr 1.088956e-03\n",
      "step 24400 | train_loss 1.9722 | val_loss 2.0561 | lr 1.075939e-03\n",
      "step 24600 | train_loss 1.9663 | val_loss 2.0469 | lr 1.062916e-03\n",
      "step 24800 | train_loss 1.9830 | val_loss 2.0479 | lr 1.049889e-03\n",
      "step 25000 | train_loss 1.9869 | val_loss 2.0500 | lr 1.036860e-03\n",
      "step 25200 | train_loss 1.9983 | val_loss 2.0511 | lr 1.023830e-03\n",
      "step 25400 | train_loss 1.9770 | val_loss 2.0471 | lr 1.010803e-03\n",
      "step 25600 | train_loss 1.9698 | val_loss 2.0469 | lr 9.977802e-04\n",
      "step 25800 | train_loss 1.9863 | val_loss 2.0535 | lr 9.847634e-04\n",
      "step 26000 | train_loss 1.9816 | val_loss 2.0461 | lr 9.717547e-04\n",
      "step 26200 | train_loss 1.9961 | val_loss 2.0448 | lr 9.587564e-04\n",
      "step 26400 | train_loss 1.9808 | val_loss 2.0404 | lr 9.457703e-04\n",
      "step 26600 | train_loss 1.9631 | val_loss 2.0377 | lr 9.327987e-04\n",
      "step 26800 | train_loss 1.7340 | val_loss 2.0364 | lr 9.198434e-04\n",
      "step 27000 | train_loss 1.7846 | val_loss 2.0375 | lr 9.069067e-04\n",
      "step 27200 | train_loss 1.8234 | val_loss 2.0368 | lr 8.939905e-04\n",
      "step 27400 | train_loss 1.8351 | val_loss 2.0336 | lr 8.810968e-04\n",
      "step 27600 | train_loss 1.8455 | val_loss 2.0353 | lr 8.682278e-04\n",
      "step 27800 | train_loss 1.8611 | val_loss 2.0335 | lr 8.553853e-04\n",
      "step 28000 | train_loss 1.8784 | val_loss 2.0367 | lr 8.425715e-04\n",
      "step 28200 | train_loss 1.8894 | val_loss 2.0314 | lr 8.297885e-04\n",
      "step 28400 | train_loss 1.9076 | val_loss 2.0338 | lr 8.170381e-04\n",
      "step 28600 | train_loss 1.9059 | val_loss 2.0345 | lr 8.043224e-04\n",
      "step 28800 | train_loss 1.9117 | val_loss 2.0304 | lr 7.916434e-04\n",
      "step 29000 | train_loss 1.9054 | val_loss 2.0366 | lr 7.790032e-04\n",
      "step 29200 | train_loss 1.8956 | val_loss 2.0324 | lr 7.664036e-04\n",
      "step 29400 | train_loss 1.9124 | val_loss 2.0347 | lr 7.538468e-04\n",
      "step 29600 | train_loss 1.8997 | val_loss 2.0288 | lr 7.413347e-04\n",
      "step 29800 | train_loss 1.9199 | val_loss 2.0315 | lr 7.288692e-04\n",
      "step 30000 | train_loss 1.9150 | val_loss 2.0308 | lr 7.164524e-04\n",
      "step 30200 | train_loss 1.9299 | val_loss 2.0292 | lr 7.040862e-04\n",
      "step 30400 | train_loss 1.9363 | val_loss 2.0317 | lr 6.917725e-04\n",
      "step 30600 | train_loss 1.9186 | val_loss 2.0343 | lr 6.795133e-04\n",
      "step 30800 | train_loss 1.9244 | val_loss 2.0286 | lr 6.673106e-04\n",
      "step 31000 | train_loss 1.9303 | val_loss 2.0290 | lr 6.551662e-04\n",
      "step 31200 | train_loss 1.9206 | val_loss 2.0353 | lr 6.430820e-04\n",
      "step 31400 | train_loss 1.9115 | val_loss 2.0290 | lr 6.310601e-04\n",
      "step 31600 | train_loss 1.9312 | val_loss 2.0291 | lr 6.191022e-04\n",
      "step 31800 | train_loss 1.9389 | val_loss 2.0278 | lr 6.072103e-04\n",
      "step 32000 | train_loss 1.9340 | val_loss 2.0282 | lr 5.953863e-04\n",
      "step 32200 | train_loss 1.9321 | val_loss 2.0251 | lr 5.836320e-04\n",
      "step 32400 | train_loss 1.9204 | val_loss 2.0266 | lr 5.719492e-04\n",
      "step 32600 | train_loss 1.9258 | val_loss 2.0279 | lr 5.603399e-04\n",
      "step 32800 | train_loss 1.9292 | val_loss 2.0241 | lr 5.488058e-04\n",
      "step 33000 | train_loss 1.9351 | val_loss 2.0226 | lr 5.373487e-04\n",
      "step 33200 | train_loss 1.9251 | val_loss 2.0245 | lr 5.259706e-04\n",
      "step 33400 | train_loss 1.9149 | val_loss 2.0175 | lr 5.146731e-04\n",
      "step 33600 | train_loss 1.7863 | val_loss 2.0208 | lr 5.034581e-04\n",
      "step 33800 | train_loss 1.8052 | val_loss 2.0197 | lr 4.923273e-04\n",
      "step 34000 | train_loss 1.8148 | val_loss 2.0213 | lr 4.812825e-04\n",
      "step 34200 | train_loss 1.8217 | val_loss 2.0191 | lr 4.703254e-04\n",
      "step 34400 | train_loss 1.8385 | val_loss 2.0198 | lr 4.594578e-04\n",
      "step 34600 | train_loss 1.8318 | val_loss 2.0174 | lr 4.486814e-04\n",
      "step 34800 | train_loss 1.8395 | val_loss 2.0190 | lr 4.379978e-04\n",
      "step 35000 | train_loss 1.8628 | val_loss 2.0130 | lr 4.274089e-04\n",
      "step 35200 | train_loss 1.8640 | val_loss 2.0166 | lr 4.169161e-04\n",
      "step 35400 | train_loss 1.8604 | val_loss 2.0144 | lr 4.065213e-04\n",
      "step 35600 | train_loss 1.8611 | val_loss 2.0161 | lr 3.962260e-04\n",
      "step 35800 | train_loss 1.8645 | val_loss 2.0153 | lr 3.860318e-04\n",
      "step 36000 | train_loss 1.8655 | val_loss 2.0130 | lr 3.759404e-04\n",
      "step 36200 | train_loss 1.8628 | val_loss 2.0145 | lr 3.659534e-04\n",
      "step 36400 | train_loss 1.8639 | val_loss 2.0144 | lr 3.560724e-04\n",
      "step 36600 | train_loss 1.8733 | val_loss 2.0130 | lr 3.462988e-04\n",
      "step 36800 | train_loss 1.8717 | val_loss 2.0122 | lr 3.366343e-04\n",
      "step 37000 | train_loss 1.8730 | val_loss 2.0132 | lr 3.270804e-04\n",
      "step 37200 | train_loss 1.8757 | val_loss 2.0148 | lr 3.176385e-04\n",
      "step 37400 | train_loss 1.8760 | val_loss 2.0117 | lr 3.083103e-04\n",
      "step 37600 | train_loss 1.8807 | val_loss 2.0132 | lr 2.990970e-04\n",
      "step 37800 | train_loss 1.8787 | val_loss 2.0148 | lr 2.900003e-04\n",
      "step 38000 | train_loss 1.8738 | val_loss 2.0130 | lr 2.810215e-04\n",
      "step 38200 | train_loss 1.8837 | val_loss 2.0141 | lr 2.721621e-04\n",
      "step 38400 | train_loss 1.8805 | val_loss 2.0136 | lr 2.634234e-04\n",
      "step 38600 | train_loss 1.8818 | val_loss 2.0126 | lr 2.548068e-04\n",
      "step 38800 | train_loss 1.8888 | val_loss 2.0121 | lr 2.463138e-04\n",
      "step 39000 | train_loss 1.8792 | val_loss 2.0105 | lr 2.379456e-04\n",
      "step 39200 | train_loss 1.8820 | val_loss 2.0110 | lr 2.297035e-04\n",
      "step 39400 | train_loss 1.8781 | val_loss 2.0095 | lr 2.215889e-04\n",
      "step 39600 | train_loss 1.8878 | val_loss 2.0084 | lr 2.136030e-04\n",
      "step 39800 | train_loss 1.8849 | val_loss 2.0061 | lr 2.057472e-04\n",
      "step 40000 | train_loss 1.8780 | val_loss 2.0056 | lr 1.980226e-04\n",
      "step 40200 | train_loss 1.8145 | val_loss 2.0063 | lr 1.904304e-04\n",
      "step 40400 | train_loss 1.8197 | val_loss 2.0059 | lr 1.829719e-04\n",
      "step 40600 | train_loss 1.8284 | val_loss 2.0055 | lr 1.756483e-04\n",
      "step 40800 | train_loss 1.8309 | val_loss 2.0057 | lr 1.684606e-04\n",
      "step 41000 | train_loss 1.8298 | val_loss 2.0043 | lr 1.614101e-04\n",
      "step 41200 | train_loss 1.8319 | val_loss 2.0048 | lr 1.544978e-04\n",
      "step 41400 | train_loss 1.8322 | val_loss 2.0045 | lr 1.477249e-04\n",
      "step 41600 | train_loss 1.8320 | val_loss 2.0036 | lr 1.410923e-04\n",
      "step 41800 | train_loss 1.8371 | val_loss 2.0036 | lr 1.346012e-04\n",
      "step 42000 | train_loss 1.8374 | val_loss 2.0028 | lr 1.282526e-04\n",
      "step 42200 | train_loss 1.8401 | val_loss 2.0030 | lr 1.220475e-04\n",
      "step 42400 | train_loss 1.8379 | val_loss 2.0032 | lr 1.159868e-04\n",
      "step 42600 | train_loss 1.8387 | val_loss 2.0028 | lr 1.100715e-04\n",
      "step 42800 | train_loss 1.8410 | val_loss 2.0029 | lr 1.043026e-04\n",
      "step 43000 | train_loss 1.8378 | val_loss 2.0020 | lr 9.868099e-05\n",
      "step 43200 | train_loss 1.8400 | val_loss 2.0014 | lr 9.320749e-05\n",
      "step 43400 | train_loss 1.8402 | val_loss 2.0013 | lr 8.788301e-05\n",
      "step 43600 | train_loss 1.8417 | val_loss 2.0002 | lr 8.270838e-05\n",
      "step 43800 | train_loss 1.8432 | val_loss 2.0000 | lr 7.768442e-05\n",
      "step 44000 | train_loss 1.8421 | val_loss 2.0009 | lr 7.281193e-05\n",
      "step 44200 | train_loss 1.8428 | val_loss 2.0003 | lr 6.809167e-05\n",
      "step 44400 | train_loss 1.8427 | val_loss 2.0007 | lr 6.352439e-05\n",
      "step 44600 | train_loss 1.8427 | val_loss 2.0017 | lr 5.911081e-05\n",
      "step 44800 | train_loss 1.8416 | val_loss 2.0016 | lr 5.485163e-05\n",
      "step 45000 | train_loss 1.8424 | val_loss 2.0014 | lr 5.074752e-05\n",
      "step 45200 | train_loss 1.8435 | val_loss 2.0015 | lr 4.679913e-05\n",
      "step 45400 | train_loss 1.8435 | val_loss 2.0017 | lr 4.300708e-05\n",
      "step 45600 | train_loss 1.8444 | val_loss 2.0019 | lr 3.937198e-05\n",
      "step 45800 | train_loss 1.8445 | val_loss 2.0019 | lr 3.589438e-05\n",
      "step 46000 | train_loss 1.8445 | val_loss 2.0015 | lr 3.257485e-05\n",
      "step 46200 | train_loss 1.8455 | val_loss 2.0011 | lr 2.941391e-05\n",
      "step 46400 | train_loss 1.8462 | val_loss 2.0009 | lr 2.641206e-05\n",
      "step 46600 | train_loss 1.8463 | val_loss 2.0008 | lr 2.356977e-05\n",
      "step 46800 | train_loss 1.8397 | val_loss 2.0006 | lr 2.088748e-05\n",
      "step 47000 | train_loss 1.8380 | val_loss 2.0006 | lr 1.836564e-05\n",
      "step 47200 | train_loss 1.8380 | val_loss 2.0005 | lr 1.600462e-05\n",
      "step 47400 | train_loss 1.8382 | val_loss 2.0004 | lr 1.380481e-05\n",
      "step 47600 | train_loss 1.8388 | val_loss 2.0006 | lr 1.176655e-05\n",
      "step 47800 | train_loss 1.8389 | val_loss 2.0004 | lr 9.890163e-06\n",
      "step 48000 | train_loss 1.8386 | val_loss 2.0005 | lr 8.175948e-06\n",
      "step 48200 | train_loss 1.8387 | val_loss 2.0005 | lr 6.624175e-06\n",
      "step 48400 | train_loss 1.8387 | val_loss 2.0004 | lr 5.235087e-06\n",
      "step 48600 | train_loss 1.8386 | val_loss 2.0004 | lr 4.008905e-06\n",
      "step 48800 | train_loss 1.8386 | val_loss 2.0004 | lr 2.945822e-06\n",
      "step 49000 | train_loss 1.8385 | val_loss 2.0004 | lr 2.046006e-06\n",
      "step 49200 | train_loss 1.8385 | val_loss 2.0004 | lr 1.309599e-06\n",
      "step 49400 | train_loss 1.8385 | val_loss 2.0004 | lr 7.367172e-07\n",
      "step 49600 | train_loss 1.8385 | val_loss 2.0004 | lr 3.274514e-07\n",
      "step 49800 | train_loss 1.8385 | val_loss 2.0004 | lr 8.186608e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:21:31,038] Trial 18 finished with value: 2.000408693615879 and parameters: {'embedding_size': 32, 'hidden_size': 576, 'learning_rate': 0.002073719357181968, 'batch_size': 32, 'context_length': 4}. Best is trial 16 with value: 1.9904905743896961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8385 | val_loss 2.0004 | lr 2.046679e-12\n",
      "step 0 | train_loss 3.3071 | val_loss 3.3067 | lr 6.711250e-04\n",
      "step 200 | train_loss 2.4084 | val_loss 2.4944 | lr 6.710985e-04\n",
      "step 400 | train_loss 2.3621 | val_loss 2.4161 | lr 6.710190e-04\n",
      "step 600 | train_loss 2.3345 | val_loss 2.3776 | lr 6.708865e-04\n",
      "step 800 | train_loss 2.3151 | val_loss 2.3421 | lr 6.707011e-04\n",
      "step 1000 | train_loss 2.2843 | val_loss 2.3113 | lr 6.704628e-04\n",
      "step 1200 | train_loss 2.2717 | val_loss 2.2939 | lr 6.701716e-04\n",
      "step 1400 | train_loss 2.2743 | val_loss 2.2878 | lr 6.698276e-04\n",
      "step 1600 | train_loss 2.2749 | val_loss 2.2663 | lr 6.694307e-04\n",
      "step 1800 | train_loss 2.2656 | val_loss 2.2571 | lr 6.689812e-04\n",
      "step 2000 | train_loss 2.2589 | val_loss 2.2465 | lr 6.684790e-04\n",
      "step 2200 | train_loss 2.2493 | val_loss 2.2365 | lr 6.679242e-04\n",
      "step 2400 | train_loss 2.2237 | val_loss 2.2353 | lr 6.673169e-04\n",
      "step 2600 | train_loss 2.2210 | val_loss 2.2204 | lr 6.666573e-04\n",
      "step 2800 | train_loss 2.2089 | val_loss 2.2139 | lr 6.659453e-04\n",
      "step 3000 | train_loss 2.2020 | val_loss 2.2082 | lr 6.651812e-04\n",
      "step 3200 | train_loss 2.1929 | val_loss 2.2034 | lr 6.643651e-04\n",
      "step 3400 | train_loss 2.1832 | val_loss 2.1961 | lr 6.634970e-04\n",
      "step 3600 | train_loss 2.1957 | val_loss 2.1931 | lr 6.625771e-04\n",
      "step 3800 | train_loss 2.1671 | val_loss 2.1957 | lr 6.616056e-04\n",
      "step 4000 | train_loss 2.1696 | val_loss 2.1751 | lr 6.605827e-04\n",
      "step 4200 | train_loss 2.1765 | val_loss 2.1785 | lr 6.595083e-04\n",
      "step 4400 | train_loss 2.1564 | val_loss 2.1714 | lr 6.583829e-04\n",
      "step 4600 | train_loss 2.1538 | val_loss 2.1618 | lr 6.572064e-04\n",
      "step 4800 | train_loss 2.1769 | val_loss 2.1651 | lr 6.559792e-04\n",
      "step 5000 | train_loss 2.1720 | val_loss 2.1616 | lr 6.547014e-04\n",
      "step 5200 | train_loss 2.1624 | val_loss 2.1534 | lr 6.533731e-04\n",
      "step 5400 | train_loss 2.1793 | val_loss 2.1536 | lr 6.519947e-04\n",
      "step 5600 | train_loss 2.1660 | val_loss 2.1532 | lr 6.505664e-04\n",
      "step 5800 | train_loss 2.1673 | val_loss 2.1502 | lr 6.490882e-04\n",
      "step 6000 | train_loss 2.1583 | val_loss 2.1374 | lr 6.475606e-04\n",
      "step 6200 | train_loss 2.1608 | val_loss 2.1417 | lr 6.459837e-04\n",
      "step 6400 | train_loss 2.1456 | val_loss 2.1299 | lr 6.443578e-04\n",
      "step 6600 | train_loss 2.1437 | val_loss 2.1264 | lr 6.426831e-04\n",
      "step 6800 | train_loss 2.0228 | val_loss 2.1311 | lr 6.409599e-04\n",
      "step 7000 | train_loss 2.0640 | val_loss 2.1337 | lr 6.391885e-04\n",
      "step 7200 | train_loss 2.0898 | val_loss 2.1285 | lr 6.373691e-04\n",
      "step 7400 | train_loss 2.0911 | val_loss 2.1255 | lr 6.355021e-04\n",
      "step 7600 | train_loss 2.0834 | val_loss 2.1129 | lr 6.335878e-04\n",
      "step 7800 | train_loss 2.0674 | val_loss 2.1162 | lr 6.316263e-04\n",
      "step 8000 | train_loss 2.0758 | val_loss 2.1149 | lr 6.296181e-04\n",
      "step 8200 | train_loss 2.0859 | val_loss 2.1127 | lr 6.275635e-04\n",
      "step 8400 | train_loss 2.0972 | val_loss 2.1225 | lr 6.254628e-04\n",
      "step 8600 | train_loss 2.0886 | val_loss 2.1054 | lr 6.233163e-04\n",
      "step 8800 | train_loss 2.0882 | val_loss 2.1080 | lr 6.211243e-04\n",
      "step 9000 | train_loss 2.0830 | val_loss 2.1088 | lr 6.188873e-04\n",
      "step 9200 | train_loss 2.0626 | val_loss 2.1097 | lr 6.166055e-04\n",
      "step 9400 | train_loss 2.0790 | val_loss 2.1076 | lr 6.142793e-04\n",
      "step 9600 | train_loss 2.0592 | val_loss 2.1054 | lr 6.119091e-04\n",
      "step 9800 | train_loss 2.0834 | val_loss 2.1020 | lr 6.094953e-04\n",
      "step 10000 | train_loss 2.0749 | val_loss 2.1091 | lr 6.070382e-04\n",
      "step 10200 | train_loss 2.0677 | val_loss 2.1015 | lr 6.045383e-04\n",
      "step 10400 | train_loss 2.0819 | val_loss 2.0974 | lr 6.019959e-04\n",
      "step 10600 | train_loss 2.0600 | val_loss 2.0929 | lr 5.994114e-04\n",
      "step 10800 | train_loss 2.0689 | val_loss 2.0926 | lr 5.967852e-04\n",
      "step 11000 | train_loss 2.0581 | val_loss 2.0933 | lr 5.941178e-04\n",
      "step 11200 | train_loss 2.0523 | val_loss 2.0876 | lr 5.914096e-04\n",
      "step 11400 | train_loss 2.0776 | val_loss 2.0890 | lr 5.886610e-04\n",
      "step 11600 | train_loss 2.0773 | val_loss 2.0918 | lr 5.858723e-04\n",
      "step 11800 | train_loss 2.0809 | val_loss 2.0877 | lr 5.830442e-04\n",
      "step 12000 | train_loss 2.0827 | val_loss 2.0871 | lr 5.801770e-04\n",
      "step 12200 | train_loss 2.0762 | val_loss 2.0852 | lr 5.772712e-04\n",
      "step 12400 | train_loss 2.0819 | val_loss 2.0840 | lr 5.743272e-04\n",
      "step 12600 | train_loss 2.0819 | val_loss 2.0789 | lr 5.713455e-04\n",
      "step 12800 | train_loss 2.1073 | val_loss 2.0861 | lr 5.683265e-04\n",
      "step 13000 | train_loss 2.0844 | val_loss 2.0819 | lr 5.652708e-04\n",
      "step 13200 | train_loss 2.0671 | val_loss 2.0730 | lr 5.621788e-04\n",
      "step 13400 | train_loss 1.9110 | val_loss 2.0727 | lr 5.590511e-04\n",
      "step 13600 | train_loss 1.9556 | val_loss 2.0729 | lr 5.558880e-04\n",
      "step 13800 | train_loss 1.9915 | val_loss 2.0762 | lr 5.526902e-04\n",
      "step 14000 | train_loss 1.9943 | val_loss 2.0634 | lr 5.494581e-04\n",
      "step 14200 | train_loss 2.0064 | val_loss 2.0757 | lr 5.461922e-04\n",
      "step 14400 | train_loss 2.0081 | val_loss 2.0709 | lr 5.428930e-04\n",
      "step 14600 | train_loss 1.9914 | val_loss 2.0705 | lr 5.395611e-04\n",
      "step 14800 | train_loss 2.0117 | val_loss 2.0655 | lr 5.361970e-04\n",
      "step 15000 | train_loss 2.0187 | val_loss 2.0694 | lr 5.328012e-04\n",
      "step 15200 | train_loss 2.0194 | val_loss 2.0640 | lr 5.293742e-04\n",
      "step 15400 | train_loss 2.0217 | val_loss 2.0608 | lr 5.259167e-04\n",
      "step 15600 | train_loss 2.0104 | val_loss 2.0645 | lr 5.224291e-04\n",
      "step 15800 | train_loss 2.0070 | val_loss 2.0681 | lr 5.189119e-04\n",
      "step 16000 | train_loss 2.0098 | val_loss 2.0635 | lr 5.153659e-04\n",
      "step 16200 | train_loss 2.0025 | val_loss 2.0595 | lr 5.117914e-04\n",
      "step 16400 | train_loss 2.0023 | val_loss 2.0606 | lr 5.081891e-04\n",
      "step 16600 | train_loss 2.0144 | val_loss 2.0621 | lr 5.045595e-04\n",
      "step 16800 | train_loss 2.0144 | val_loss 2.0573 | lr 5.009033e-04\n",
      "step 17000 | train_loss 2.0203 | val_loss 2.0586 | lr 4.972209e-04\n",
      "step 17200 | train_loss 1.9922 | val_loss 2.0590 | lr 4.935131e-04\n",
      "step 17400 | train_loss 2.0055 | val_loss 2.0522 | lr 4.897802e-04\n",
      "step 17600 | train_loss 2.0069 | val_loss 2.0516 | lr 4.860231e-04\n",
      "step 17800 | train_loss 1.9945 | val_loss 2.0575 | lr 4.822421e-04\n",
      "step 18000 | train_loss 2.0000 | val_loss 2.0499 | lr 4.784380e-04\n",
      "step 18200 | train_loss 2.0180 | val_loss 2.0573 | lr 4.746114e-04\n",
      "step 18400 | train_loss 2.0216 | val_loss 2.0537 | lr 4.707628e-04\n",
      "step 18600 | train_loss 2.0228 | val_loss 2.0516 | lr 4.668928e-04\n",
      "step 18800 | train_loss 2.0218 | val_loss 2.0525 | lr 4.630021e-04\n",
      "step 19000 | train_loss 2.0190 | val_loss 2.0531 | lr 4.590913e-04\n",
      "step 19200 | train_loss 2.0231 | val_loss 2.0521 | lr 4.551609e-04\n",
      "step 19400 | train_loss 2.0338 | val_loss 2.0440 | lr 4.512117e-04\n",
      "step 19600 | train_loss 2.0305 | val_loss 2.0477 | lr 4.472442e-04\n",
      "step 19800 | train_loss 2.0265 | val_loss 2.0470 | lr 4.432591e-04\n",
      "step 20000 | train_loss 2.0210 | val_loss 2.0413 | lr 4.392570e-04\n",
      "step 20200 | train_loss 1.9083 | val_loss 2.0460 | lr 4.352385e-04\n",
      "step 20400 | train_loss 1.9355 | val_loss 2.0427 | lr 4.312043e-04\n",
      "step 20600 | train_loss 1.9577 | val_loss 2.0406 | lr 4.271549e-04\n",
      "step 20800 | train_loss 1.9650 | val_loss 2.0428 | lr 4.230911e-04\n",
      "step 21000 | train_loss 1.9493 | val_loss 2.0360 | lr 4.190135e-04\n",
      "step 21200 | train_loss 1.9424 | val_loss 2.0357 | lr 4.149227e-04\n",
      "step 21400 | train_loss 1.9497 | val_loss 2.0396 | lr 4.108193e-04\n",
      "step 21600 | train_loss 1.9698 | val_loss 2.0377 | lr 4.067041e-04\n",
      "step 21800 | train_loss 1.9715 | val_loss 2.0432 | lr 4.025777e-04\n",
      "step 22000 | train_loss 1.9706 | val_loss 2.0339 | lr 3.984406e-04\n",
      "step 22200 | train_loss 1.9608 | val_loss 2.0341 | lr 3.942937e-04\n",
      "step 22400 | train_loss 1.9632 | val_loss 2.0418 | lr 3.901374e-04\n",
      "step 22600 | train_loss 1.9523 | val_loss 2.0356 | lr 3.859726e-04\n",
      "step 22800 | train_loss 1.9534 | val_loss 2.0354 | lr 3.817997e-04\n",
      "step 23000 | train_loss 1.9536 | val_loss 2.0358 | lr 3.776196e-04\n",
      "step 23200 | train_loss 1.9673 | val_loss 2.0359 | lr 3.734329e-04\n",
      "step 23400 | train_loss 1.9695 | val_loss 2.0365 | lr 3.692401e-04\n",
      "step 23600 | train_loss 1.9651 | val_loss 2.0352 | lr 3.650421e-04\n",
      "step 23800 | train_loss 1.9677 | val_loss 2.0345 | lr 3.608393e-04\n",
      "step 24000 | train_loss 1.9608 | val_loss 2.0306 | lr 3.566326e-04\n",
      "step 24200 | train_loss 1.9585 | val_loss 2.0312 | lr 3.524226e-04\n",
      "step 24400 | train_loss 1.9563 | val_loss 2.0316 | lr 3.482099e-04\n",
      "step 24600 | train_loss 1.9479 | val_loss 2.0284 | lr 3.439952e-04\n",
      "step 24800 | train_loss 1.9650 | val_loss 2.0288 | lr 3.397792e-04\n",
      "step 25000 | train_loss 1.9670 | val_loss 2.0290 | lr 3.355625e-04\n",
      "step 25200 | train_loss 1.9774 | val_loss 2.0313 | lr 3.313458e-04\n",
      "step 25400 | train_loss 1.9738 | val_loss 2.0299 | lr 3.271298e-04\n",
      "step 25600 | train_loss 1.9669 | val_loss 2.0279 | lr 3.229151e-04\n",
      "step 25800 | train_loss 1.9773 | val_loss 2.0284 | lr 3.187024e-04\n",
      "step 26000 | train_loss 1.9840 | val_loss 2.0241 | lr 3.144923e-04\n",
      "step 26200 | train_loss 1.9931 | val_loss 2.0276 | lr 3.102856e-04\n",
      "step 26400 | train_loss 1.9837 | val_loss 2.0256 | lr 3.060829e-04\n",
      "step 26600 | train_loss 1.9715 | val_loss 2.0214 | lr 3.018849e-04\n",
      "step 26800 | train_loss 1.8755 | val_loss 2.0228 | lr 2.976921e-04\n",
      "step 27000 | train_loss 1.8965 | val_loss 2.0226 | lr 2.935054e-04\n",
      "step 27200 | train_loss 1.9169 | val_loss 2.0236 | lr 2.893252e-04\n",
      "step 27400 | train_loss 1.9210 | val_loss 2.0178 | lr 2.851524e-04\n",
      "step 27600 | train_loss 1.9198 | val_loss 2.0216 | lr 2.809876e-04\n",
      "step 27800 | train_loss 1.9182 | val_loss 2.0195 | lr 2.768313e-04\n",
      "step 28000 | train_loss 1.9232 | val_loss 2.0206 | lr 2.726843e-04\n",
      "step 28200 | train_loss 1.9276 | val_loss 2.0176 | lr 2.685473e-04\n",
      "step 28400 | train_loss 1.9366 | val_loss 2.0226 | lr 2.644209e-04\n",
      "step 28600 | train_loss 1.9352 | val_loss 2.0189 | lr 2.603056e-04\n",
      "step 28800 | train_loss 1.9385 | val_loss 2.0165 | lr 2.562023e-04\n",
      "step 29000 | train_loss 1.9351 | val_loss 2.0222 | lr 2.521115e-04\n",
      "step 29200 | train_loss 1.9255 | val_loss 2.0211 | lr 2.480339e-04\n",
      "step 29400 | train_loss 1.9283 | val_loss 2.0198 | lr 2.439701e-04\n",
      "step 29600 | train_loss 1.9227 | val_loss 2.0176 | lr 2.399207e-04\n",
      "step 29800 | train_loss 1.9299 | val_loss 2.0186 | lr 2.358865e-04\n",
      "step 30000 | train_loss 1.9374 | val_loss 2.0180 | lr 2.318680e-04\n",
      "step 30200 | train_loss 1.9341 | val_loss 2.0166 | lr 2.278658e-04\n",
      "step 30400 | train_loss 1.9398 | val_loss 2.0162 | lr 2.238807e-04\n",
      "step 30600 | train_loss 1.9248 | val_loss 2.0182 | lr 2.199132e-04\n",
      "step 30800 | train_loss 1.9271 | val_loss 2.0133 | lr 2.159640e-04\n",
      "step 31000 | train_loss 1.9346 | val_loss 2.0136 | lr 2.120337e-04\n",
      "step 31200 | train_loss 1.9238 | val_loss 2.0174 | lr 2.081229e-04\n",
      "step 31400 | train_loss 1.9217 | val_loss 2.0135 | lr 2.042322e-04\n",
      "step 31600 | train_loss 1.9344 | val_loss 2.0156 | lr 2.003622e-04\n",
      "step 31800 | train_loss 1.9387 | val_loss 2.0145 | lr 1.965136e-04\n",
      "step 32000 | train_loss 1.9375 | val_loss 2.0144 | lr 1.926869e-04\n",
      "step 32200 | train_loss 1.9359 | val_loss 2.0145 | lr 1.888828e-04\n",
      "step 32400 | train_loss 1.9345 | val_loss 2.0140 | lr 1.851019e-04\n",
      "step 32600 | train_loss 1.9421 | val_loss 2.0129 | lr 1.813447e-04\n",
      "step 32800 | train_loss 1.9466 | val_loss 2.0102 | lr 1.776119e-04\n",
      "step 33000 | train_loss 1.9502 | val_loss 2.0140 | lr 1.739040e-04\n",
      "step 33200 | train_loss 1.9478 | val_loss 2.0124 | lr 1.702217e-04\n",
      "step 33400 | train_loss 1.9439 | val_loss 2.0089 | lr 1.665654e-04\n",
      "step 33600 | train_loss 1.8862 | val_loss 2.0108 | lr 1.629359e-04\n",
      "step 33800 | train_loss 1.8932 | val_loss 2.0089 | lr 1.593336e-04\n",
      "step 34000 | train_loss 1.9045 | val_loss 2.0089 | lr 1.557591e-04\n",
      "step 34200 | train_loss 1.9080 | val_loss 2.0093 | lr 1.522130e-04\n",
      "step 34400 | train_loss 1.9053 | val_loss 2.0082 | lr 1.486959e-04\n",
      "step 34600 | train_loss 1.8980 | val_loss 2.0078 | lr 1.452083e-04\n",
      "step 34800 | train_loss 1.9019 | val_loss 2.0091 | lr 1.417508e-04\n",
      "step 35000 | train_loss 1.9103 | val_loss 2.0075 | lr 1.383238e-04\n",
      "step 35200 | train_loss 1.9120 | val_loss 2.0096 | lr 1.349280e-04\n",
      "step 35400 | train_loss 1.9134 | val_loss 2.0075 | lr 1.315639e-04\n",
      "step 35600 | train_loss 1.9111 | val_loss 2.0076 | lr 1.282320e-04\n",
      "step 35800 | train_loss 1.9104 | val_loss 2.0099 | lr 1.249328e-04\n",
      "step 36000 | train_loss 1.9098 | val_loss 2.0077 | lr 1.216669e-04\n",
      "step 36200 | train_loss 1.9073 | val_loss 2.0071 | lr 1.184348e-04\n",
      "step 36400 | train_loss 1.9049 | val_loss 2.0072 | lr 1.152369e-04\n",
      "step 36600 | train_loss 1.9076 | val_loss 2.0057 | lr 1.120739e-04\n",
      "step 36800 | train_loss 1.9107 | val_loss 2.0063 | lr 1.089461e-04\n",
      "step 37000 | train_loss 1.9110 | val_loss 2.0058 | lr 1.058542e-04\n",
      "step 37200 | train_loss 1.9112 | val_loss 2.0066 | lr 1.027985e-04\n",
      "step 37400 | train_loss 1.9091 | val_loss 2.0047 | lr 9.977952e-05\n",
      "step 37600 | train_loss 1.9104 | val_loss 2.0056 | lr 9.679781e-05\n",
      "step 37800 | train_loss 1.9079 | val_loss 2.0052 | lr 9.385380e-05\n",
      "step 38000 | train_loss 1.9031 | val_loss 2.0051 | lr 9.094796e-05\n",
      "step 38200 | train_loss 1.9089 | val_loss 2.0059 | lr 8.808075e-05\n",
      "step 38400 | train_loss 1.9070 | val_loss 2.0048 | lr 8.525262e-05\n",
      "step 38600 | train_loss 1.9098 | val_loss 2.0051 | lr 8.246402e-05\n",
      "step 38800 | train_loss 1.9143 | val_loss 2.0048 | lr 7.971538e-05\n",
      "step 39000 | train_loss 1.9101 | val_loss 2.0045 | lr 7.700715e-05\n",
      "step 39200 | train_loss 1.9139 | val_loss 2.0045 | lr 7.433974e-05\n",
      "step 39400 | train_loss 1.9161 | val_loss 2.0038 | lr 7.171358e-05\n",
      "step 39600 | train_loss 1.9200 | val_loss 2.0042 | lr 6.912909e-05\n",
      "step 39800 | train_loss 1.9183 | val_loss 2.0034 | lr 6.658668e-05\n",
      "step 40000 | train_loss 1.9185 | val_loss 2.0035 | lr 6.408673e-05\n",
      "step 40200 | train_loss 1.8915 | val_loss 2.0033 | lr 6.162966e-05\n",
      "step 40400 | train_loss 1.8936 | val_loss 2.0031 | lr 5.921584e-05\n",
      "step 40600 | train_loss 1.8968 | val_loss 2.0027 | lr 5.684566e-05\n",
      "step 40800 | train_loss 1.8995 | val_loss 2.0022 | lr 5.451949e-05\n",
      "step 41000 | train_loss 1.8998 | val_loss 2.0023 | lr 5.223771e-05\n",
      "step 41200 | train_loss 1.8994 | val_loss 2.0025 | lr 5.000066e-05\n",
      "step 41400 | train_loss 1.8980 | val_loss 2.0023 | lr 4.780871e-05\n",
      "step 41600 | train_loss 1.8982 | val_loss 2.0021 | lr 4.566220e-05\n",
      "step 41800 | train_loss 1.8995 | val_loss 2.0029 | lr 4.356146e-05\n",
      "step 42000 | train_loss 1.9006 | val_loss 2.0022 | lr 4.150684e-05\n",
      "step 42200 | train_loss 1.9017 | val_loss 2.0020 | lr 3.949865e-05\n",
      "step 42400 | train_loss 1.9007 | val_loss 2.0027 | lr 3.753721e-05\n",
      "step 42600 | train_loss 1.8990 | val_loss 2.0026 | lr 3.562284e-05\n",
      "step 42800 | train_loss 1.9003 | val_loss 2.0023 | lr 3.375582e-05\n",
      "step 43000 | train_loss 1.8988 | val_loss 2.0017 | lr 3.193647e-05\n",
      "step 43200 | train_loss 1.8992 | val_loss 2.0014 | lr 3.016506e-05\n",
      "step 43400 | train_loss 1.8993 | val_loss 2.0011 | lr 2.844188e-05\n",
      "step 43600 | train_loss 1.8993 | val_loss 2.0008 | lr 2.676720e-05\n",
      "step 43800 | train_loss 1.9007 | val_loss 2.0004 | lr 2.514128e-05\n",
      "step 44000 | train_loss 1.8988 | val_loss 2.0005 | lr 2.356438e-05\n",
      "step 44200 | train_loss 1.8999 | val_loss 2.0003 | lr 2.203674e-05\n",
      "step 44400 | train_loss 1.9004 | val_loss 2.0004 | lr 2.055862e-05\n",
      "step 44600 | train_loss 1.8992 | val_loss 2.0007 | lr 1.913024e-05\n",
      "step 44800 | train_loss 1.8987 | val_loss 2.0006 | lr 1.775182e-05\n",
      "step 45000 | train_loss 1.8994 | val_loss 2.0006 | lr 1.642360e-05\n",
      "step 45200 | train_loss 1.8996 | val_loss 2.0006 | lr 1.514577e-05\n",
      "step 45400 | train_loss 1.8999 | val_loss 2.0006 | lr 1.391853e-05\n",
      "step 45600 | train_loss 1.9002 | val_loss 2.0006 | lr 1.274209e-05\n",
      "step 45800 | train_loss 1.9002 | val_loss 2.0005 | lr 1.161662e-05\n",
      "step 46000 | train_loss 1.9007 | val_loss 2.0005 | lr 1.054231e-05\n",
      "step 46200 | train_loss 1.9018 | val_loss 2.0004 | lr 9.519326e-06\n",
      "step 46400 | train_loss 1.9020 | val_loss 2.0006 | lr 8.547826e-06\n",
      "step 46600 | train_loss 1.9021 | val_loss 2.0005 | lr 7.627965e-06\n",
      "step 46800 | train_loss 1.8997 | val_loss 2.0004 | lr 6.759889e-06\n",
      "step 47000 | train_loss 1.8992 | val_loss 2.0005 | lr 5.943734e-06\n",
      "step 47200 | train_loss 1.8993 | val_loss 2.0005 | lr 5.179630e-06\n",
      "step 47400 | train_loss 1.8996 | val_loss 2.0004 | lr 4.467698e-06\n",
      "step 47600 | train_loss 1.8998 | val_loss 2.0005 | lr 3.808049e-06\n",
      "step 47800 | train_loss 1.8999 | val_loss 2.0004 | lr 3.200788e-06\n",
      "step 48000 | train_loss 1.8997 | val_loss 2.0004 | lr 2.646010e-06\n",
      "step 48200 | train_loss 1.8997 | val_loss 2.0005 | lr 2.143805e-06\n",
      "step 48400 | train_loss 1.8998 | val_loss 2.0005 | lr 1.694249e-06\n",
      "step 48600 | train_loss 1.8998 | val_loss 2.0005 | lr 1.297416e-06\n",
      "step 48800 | train_loss 1.8998 | val_loss 2.0005 | lr 9.533665e-07\n",
      "step 49000 | train_loss 1.8998 | val_loss 2.0005 | lr 6.621559e-07\n",
      "step 49200 | train_loss 1.8998 | val_loss 2.0005 | lr 4.238300e-07\n",
      "step 49400 | train_loss 1.8998 | val_loss 2.0005 | lr 2.384263e-07\n",
      "step 49600 | train_loss 1.8998 | val_loss 2.0005 | lr 1.059742e-07\n",
      "step 49800 | train_loss 1.8998 | val_loss 2.0005 | lr 2.649460e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 22:22:46,558] Trial 19 finished with value: 2.0004928798547814 and parameters: {'embedding_size': 21, 'hidden_size': 521, 'learning_rate': 0.0006711249701238267, 'batch_size': 32, 'context_length': 5}. Best is trial 16 with value: 1.9904905743896961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49999 | train_loss 1.8998 | val_loss 2.0005 | lr 6.623738e-13\n",
      "step 0 | train_loss 3.3142 | val_loss 3.3156 | lr 8.683602e-04\n",
      "step 200 | train_loss 2.3656 | val_loss 2.4419 | lr 8.683259e-04\n",
      "step 400 | train_loss 2.3226 | val_loss 2.3692 | lr 8.682231e-04\n",
      "step 600 | train_loss 2.3220 | val_loss 2.3399 | lr 8.680517e-04\n",
      "step 800 | train_loss 2.2796 | val_loss 2.2960 | lr 8.678118e-04\n",
      "step 1000 | train_loss 2.2567 | val_loss 2.2754 | lr 8.675035e-04\n",
      "step 1200 | train_loss 2.2439 | val_loss 2.2696 | lr 8.671267e-04\n",
      "step 1400 | train_loss 2.2744 | val_loss 2.2659 | lr 8.666815e-04\n",
      "step 1600 | train_loss 2.2593 | val_loss 2.2411 | lr 8.661680e-04\n",
      "step 1800 | train_loss 2.2540 | val_loss 2.2411 | lr 8.655864e-04\n",
      "step 2000 | train_loss 2.2448 | val_loss 2.2326 | lr 8.649366e-04\n",
      "step 2200 | train_loss 2.2236 | val_loss 2.2207 | lr 8.642187e-04\n",
      "step 2400 | train_loss 2.1911 | val_loss 2.2185 | lr 8.634330e-04\n",
      "step 2600 | train_loss 2.2097 | val_loss 2.2007 | lr 8.625795e-04\n",
      "step 2800 | train_loss 2.2055 | val_loss 2.1960 | lr 8.616583e-04\n",
      "step 3000 | train_loss 2.2026 | val_loss 2.1952 | lr 8.606697e-04\n",
      "step 3200 | train_loss 2.1829 | val_loss 2.1861 | lr 8.596137e-04\n",
      "step 3400 | train_loss 2.1873 | val_loss 2.1798 | lr 8.584905e-04\n",
      "step 3600 | train_loss 2.1921 | val_loss 2.1768 | lr 8.573003e-04\n",
      "step 3800 | train_loss 2.1634 | val_loss 2.1859 | lr 8.560433e-04\n",
      "step 4000 | train_loss 2.1790 | val_loss 2.1677 | lr 8.547196e-04\n",
      "step 4200 | train_loss 2.1751 | val_loss 2.1662 | lr 8.533296e-04\n",
      "step 4400 | train_loss 2.1731 | val_loss 2.1635 | lr 8.518734e-04\n",
      "step 4600 | train_loss 2.1625 | val_loss 2.1469 | lr 8.503512e-04\n",
      "step 4800 | train_loss 2.1886 | val_loss 2.1479 | lr 8.487633e-04\n",
      "step 5000 | train_loss 2.1731 | val_loss 2.1494 | lr 8.471099e-04\n",
      "step 5200 | train_loss 2.1584 | val_loss 2.1476 | lr 8.453913e-04\n",
      "step 5400 | train_loss 2.1835 | val_loss 2.1460 | lr 8.436078e-04\n",
      "step 5600 | train_loss 2.1494 | val_loss 2.1396 | lr 8.417597e-04\n",
      "step 5800 | train_loss 2.1544 | val_loss 2.1430 | lr 8.398471e-04\n",
      "step 6000 | train_loss 2.1582 | val_loss 2.1289 | lr 8.378706e-04\n",
      "step 6200 | train_loss 2.1630 | val_loss 2.1302 | lr 8.358302e-04\n",
      "step 6400 | train_loss 2.1438 | val_loss 2.1204 | lr 8.337265e-04\n",
      "step 6600 | train_loss 2.1525 | val_loss 2.1150 | lr 8.315596e-04\n",
      "step 6800 | train_loss 1.9603 | val_loss 2.1163 | lr 8.293300e-04\n",
      "step 7000 | train_loss 2.0226 | val_loss 2.1293 | lr 8.270380e-04\n",
      "step 7200 | train_loss 2.0619 | val_loss 2.1230 | lr 8.246840e-04\n",
      "step 7400 | train_loss 2.0653 | val_loss 2.1214 | lr 8.222683e-04\n",
      "step 7600 | train_loss 2.0640 | val_loss 2.1070 | lr 8.197913e-04\n",
      "step 7800 | train_loss 2.0440 | val_loss 2.1083 | lr 8.172534e-04\n",
      "step 8000 | train_loss 2.0528 | val_loss 2.1025 | lr 8.146550e-04\n",
      "step 8200 | train_loss 2.0828 | val_loss 2.1052 | lr 8.119966e-04\n",
      "step 8400 | train_loss 2.0832 | val_loss 2.1135 | lr 8.092785e-04\n",
      "step 8600 | train_loss 2.0780 | val_loss 2.0968 | lr 8.065011e-04\n",
      "step 8800 | train_loss 2.0668 | val_loss 2.0974 | lr 8.036650e-04\n",
      "step 9000 | train_loss 2.0695 | val_loss 2.0973 | lr 8.007705e-04\n",
      "step 9200 | train_loss 2.0580 | val_loss 2.1019 | lr 7.978181e-04\n",
      "step 9400 | train_loss 2.0716 | val_loss 2.0970 | lr 7.948083e-04\n",
      "step 9600 | train_loss 2.0657 | val_loss 2.0956 | lr 7.917416e-04\n",
      "step 9800 | train_loss 2.0857 | val_loss 2.0944 | lr 7.886184e-04\n",
      "step 10000 | train_loss 2.0804 | val_loss 2.1020 | lr 7.854392e-04\n",
      "step 10200 | train_loss 2.0826 | val_loss 2.0936 | lr 7.822045e-04\n",
      "step 10400 | train_loss 2.0920 | val_loss 2.0926 | lr 7.789149e-04\n",
      "step 10600 | train_loss 2.0728 | val_loss 2.0839 | lr 7.755709e-04\n",
      "step 10800 | train_loss 2.0813 | val_loss 2.0844 | lr 7.721729e-04\n",
      "step 11000 | train_loss 2.0736 | val_loss 2.0902 | lr 7.687216e-04\n",
      "step 11200 | train_loss 2.0695 | val_loss 2.0819 | lr 7.652175e-04\n",
      "step 11400 | train_loss 2.0965 | val_loss 2.0803 | lr 7.616610e-04\n",
      "step 11600 | train_loss 2.0900 | val_loss 2.0856 | lr 7.580529e-04\n",
      "step 11800 | train_loss 2.0839 | val_loss 2.0812 | lr 7.543936e-04\n",
      "step 12000 | train_loss 2.0859 | val_loss 2.0848 | lr 7.506838e-04\n",
      "step 12200 | train_loss 2.0743 | val_loss 2.0797 | lr 7.469240e-04\n",
      "step 12400 | train_loss 2.0791 | val_loss 2.0795 | lr 7.431147e-04\n",
      "step 12600 | train_loss 2.0783 | val_loss 2.0750 | lr 7.392567e-04\n",
      "step 12800 | train_loss 2.1147 | val_loss 2.0791 | lr 7.353506e-04\n",
      "step 13000 | train_loss 2.0851 | val_loss 2.0757 | lr 7.313968e-04\n",
      "step 13200 | train_loss 2.0704 | val_loss 2.0666 | lr 7.273962e-04\n",
      "step 13400 | train_loss 1.8409 | val_loss 2.0647 | lr 7.233492e-04\n",
      "step 13600 | train_loss 1.8982 | val_loss 2.0681 | lr 7.192566e-04\n",
      "step 13800 | train_loss 1.9449 | val_loss 2.0786 | lr 7.151189e-04\n",
      "step 14000 | train_loss 1.9581 | val_loss 2.0597 | lr 7.109369e-04\n",
      "step 14200 | train_loss 1.9686 | val_loss 2.0706 | lr 7.067112e-04\n",
      "step 14400 | train_loss 1.9821 | val_loss 2.0655 | lr 7.024425e-04\n",
      "step 14600 | train_loss 1.9582 | val_loss 2.0638 | lr 6.981313e-04\n",
      "step 14800 | train_loss 1.9863 | val_loss 2.0544 | lr 6.937786e-04\n",
      "step 15000 | train_loss 2.0035 | val_loss 2.0635 | lr 6.893848e-04\n",
      "step 15200 | train_loss 2.0062 | val_loss 2.0577 | lr 6.849507e-04\n",
      "step 15400 | train_loss 2.0060 | val_loss 2.0524 | lr 6.804770e-04\n",
      "step 15600 | train_loss 1.9883 | val_loss 2.0559 | lr 6.759644e-04\n",
      "step 15800 | train_loss 1.9849 | val_loss 2.0614 | lr 6.714137e-04\n",
      "step 16000 | train_loss 2.0059 | val_loss 2.0594 | lr 6.668254e-04\n",
      "step 16200 | train_loss 1.9993 | val_loss 2.0525 | lr 6.622005e-04\n",
      "step 16400 | train_loss 2.0015 | val_loss 2.0527 | lr 6.575395e-04\n",
      "step 16600 | train_loss 2.0093 | val_loss 2.0556 | lr 6.528433e-04\n",
      "step 16800 | train_loss 2.0275 | val_loss 2.0504 | lr 6.481125e-04\n",
      "step 17000 | train_loss 2.0132 | val_loss 2.0500 | lr 6.433480e-04\n",
      "step 17200 | train_loss 1.9950 | val_loss 2.0530 | lr 6.385504e-04\n",
      "step 17400 | train_loss 2.0136 | val_loss 2.0482 | lr 6.337205e-04\n",
      "step 17600 | train_loss 2.0164 | val_loss 2.0474 | lr 6.288592e-04\n",
      "step 17800 | train_loss 2.0097 | val_loss 2.0541 | lr 6.239671e-04\n",
      "step 18000 | train_loss 2.0177 | val_loss 2.0452 | lr 6.190450e-04\n",
      "step 18200 | train_loss 2.0332 | val_loss 2.0501 | lr 6.140937e-04\n",
      "step 18400 | train_loss 2.0292 | val_loss 2.0488 | lr 6.091141e-04\n",
      "step 18600 | train_loss 2.0228 | val_loss 2.0508 | lr 6.041068e-04\n",
      "step 18800 | train_loss 2.0253 | val_loss 2.0473 | lr 5.990726e-04\n",
      "step 19000 | train_loss 2.0215 | val_loss 2.0481 | lr 5.940125e-04\n",
      "step 19200 | train_loss 2.0235 | val_loss 2.0511 | lr 5.889270e-04\n",
      "step 19400 | train_loss 2.0300 | val_loss 2.0413 | lr 5.838172e-04\n",
      "step 19600 | train_loss 2.0254 | val_loss 2.0416 | lr 5.786837e-04\n",
      "step 19800 | train_loss 2.0187 | val_loss 2.0453 | lr 5.735274e-04\n",
      "step 20000 | train_loss 2.0227 | val_loss 2.0339 | lr 5.683491e-04\n",
      "step 20200 | train_loss 1.8463 | val_loss 2.0396 | lr 5.631496e-04\n",
      "step 20400 | train_loss 1.8804 | val_loss 2.0438 | lr 5.579298e-04\n",
      "step 20600 | train_loss 1.9028 | val_loss 2.0418 | lr 5.526904e-04\n",
      "step 20800 | train_loss 1.9214 | val_loss 2.0431 | lr 5.474323e-04\n",
      "step 21000 | train_loss 1.9122 | val_loss 2.0353 | lr 5.421563e-04\n",
      "step 21200 | train_loss 1.8977 | val_loss 2.0320 | lr 5.368633e-04\n",
      "step 21400 | train_loss 1.9080 | val_loss 2.0362 | lr 5.315540e-04\n",
      "step 21600 | train_loss 1.9363 | val_loss 2.0329 | lr 5.262294e-04\n",
      "step 21800 | train_loss 1.9513 | val_loss 2.0393 | lr 5.208902e-04\n",
      "step 22000 | train_loss 1.9487 | val_loss 2.0274 | lr 5.155373e-04\n",
      "step 22200 | train_loss 1.9374 | val_loss 2.0295 | lr 5.101716e-04\n",
      "step 22400 | train_loss 1.9345 | val_loss 2.0355 | lr 5.047939e-04\n",
      "step 22600 | train_loss 1.9329 | val_loss 2.0294 | lr 4.994051e-04\n",
      "step 22800 | train_loss 1.9364 | val_loss 2.0287 | lr 4.940059e-04\n",
      "step 23000 | train_loss 1.9475 | val_loss 2.0277 | lr 4.885973e-04\n",
      "step 23200 | train_loss 1.9573 | val_loss 2.0285 | lr 4.831801e-04\n",
      "step 23400 | train_loss 1.9641 | val_loss 2.0305 | lr 4.777552e-04\n",
      "step 23600 | train_loss 1.9614 | val_loss 2.0269 | lr 4.723233e-04\n",
      "step 23800 | train_loss 1.9567 | val_loss 2.0257 | lr 4.668855e-04\n",
      "step 24000 | train_loss 1.9607 | val_loss 2.0268 | lr 4.614425e-04\n",
      "step 24200 | train_loss 1.9599 | val_loss 2.0248 | lr 4.559952e-04\n",
      "step 24400 | train_loss 1.9584 | val_loss 2.0305 | lr 4.505444e-04\n",
      "step 24600 | train_loss 1.9504 | val_loss 2.0239 | lr 4.450911e-04\n",
      "step 24800 | train_loss 1.9708 | val_loss 2.0226 | lr 4.396360e-04\n",
      "step 25000 | train_loss 1.9654 | val_loss 2.0234 | lr 4.341801e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-07-19 22:23:21,219] Trial 20 failed with parameters: {'embedding_size': 32, 'hidden_size': 654, 'learning_rate': 0.0008683602070624887, 'batch_size': 32, 'context_length': 4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50196\\.conda\\envs\\deeplearning\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\50196\\AppData\\Local\\Temp\\ipykernel_46372\\779706708.py\", line 23, in objective\n",
      "    val_loss = train(model, train_tokens, val_tokens, context_length, batch_size, num_steps, learning_rate, device)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\50196\\AppData\\Local\\Temp\\ipykernel_46372\\2298479787.py\", line 15, in train\n",
      "    loss.backward()\n",
      "  File \"C:\\Users\\50196\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\_tensor.py\", line 525, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\Users\\50196\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 267, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"C:\\Users\\50196\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 744, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-07-19 22:23:21,224] Trial 20 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25200 | train_loss 1.9767 | val_loss 2.0272 | lr 4.287242e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m      2\u001B[0m     study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminimize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m     study\u001B[38;5;241m.\u001B[39moptimize(objective, n_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBest trial:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m     trial \u001B[38;5;241m=\u001B[39m study\u001B[38;5;241m.\u001B[39mbest_trial\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\optuna\\study\\study.py:451\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    350\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    357\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    358\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \n\u001B[0;32m    361\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 451\u001B[0m     _optimize(\n\u001B[0;32m    452\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    453\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m    454\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[0;32m    455\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    456\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[0;32m    457\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[0;32m    458\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    459\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[0;32m    460\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[0;32m    461\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 62\u001B[0m         _optimize_sequential(\n\u001B[0;32m     63\u001B[0m             study,\n\u001B[0;32m     64\u001B[0m             func,\n\u001B[0;32m     65\u001B[0m             n_trials,\n\u001B[0;32m     66\u001B[0m             timeout,\n\u001B[0;32m     67\u001B[0m             catch,\n\u001B[0;32m     68\u001B[0m             callbacks,\n\u001B[0;32m     69\u001B[0m             gc_after_trial,\n\u001B[0;32m     70\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     71\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     72\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[0;32m     73\u001B[0m         )\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     75\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    156\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 159\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    240\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    243\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    246\u001B[0m ):\n\u001B[1;32m--> 247\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 196\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[0;32m    197\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    198\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    199\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[9], line 23\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m     22\u001B[0m num_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50000\u001B[39m\n\u001B[1;32m---> 23\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m train(model, train_tokens, val_tokens, context_length, batch_size, num_steps, learning_rate, device)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m val_loss\n",
      "Cell \u001B[1;32mIn[7], line 15\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_tokens, val_tokens, context_length, batch_size, num_steps, learning_rate, device)\u001B[0m\n\u001B[0;32m     13\u001B[0m inputs, targets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(train_data_iter)\n\u001B[0;32m     14\u001B[0m logits, loss \u001B[38;5;241m=\u001B[39m model(inputs, targets)\n\u001B[1;32m---> 15\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    527\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m _engine_run_backward(\n\u001B[0;32m    268\u001B[0m     tensors,\n\u001B[0;32m    269\u001B[0m     grad_tensors_,\n\u001B[0;32m    270\u001B[0m     retain_graph,\n\u001B[0;32m    271\u001B[0m     create_graph,\n\u001B[0;32m    272\u001B[0m     inputs,\n\u001B[0;32m    273\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    274\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    275\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
