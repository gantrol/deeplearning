1
00:00:00,000 --> 00:00:00,056
Hi everyone.

2
00:00:01,046 --> 00:00:07,060
So today we are going to be continuing our Zero to Hero series, and in particular, today we are going to reproduce the GPT-2 model,

3
00:00:07,098 --> 00:00:10,072
the 124 million version of it.

4
00:00:11,070 --> 00:00:17,060
So when OpenAI released GPT-2, this was 2019, and they released it with this blog post.

5
00:00:18,002 --> 00:00:22,086
On top of that, they released this paper, and on top of that, they released this code on GitHub.

6
00:00:23,020 --> 00:00:24,060
So OpenAI slash GPT-2.

7
00:00:25,012 --> 00:00:30,028
Now, when we talk about reproducing GPT-2, we have to be careful, because in particular in this video,

8
00:00:30,060 --> 00:00:33,066
we are going to be reproducing the 124 million parameter model.

9
00:00:34,038 --> 00:00:39,034
So the thing to realize is that there is always a mini-series when these releases are made.

10
00:00:39,074 --> 00:00:46,084
So there are the GPT-2 mini-series made up of models at different sizes, and usually the biggest model is called the GPT-2.

11
00:00:47,080 --> 00:00:53,012
But basically the reason we do that is because you can put the model sizes on the x-axis of plots like this,

12
00:00:53,060 --> 00:00:58,018
and on the y-axis you put a lot of downstream metrics that you are interested in, like translation, summarization,

13
00:00:58,040 --> 00:01:01,066
question answering, and so on, and you can chart out these scaling laws.

14
00:01:02,036 --> 00:01:06,084
So basically as the model size increases, you're getting better and better at downstream metrics.

15
00:01:07,074 --> 00:01:15,024
And so in particular for GPT-2, if we scroll down in the paper, there are four models in the GPT-2 mini-series,

16
00:01:15,052 --> 00:01:20,054
starting at 124 million all the way up to 1558 million.

17
00:01:21,014 --> 00:01:25,046
Now, the reason my numbers, the way I say them, disagree with this table is that this table is wrong.

18
00:01:25,092 --> 00:01:34,028
If you actually go to the GPT-2 GitHub repo, they sort of say that there was an error in how they added up the parameters,

19
00:01:34,057 --> 00:01:37,038
but basically this is the 124 million parameter model, etc.

20
00:01:38,002 --> 00:01:45,060
So the 124 million parameter had 12 layers in the transformer, and it had 768 channels in the transformer,

21
00:01:45,096 --> 00:01:46,094
768 dimensions.

22
00:01:47,057 --> 00:01:52,046
And I'm going to be assuming some familiarity with what these terms mean, because I covered all of this in my previous video,

23
00:01:52,068 --> 00:01:55,084
Let's Build GPT-2, Let's Build GPT from Scratch.

24
00:01:56,018 --> 00:01:58,038
So I covered that in the previous video in this playlist.

25
00:01:59,006 --> 00:02:04,098
Now, if we do everything correctly and everything works out well, by the end of this video we're going to see something like this,

26
00:02:05,054 --> 00:02:14,088
where we're looking at the validation loss, which basically measures how good we are at predicting the next token in a sequence on some validation data that

27
00:02:14,088 --> 00:02:16,016
the model has not seen during training.

28
00:02:16,086 --> 00:02:22,096
And we see that we go from doing that task not very well, because we're initializing from scratch, all the way to doing that

29
00:02:22,096 --> 00:02:25,062
task quite well by the end of the training.

30
00:02:26,014 --> 00:02:29,096
And hopefully we're going to beat the GPT-2 124M model.

31
00:02:30,070 --> 00:02:37,084
Now, previously when they were working on this, this is already five years ago, so this was probably a fairly complicated optimization at the time,

32
00:02:37,084 --> 00:02:40,032
and the GPUs and the compute was a lot smaller.

33
00:02:40,074 --> 00:02:49,046
Today you can reproduce this model in roughly an hour, or probably less even, and it will cost you about 10 bucks if you want to do this on the cloud compute,

34
00:02:49,088 --> 00:02:51,086
a sort of computer that you can all rent.

35
00:02:52,044 --> 00:02:58,010
And if you pay $10 for that computer, you wait about an hour or less, you can actually achieve a model that

36
00:02:58,010 --> 00:03:00,054
is as good as this model that OpenAI released.

37
00:03:01,050 --> 00:03:07,096
And one more thing to mention is, unlike many other models, OpenAI did release the weights for GPT-2,

38
00:03:08,038 --> 00:03:10,022
so those weights are all available in this repository.

39
00:03:11,018 --> 00:03:15,068
But the GPT-2 paper is not always as good with all of the details of training.

40
00:03:16,034 --> 00:03:24,032
So in addition to the GPT-2 paper, we're going to be referencing the GPT-3 paper, which is a lot more concrete in a lot of the parameters and

41
00:03:24,032 --> 00:03:31,052
optimization settings and so on, and it's not a huge departure in the architecture from the GPT-2 version of the model.

42
00:03:31,090 --> 00:03:37,014
So we're going to be referencing both GPT-2 and GPT-3 as we try to reproduce GPT-2 124M.

43
00:03:37,060 --> 00:03:38,056
So let's go.

44
00:03:39,030 --> 00:03:42,072
So the first thing I would like to do is actually start at the end, or at the target.

45
00:03:43,020 --> 00:03:49,060
So in other words, let's load the GPT-2 124M model as it was released by OpenAI, and maybe take it for a spin.

46
00:03:49,070 --> 00:03:51,002
Let's sample some tokens from it.

47
00:03:51,052 --> 00:03:57,094
Now, the issue with that is, when you go to the code base of GPT-2 and you go into the source, and you click in on the model.py,

48
00:03:58,020 --> 00:04:00,057
you'll realize that actually this is using TensorFlow.

49
00:04:01,054 --> 00:04:10,057
So the original GPT-2 code here was written in TensorFlow, which is, you know, not, let's just say, not used as much anymore.

50
00:04:11,048 --> 00:04:16,016
So we'd like to use PyTorch, because it's a lot friendlier, easier, and I just personally like it a lot more.

51
00:04:16,064 --> 00:04:18,068
The problem with that is the initial code is in TensorFlow.

52
00:04:19,012 --> 00:04:20,006
We'd like to use PyTorch.

53
00:04:20,038 --> 00:04:27,038
So instead, to get the target, we're going to use the Hugging Face Transformers code, which I like a lot more.

54
00:04:27,098 --> 00:04:35,096
So when you go into the transformer source, transformers, models, GPT-2, you will see that they have the GPT-2 implementation of that

55
00:04:35,096 --> 00:04:37,054
transformer here in this file.

56
00:04:38,084 --> 00:04:43,086
And it's like medium readable, but not fully readable.

57
00:04:45,012 --> 00:04:51,086
But what it does is it did all the work of converting all those weights from TensorFlow to PyTorch-friendly,

58
00:04:51,088 --> 00:04:53,074
and so it's much easier to load and work with.

59
00:04:54,036 --> 00:05:00,092
So in particular, we can look at the GPT-2 model here, and we can load it using Hugging Face Transformers.

60
00:05:01,046 --> 00:05:03,057
So swinging over, this is what that looks like.

61
00:05:04,092 --> 00:05:11,082
From transformers, import the GPT-2 LM head model, and then from pre-trained GPT-2.

62
00:05:13,026 --> 00:05:20,084
Now, one awkward thing about this is that when you do GPT-2 as the model that we're loading, this actually is the 124 million parameter model.

63
00:05:21,040 --> 00:05:27,028
If you want the actual, the GPT-2, the 1.5 billion, then you actually want to do dash XL.

64
00:05:28,000 --> 00:05:30,066
So this is the 124M, our target.

65
00:05:31,084 --> 00:05:38,030
Now what we're doing is when we actually get this, we're initializing the PyTorch NN module as defined here in this class.

66
00:05:39,070 --> 00:05:43,086
From it, I want to get just the state dict, which is just the raw tensors.

67
00:05:44,022 --> 00:05:46,090
So we just have the tensors of that file.

68
00:05:47,096 --> 00:05:52,074
And by the way, here, this is a Jupyter notebook, but this is Jupyter notebook running inside VS Code.

69
00:05:53,048 --> 00:05:58,004
So I like to work with it all in a single interface, so I like to use VS Code.

70
00:05:58,024 --> 00:06:02,056
So this is the Jupyter notebook extension inside VS Code.

71
00:06:03,048 --> 00:06:06,086
So when we get the state dict, this is just a dict.

72
00:06:07,018 --> 00:06:12,062
So we can print the key and the value, which is the tensor, and let's just look at the shapes.

73
00:06:13,044 --> 00:06:19,098
So these are the different parameters inside the GPT-2 model and their shape.

74
00:06:20,057 --> 00:06:29,046
So the W weight for token embedding is of size 50,257 by 768.

75
00:06:29,098 --> 00:06:35,086
Where this is coming from is that we have 50,257 tokens in the GPT-2 vocabulary.

76
00:06:37,046 --> 00:06:43,094
And the tokens, by the way, these are exactly the tokens that we've spoken about in the previous video on my tokenization series.

77
00:06:44,057 --> 00:06:48,004
So the previous video, just before this, I go into a ton of detail on tokenization.

78
00:06:48,060 --> 00:06:51,016
GPT-2 tokenizer happens to have this many tokens.

79
00:06:51,078 --> 00:07:01,038
For each token, we have a 768-dimensional embedding that is the distributed representation that stands in for that

80
00:07:01,038 --> 00:07:01,070
token.

81
00:07:02,026 --> 00:07:08,040
So each token is a little string piece, and then these 768 numbers are the vector that represents that

82
00:07:08,040 --> 00:07:08,074
token.

83
00:07:09,098 --> 00:07:11,072
And so this is just our lookup table for tokens.

84
00:07:12,010 --> 00:07:15,026
And then here, we have the lookup table for the positions.

85
00:07:15,086 --> 00:07:25,078
So because GPT-2 has a maximum sequence length of 1024, we have up to 1024 positions that each token can be attending to in the past.

86
00:07:26,052 --> 00:07:33,078
And every one of those positions in GPT-2 has a fixed vector of 768 that is learned by optimization.

87
00:07:34,086 --> 00:07:38,048
And so this is the position embedding and the token embedding.

88
00:07:39,054 --> 00:07:44,057
And then everything here is just the other weights and biases and everything else of this transformer.

89
00:07:45,094 --> 00:07:51,044
So when you just take, for example, the positional embeddings and flatten it out and take just the 20 elements,

90
00:07:51,074 --> 00:07:53,048
you can see that these are just the parameters.

91
00:07:53,066 --> 00:07:58,056
These are weights, floats, just we can take and we can plot them.

92
00:07:58,094 --> 00:08:00,080
So these are the position embeddings.

93
00:08:01,066 --> 00:08:02,068
And we get something like this.

94
00:08:02,076 --> 00:08:03,096
And you can see that this has structure.

95
00:08:04,060 --> 00:08:12,016
And it has structure because what we have here really is every row in this visualization is a different position,

96
00:08:12,076 --> 00:08:17,062
a fixed absolute position in the range from 0 to 1024.

97
00:08:18,026 --> 00:08:21,050
And each row here is the representation of that position.

98
00:08:22,057 --> 00:08:29,062
And so it has structure because these positional embeddings end up learning these sinusoids and cosines that

99
00:08:29,062 --> 00:08:32,026
sort of like represent each of these positions.

100
00:08:32,074 --> 00:08:40,054
And each row here stands in for that position and is processed by the transformer to recover all the relative positions and

101
00:08:40,054 --> 00:08:46,074
sort of realize which token is where and attend to them depending on their position, not just their content.

102
00:08:47,082 --> 00:08:55,004
So when we actually just look into an individual column inside these, and I just grabbed three random columns,

103
00:08:55,048 --> 00:09:01,092
you'll see that, for example, here we are focusing on every single channel.

104
00:09:02,036 --> 00:09:13,030
And we're looking at what that channel is doing as a function of position from 0 to 1023 really.

105
00:09:14,024 --> 00:09:20,028
And we can see that some of these channels basically respond more or less to different parts of the position spectrum.

106
00:09:20,066 --> 00:09:30,022
So this green channel really likes to fire for everything after 200 up to 800, but not less, but a lot less,

107
00:09:30,032 --> 00:09:32,022
and has a sharp drop-off here near 0.

108
00:09:32,076 --> 00:09:35,082
So who knows what these embeddings are doing and why they are the way they are.

109
00:09:36,028 --> 00:09:40,036
You can tell, for example, that because they're a bit more jagged and they're kind of noisy, you can tell that

110
00:09:40,036 --> 00:09:41,090
this model was not fully trained.

111
00:09:42,034 --> 00:09:46,082
And the more trained this model was, the more you would expect to smooth this out.

112
00:09:47,034 --> 00:09:50,000
And so this is telling you that this is a little bit of an under-trained model.

113
00:09:51,026 --> 00:09:54,062
But in principle, actually, these curves don't even have to be smooth.

114
00:09:54,076 --> 00:09:56,070
This should just be totally random noise.

115
00:09:57,010 --> 00:10:04,000
And in fact, in the beginning of the optimization, it is complete random noise because this position embedding table is initialized completely at random.

116
00:10:04,048 --> 00:10:06,004
So in the beginning, you have jaggedness.

117
00:10:06,088 --> 00:10:13,008
And the fact that you end up with something smooth is already kind of impressive, that that just falls out of the optimization.

118
00:10:13,048 --> 00:10:17,066
Because in principle, you shouldn't even be able to get any single graph out of this that makes sense.

119
00:10:17,096 --> 00:10:22,056
But we actually get something that looks a little bit noisy, but for the most part looks sinusoidal-like.

120
00:10:23,016 --> 00:10:31,084
In the original transformer paper, the attention is all you need paper, the positional embeddings are actually initialized and

121
00:10:31,084 --> 00:10:36,080
fixed, if I remember correctly, to sinusoids and cosines of different frequencies.

122
00:10:37,034 --> 00:10:38,098
And that's the positional encoding, and it's fixed.

123
00:10:39,044 --> 00:10:43,092
But in GPT-2, these are just parameters, and they're trained from scratch, just like any other parameter.

124
00:10:44,046 --> 00:10:46,044
And that seems to work about as well.

125
00:10:46,082 --> 00:10:51,026
And so what they do is they kind of like recover these sinusoidal-like features during the optimization.

126
00:10:52,054 --> 00:10:55,072
We can also look at any of the other matrices here.

127
00:10:56,000 --> 00:11:06,020
So here I took the first layer of the transformer and looking at one of its weights and just the first block of 300x300,

128
00:11:07,008 --> 00:11:08,034
and you see some structure.

129
00:11:08,072 --> 00:11:11,036
But again, who knows what any of this is?

130
00:11:11,066 --> 00:11:16,056
If you're into mechanistic interoperability, you might get a real kick out of trying to figure out what

131
00:11:16,056 --> 00:11:19,030
is going on, what is this structure, and what does this all mean.

132
00:11:19,060 --> 00:11:21,018
But we're not going to be doing that in this video.

133
00:11:21,064 --> 00:11:25,010
But we definitely see that there's some interesting structure, and that's kind of cool.

134
00:11:25,080 --> 00:11:29,062
What we're mostly interested in is we've loaded the weights of this model that was released by OpenAI,

135
00:11:30,012 --> 00:11:36,057
and now using the Hugging Face transformers, we can not just get all the raw weights, but we can also

136
00:11:36,057 --> 00:11:40,057
get the what they call pipeline and sample from it.

137
00:11:41,008 --> 00:11:50,050
So this is the prefix, hello, I'm a language model, comma, and then we're sampling 30 tokens, and we're getting five sequences,

138
00:11:50,092 --> 00:11:53,008
and I ran this, and this is what it produced.

139
00:11:54,016 --> 00:11:59,000
Hello, I'm a language model, but what I'm really doing is making a human-readable document.

140
00:11:59,030 --> 00:12:02,000
There are other languages, but those are dot dot dot.

141
00:12:02,046 --> 00:12:09,066
So you can read through these if you like, but basically these are five different completions of the same prefix from this GPT2124M.

142
00:12:10,014 --> 00:12:22,026
Now, if I go here, I took this example from here, and sadly, even though we are fixing the seed, we are getting different generations from the snippet than what

143
00:12:22,026 --> 00:12:22,084
they got.

144
00:12:23,040 --> 00:12:31,006
So presumably the code changed, but what we see though at this stage that's important is that we are getting coherent text.

145
00:12:31,062 --> 00:12:33,076
So we've loaded the model successfully.

146
00:12:34,002 --> 00:12:42,090
We can look at all its parameters, and the keys tell us where in the model these come from, and we want to actually write our own GPT2 class so

147
00:12:42,090 --> 00:12:44,068
that we have full understanding of what's happening there.

148
00:12:44,092 --> 00:12:50,030
We don't want to be working with something like the modeling GPT2.py, because it's just too complicated.

149
00:12:50,042 --> 00:12:52,004
We want to write this from scratch ourselves.

150
00:12:52,048 --> 00:13:02,002
So we're going to be implementing the GPT model here in parallel, and as our first task, let's load the GPT2 on 24M into the class that

151
00:13:02,002 --> 00:13:03,078
we are going to develop here from scratch.

152
00:13:03,080 --> 00:13:11,010
That's going to give us confidence that we can load the OpenAI model, and therefore there's a setting of weights that

153
00:13:11,010 --> 00:13:16,068
exactly is the 124 model, but then of course what we're going to do is we're going to initialize the model from scratch instead,

154
00:13:17,010 --> 00:13:23,054
and try to train it ourselves on a bunch of documents that we're going to get, and we're going to try to surpass that

155
00:13:23,054 --> 00:13:23,076
model.

156
00:13:24,030 --> 00:13:28,010
So we're going to get different weights, and everything's going to look different, hopefully better even,

157
00:13:28,074 --> 00:13:35,012
but we're going to have a lot of confidence that because we can load the OpenAI model, we are in the same model family and

158
00:13:35,012 --> 00:13:39,034
model class, and we just have to rediscover a good setting of the weights, but from scratch.

159
00:13:39,094 --> 00:13:46,086
So let's now write the GPT2 model, and let's load the weights, and make sure that we can also generate text that

160
00:13:46,086 --> 00:13:47,034
looks coherent.

161
00:13:48,014 --> 00:13:53,082
Okay, so let's now swing over to the attention is all you need paper that started everything, and let's scroll over to the model architecture,

162
00:13:54,006 --> 00:13:54,092
the original transformer.

163
00:13:55,078 --> 00:13:59,074
Now remember that GPT2 is slightly modified from the original transformer.

164
00:14:00,016 --> 00:14:03,030
In particular, we do not have the encoder.

165
00:14:03,068 --> 00:14:06,044
GPT2 is a decoder-only transformer, as we call it.

166
00:14:06,068 --> 00:14:12,084
So this entire encoder here is missing, and in addition to that, this cross attention here that was using that

167
00:14:12,084 --> 00:14:16,060
encoder is also missing, so we delete this entire part.

168
00:14:17,042 --> 00:14:23,016
Everything else stays almost the same, but there are some differences that we're going to sort of look at here.

169
00:14:23,068 --> 00:14:25,086
So there are two main differences.

170
00:14:27,056 --> 00:14:34,064
When we go to the GPT2 paper under 2.3.model, we notice that first there's a reshuffling of the layer norms,

171
00:14:34,078 --> 00:14:43,046
so they change place, and second, an additional layer normalization was added here to the final self-attention block.

172
00:14:43,082 --> 00:14:50,020
So basically, all the layer norms here, instead of being after the MLP or after the attention, they swing before it,

173
00:14:50,046 --> 00:14:54,006
and an additional layer norm gets added here right before the final classifier.

174
00:14:55,020 --> 00:15:03,026
So now let's implement some of the first sort of skeleton NN modules here in our GPT NN module, and in particular,

175
00:15:03,040 --> 00:15:08,057
we're going to try to match up this schema here that is used by Hugging Face Transformers, because that

176
00:15:08,057 --> 00:15:11,078
will make it much easier to load these weights from this state dict.

177
00:15:12,044 --> 00:15:15,074
So we want something that reflects this schema here.

178
00:15:16,052 --> 00:15:17,052
So here's what I came up with.

179
00:15:20,094 --> 00:15:26,094
Basically, we see that the main container here that has all the modules is called transformer, so I'm reflecting that

180
00:15:26,094 --> 00:15:34,008
with an NN module dict, and this is basically a module that allows you to index into the submodules using keys,

181
00:15:34,032 --> 00:15:36,014
just like a dictionary, strings.

182
00:15:36,086 --> 00:15:44,084
Within it, we have the weights of the token embeddings, WT, and that's an NN embedding, and the weights of the position embeddings,

183
00:15:44,094 --> 00:15:54,094
which is also just an NN embedding, and if you remember, an NN embedding is really just a fancy little wrapper module around just a single array of numbers,

184
00:15:55,010 --> 00:15:58,042
a single block of numbers just like this.

185
00:15:58,054 --> 00:16:08,048
It's a single tensor, and an NN embedding is a glorified wrapper around a tensor that allows you to access its elements by indexing into the rows.

186
00:16:09,054 --> 00:16:17,066
Now, in addition to that, we see here that we have a .h, and then this is indexed using numbers instead of indexed using strings,

187
00:16:18,002 --> 00:16:26,076
so there's a .h, .0, 1, 2, etc., all the way up to .h.11, and that's because there are 12 layers here in this transformer.

188
00:16:27,050 --> 00:16:34,020
So to reflect that, I'm creating also an h, I think that probably stands for hidden, and instead of a module dict,

189
00:16:34,036 --> 00:16:45,018
this is a model list, so we can index it using integers exactly as we see here, .0, .1, 2, etc., and the module list has N layer blocks,

190
00:16:45,050 --> 00:16:48,048
and the blocks are yet to be defined in a module in a bit.

191
00:16:49,042 --> 00:16:56,062
In addition to that, following the GPT-2 paper, we need an additional final layer norm that we're going to put in there,

192
00:16:56,076 --> 00:17:07,028
and then we have the final classifier, the language model head, which projects from 768, the number of embedding dimensions in this GPT,

193
00:17:07,052 --> 00:17:15,030
all the way to the vocab size, which is 50,257, and GPT-2 uses no bias for this final sort of projection.

194
00:17:16,012 --> 00:17:25,024
So this is the skeleton, and you can see that it reflects this, so the WTE is the token embeddings, here it's called output embedding,

195
00:17:25,028 --> 00:17:31,054
but it's really the token embeddings, the PE is the positional encodings, those two pieces of information,

196
00:17:31,057 --> 00:17:34,022
as we saw previously, are going to add, and then go into the transformer.

197
00:17:34,098 --> 00:17:42,092
The .h is the, all the blocks in gray, and the LNF is this new layer that gets added here by the GPT-2 model,

198
00:17:43,028 --> 00:17:46,018
and LM head is this linear part here.

199
00:17:47,014 --> 00:17:51,066
So that's the skeleton of the GPT-2, we now have to implement the block.

200
00:17:52,068 --> 00:17:54,090
Okay, so let's now recurse to the block itself.

201
00:17:55,034 --> 00:17:56,046
So we want to define the block.

202
00:17:58,030 --> 00:17:59,082
So I'll start putting them here.

203
00:18:00,054 --> 00:18:03,072
So the block, I like to write out like this.

204
00:18:05,064 --> 00:18:09,060
These are some of the initializations, and then this is the actual forward pass of what this block computes.

205
00:18:10,054 --> 00:18:15,080
And notice here that there's a change from the transformer again, that is mentioned in the GPT-2 paper.

206
00:18:16,056 --> 00:18:21,062
So here, the layer normalizations are after the application of attention, or feed forward.

207
00:18:22,000 --> 00:18:28,057
In addition to that note, that the normalizations are inside the residual stream, you see how feed forward is applied,

208
00:18:28,078 --> 00:18:31,090
and this arrow goes through and through the normalization.

209
00:18:32,064 --> 00:18:36,057
So that means that your residual pathway has normalizations inside them.

210
00:18:36,092 --> 00:18:38,094
And this is not very good or desirable.

211
00:18:39,044 --> 00:18:46,026
You actually prefer to have a single, clean residual stream, all the way from supervision, all the way down to the inputs,

212
00:18:46,040 --> 00:18:46,080
the tokens.

213
00:18:47,060 --> 00:18:54,096
And this is very desirable and nice, because the gradients that flow from the top, if you remember from your micrograd,

214
00:18:55,042 --> 00:19:00,090
addition just distributes gradients during the backward stage to both of its branches equally.

215
00:19:01,038 --> 00:19:04,056
So addition is a branch in the gradients.

216
00:19:05,006 --> 00:19:12,048
And so that means that the gradients from the top flow straight to the inputs, the tokens, through the residual pathways unchanged.

217
00:19:13,006 --> 00:19:19,012
But then in addition to that, the gradient also flows through the blocks, and the blocks, you know, contribute their own contribution over time,

218
00:19:19,024 --> 00:19:21,004
and kick in and change the optimization over time.

219
00:19:21,086 --> 00:19:26,044
But basically, clean residual pathway is desirable from an optimization perspective.

220
00:19:27,032 --> 00:19:33,072
And then this is the pre-normalization version, where you see that Rx first goes through the layer normalization,

221
00:19:34,006 --> 00:19:42,002
and then the attention, and then goes back out to go to the layer normalization number two, and the multilayer perceptron,

222
00:19:42,048 --> 00:19:45,044
sometimes also referred to as a feed forward network, or an FFM.

223
00:19:46,026 --> 00:19:48,066
And then that goes into the residual stream again.

224
00:19:49,092 --> 00:19:54,080
And the one more thing that is kind of interesting to note is that recall that attention is a communication operation.

225
00:19:55,028 --> 00:20:01,078
It is where all the tokens, and there's 1024 tokens lined up in a sequence, and this is where the tokens communicate.

226
00:20:01,094 --> 00:20:03,028
This is where they exchange information.

227
00:20:03,082 --> 00:20:07,050
So attention is a aggregation function.

228
00:20:07,080 --> 00:20:08,090
It's a pooling function.

229
00:20:09,030 --> 00:20:11,048
It's a weighted sum function.

230
00:20:11,088 --> 00:20:13,098
It is a reduce operation.

231
00:20:14,044 --> 00:20:19,048
Whereas MLP, this MLP here happens at every single token individually.

232
00:20:19,072 --> 00:20:22,066
There's no information being collected or exchanged between the tokens.

233
00:20:23,056 --> 00:20:26,092
So the attention is the reduce, and the MLP is the map.

234
00:20:27,032 --> 00:20:32,032
And what you end up with is that the transformer just ends up just being a repeated application of map reduce,

235
00:20:33,018 --> 00:20:34,046
if you want to think about it that way.

236
00:20:35,004 --> 00:20:40,032
So this is where they communicate, and this is where they think individually about the information that

237
00:20:40,032 --> 00:20:40,072
they gathered.

238
00:20:41,030 --> 00:20:47,082
And every one of these blocks iteratively refines the representation inside the residual stream.

239
00:20:48,064 --> 00:20:51,096
So this is our block, slightly modified from this picture.

240
00:20:53,014 --> 00:20:55,030
Okay, so let's now move on to the MLP.

241
00:20:55,078 --> 00:20:58,076
So the MLP block I implemented as follows.

242
00:20:59,010 --> 00:21:00,034
It is relatively straightforward.

243
00:21:00,062 --> 00:21:06,038
We basically have two linear projections here that are sandwiched in between the Gelu nonlinearity.

244
00:21:07,052 --> 00:21:10,018
So nn.gelu approximate is 10h.

245
00:21:10,096 --> 00:21:18,082
Now when we swing over to the PyTorch documentation, this is nn.gelu, and it has this format, and it has two versions.

246
00:21:19,008 --> 00:21:24,028
The original version of Gelu, which we'll step into in a bit, and the approximate version of Gelu, which

247
00:21:24,028 --> 00:21:25,068
we can request using 10h.

248
00:21:26,038 --> 00:21:36,080
So as you can see, just as a preview here, Gelu is basically like a Relu, except there's no flat, exactly flat tail here at exactly zero.

249
00:21:37,060 --> 00:21:40,030
But otherwise it looks very much like a slightly smoother Relu.

250
00:21:41,000 --> 00:21:49,020
It comes from this paper here, Gaussian Error Linear Units, and you can step through this paper, and there's some mathematical kind of like reasoning that

251
00:21:49,020 --> 00:21:52,042
leads to an interpretation that leads to the specific formulation.

252
00:21:52,098 --> 00:21:58,028
It has to do with stochastic radial risers, and the expectation of a modification to adaptive dropout.

253
00:21:58,056 --> 00:22:00,062
So you can read through all of that if you'd like here.

254
00:22:01,068 --> 00:22:07,092
And there's a little bit of history as to why there's an approximate version of Gelu, and that comes from this issue here,

255
00:22:08,038 --> 00:22:09,008
as far as I can tell.

256
00:22:10,040 --> 00:22:18,018
And in this issue, Daniel Hendrix mentions that at the time when they developed this nonlinearity, the IRF function,

257
00:22:18,038 --> 00:22:24,038
which you need to evaluate the exact Gelu, was very slow in TensorFlow, so they ended up basically developing this approximation.

258
00:22:24,096 --> 00:22:29,016
And this approximation that then ended up being picked up by BERT and by GPT-2, etc.

259
00:22:29,086 --> 00:22:32,048
But today there's no real good reason to use the approximate version.

260
00:22:32,076 --> 00:22:38,094
You'd prefer to just use the exact version, because my expectation is that there's no big difference anymore,

261
00:22:39,054 --> 00:22:42,076
and this is kind of like a historical kind of quirk.

262
00:22:43,044 --> 00:22:52,010
But we are trying to reproduce GPT-2 exactly, and GPT-2 used the 10H approximate version, so we prefer to stick with that.

263
00:22:53,072 --> 00:22:59,090
Now one other reason to actually just intuitively use Gelu instead of Relu is previously in videos in the past,

264
00:23:00,000 --> 00:23:06,092
we've spoken about the dead Relu neuron problem, where in this tail of a Relu, if it's exactly flat at zero,

265
00:23:07,044 --> 00:23:10,078
any activations that fall there will get exactly zero gradient.

266
00:23:10,098 --> 00:23:17,064
There's no change, there's no adaptation, there's no development of the network if any of these activations end in this flat region.

267
00:23:18,032 --> 00:23:24,002
But the Gelu always contributes a local gradient, and so there's going to be a change, always going to be an adaptation,

268
00:23:24,074 --> 00:23:29,034
and sort of smoothing it out ends up empirically working better in practice, as demonstrated in this paper,

269
00:23:29,070 --> 00:23:34,024
and also as demonstrated by it being picked up by the BERT paper, GPT-2 paper, and so on.

270
00:23:34,070 --> 00:23:39,072
So for that reason we adopt this non-linearity here in the 10H, in the GPT-2 reproduction.

271
00:23:40,022 --> 00:23:47,040
Now in more modern networks, also like Lama3 and so on, this non-linearity also further changes to SuiGlu and

272
00:23:47,040 --> 00:23:51,064
other variants like that, but for GPT-2 they use this approximate Gelu.

273
00:23:52,036 --> 00:23:54,080
Okay, and finally we have the attention operation.

274
00:23:55,064 --> 00:23:57,042
So let me paste in my attention.

275
00:24:00,040 --> 00:24:05,032
So I know this is a lot, so I'm gonna go through this a bit quickly, a bit slowly, but not too slowly,

276
00:24:05,052 --> 00:24:08,018
because we have covered this in the previous video, and I would just point you there.

277
00:24:09,096 --> 00:24:11,028
So this is the attention operation.

278
00:24:11,082 --> 00:24:17,078
Now in the previous video, you will remember this is not just attention, this is multi-headed attention,

279
00:24:18,002 --> 00:24:18,018
right?

280
00:24:18,098 --> 00:24:25,028
And so in the previous video we had this multi-headed attention module, and this implementation made it obvious that

281
00:24:25,028 --> 00:24:27,018
these heads are not actually that complicated.

282
00:24:27,088 --> 00:24:34,054
There's basically in parallel inside every attention block, there's multiple heads, and they're all functioning in parallel,

283
00:24:35,014 --> 00:24:41,050
and their outputs are just being concatenated, and that becomes the output of the multi-headed attention.

284
00:24:42,032 --> 00:24:46,056
So the heads are just parallel streams, and their outputs get concatenated.

285
00:24:47,064 --> 00:24:53,030
And so it was very simple and made the head be fairly straightforward in terms of its implementation.

286
00:24:55,006 --> 00:25:00,050
What happens here is that instead of having two separate modules, and indeed many more modules that get concatenated,

287
00:25:00,086 --> 00:25:05,020
all of that is just put into a single self-attention module.

288
00:25:05,076 --> 00:25:14,062
And instead I'm being very careful and doing a bunch of transpose split tensor gymnastics to make this very efficient in PyTorch.

289
00:25:14,092 --> 00:25:22,044
But fundamentally and algorithmically, nothing is different from the implementation we saw before in this Git repository.

290
00:25:23,068 --> 00:25:32,064
So to remind you very briefly, and I don't want to go into this in too much time, but we have these tokens lined up in a sequence,

291
00:25:32,078 --> 00:25:34,002
and there's 1020 of them.

292
00:25:35,010 --> 00:25:41,022
And then each token at this stage of the attention emits three vectors, the query, key, and the value.

293
00:25:42,024 --> 00:25:51,042
And first what happens here is that the queries and the keys have to multiply each other to get sort of the attention amount,

294
00:25:51,080 --> 00:25:53,044
like how interesting they find each other.

295
00:25:53,088 --> 00:25:55,024
So they have to interact multiplicatively.

296
00:25:56,002 --> 00:26:01,084
So what we're doing here is we're calculating the QKV while splitting it, and then there's a bunch of gymnastics as I mentioned here.

297
00:26:02,052 --> 00:26:09,008
And the way this works is that we're basically making the number of heads, NH, into a batch dimension.

298
00:26:09,068 --> 00:26:15,088
And so it's a batch dimension just like B, so that in these operations that follow, PyTorch treats B and

299
00:26:15,088 --> 00:26:22,098
NH as batches, and it applies all the operations on all of them in parallel, in both the batch and the heads.

300
00:26:23,046 --> 00:26:29,046
And the operations that get applied are, number one, the queries and the keys interact to give us our attention.

301
00:26:30,022 --> 00:26:36,094
This is the autoregressive mask that makes sure that the tokens only attend to tokens before them, and

302
00:26:36,094 --> 00:26:38,034
never to tokens in the future.

303
00:26:40,004 --> 00:26:44,020
The softmax here normalizes the attention, so it sums to one always.

304
00:26:45,010 --> 00:26:53,035
And then recall from the previous video that doing the attention matrix multiplied with the values is basically a way to do a weighted sum of the values of the tokens that

305
00:26:53,035 --> 00:26:55,060
we found interesting at every single token.

306
00:26:56,030 --> 00:27:04,016
And then the final transpose contiguous and view is just reassembling all of that again, and this actually performs the concatenation operation.

307
00:27:05,024 --> 00:27:12,008
So you can step through this slowly if you'd like, but this is equivalent mathematically to our previous implementation.

308
00:27:12,034 --> 00:27:15,088
It's just more efficient in PyTorch, so that's why I chose this implementation instead.

309
00:27:17,008 --> 00:27:19,098
Now in addition to that, I'm being careful with how I name my variables.

310
00:27:20,038 --> 00:27:28,086
So for example, seaten is the same as seaten, and so actually our keys should basically exactly follow the schema of the Hugging Face Transformers code,

311
00:27:29,022 --> 00:27:35,070
and that will make it very easy for us to now port over all the weights from exactly this sort of naming conventions,

312
00:27:36,020 --> 00:27:38,018
because all of our variables are named the same thing.

313
00:27:38,090 --> 00:27:47,074
But at this point we have finished the GPT-2 implementation, and what that allows us to do is we don't have to basically use this file from Hugging Face,

314
00:27:48,016 --> 00:27:49,002
which is fairly long.

315
00:27:50,046 --> 00:27:55,036
This is 2,000 lines of code.

316
00:27:56,074 --> 00:28:01,060
Instead we just have less than 100 lines of code, and this is the complete GPT-2 implementation.

317
00:28:02,020 --> 00:28:07,018
So at this stage we should just be able to take over all the weights, set them, and then do generation.

318
00:28:07,068 --> 00:28:08,086
So let's see what that looks like.

319
00:28:09,016 --> 00:28:15,094
Okay, so here I've also changed the GPT config so that the numbers here, the hyperparameters, agree with the GPT-2-124m model.

320
00:28:16,024 --> 00:28:20,022
So the maximum sequence length, which I call block size here, is 124.

321
00:28:20,084 --> 00:28:28,046
The number of tokens is 52,5257, which if you watch my tokenizer video, know that this is 50,000 merges,

322
00:28:28,078 --> 00:28:38,066
BPE merges, 256 byte tokens, the leaves of the BPE tree, and one special end of text token that delimits different documents and

323
00:28:38,066 --> 00:28:40,020
can start generation as well.

324
00:28:41,018 --> 00:28:46,078
And there are 12 layers, there are 12 heads in the attention, and the dimension of the transformers was 768.

325
00:28:47,066 --> 00:28:55,057
So here's how we can now load the parameters from Hugging Face to our code here and initialize the GPT class with those parameters.

326
00:28:56,000 --> 00:28:58,044
So let me just copy-paste a bunch of code here.

327
00:28:59,074 --> 00:29:07,066
And I'm not going to go through this code too slow, too quickly, too slowly, because honestly it's not that

328
00:29:07,066 --> 00:29:08,076
interesting, it's not that exciting.

329
00:29:08,092 --> 00:29:10,064
We're just loading the weights, so it's kind of dry.

330
00:29:11,018 --> 00:29:14,066
But as I mentioned, there are four models in this mini-series of GPT-2.

331
00:29:15,016 --> 00:29:19,014
This is some of the Jupyter code that we had here on the right.

332
00:29:19,042 --> 00:29:20,070
I'm just porting it over.

333
00:29:21,024 --> 00:29:23,022
These are the hyperparameters of the GPT-2 models.

334
00:29:24,034 --> 00:29:26,070
We're creating the config object and creating our own model.

335
00:29:27,014 --> 00:29:33,002
And then what's happening here is we're creating the state dict, both for our model and for the Hugging Face model.

336
00:29:35,036 --> 00:29:42,040
And then what we're doing here is we're going over to Hugging Face model keys, and we're copying over those tensors.

337
00:29:43,008 --> 00:29:45,068
And in the process, we are kind of ignoring a few of the buffers.

338
00:29:45,068 --> 00:29:48,052
They're not parameters, they're buffers.

339
00:29:48,086 --> 00:29:52,064
So for example, attention.bias, that's just used for the autoregressive mask.

340
00:29:53,030 --> 00:29:56,002
And so we are ignoring some of those masks, and that's it.

341
00:29:56,002 --> 00:30:04,030
And then one additional kind of annoyance is that this comes from the TensorFlow repo, and I'm not sure how this is a little bit annoying,

342
00:30:04,060 --> 00:30:07,074
but some of the weights are transposed from what PyTorch would want.

343
00:30:08,024 --> 00:30:13,084
And so manually, I hardcoded the weights that should be transposed, and then we transpose them if that

344
00:30:13,084 --> 00:30:14,020
is so.

345
00:30:14,070 --> 00:30:16,044
And then we return this model.

346
00:30:16,088 --> 00:30:25,006
So the from pre-trained is a constructor or a class method in Python that returns the GPT object if we just give it the model type,

347
00:30:27,082 --> 00:30:29,094
which in our case is GPT-2, the smallest model.

348
00:30:29,094 --> 00:30:30,008
And then we return the model type.

349
00:30:32,006 --> 00:30:34,020
So this is the code, and this is how you would use it.

350
00:30:35,016 --> 00:30:45,000
And we can pop open the terminal here in VS Code, and we can Python train GPT-2.py, and fingers crossed.

351
00:30:47,080 --> 00:30:48,092
Okay, so we didn't crash.

352
00:30:49,060 --> 00:30:55,022
And so we can load the weights and the biases and everything else into our NN module.

353
00:30:55,070 --> 00:31:00,036
But now let's also get additional confidence that this is working, and let's try to actually generate from this model.

354
00:31:00,084 --> 00:31:04,008
Okay, now before we can actually generate from this model, we have to be able to forward it.

355
00:31:04,024 --> 00:31:05,078
We didn't actually write that code yet.

356
00:31:06,022 --> 00:31:07,066
So here's the forward function.

357
00:31:09,002 --> 00:31:14,048
So the input to the forward is going to be our indices, our token indices.

358
00:31:15,000 --> 00:31:17,012
And they are always of shape B by T.

359
00:31:17,080 --> 00:31:23,090
And so we have batch dimension of B, and then we have the time dimension of up to T.

360
00:31:24,094 --> 00:31:27,014
And the T can't be more than the block size.

361
00:31:27,020 --> 00:31:29,004
The block size is the maximum sequence length.

362
00:31:29,096 --> 00:31:33,076
So B by T indices are arranged in sort of like a two-dimensional layout.

363
00:31:34,048 --> 00:31:39,094
And remember that basically every single row of this is of size up to block size.

364
00:31:40,054 --> 00:31:42,082
And this is T tokens that are in a sequence.

365
00:31:43,028 --> 00:31:48,012
And then we have B independent sequences stacked up in a batch so that this is efficient.

366
00:31:49,008 --> 00:31:52,068
Now here we are forwarding the position embeddings and the token embeddings.

367
00:31:53,018 --> 00:31:55,080
And this code should be very recognizable from the previous lecture.

368
00:31:56,044 --> 00:32:02,020
So we basically use a range, which is kind of like a version of range, but for PyTorch.

369
00:32:02,078 --> 00:32:08,076
And we're iterating from zero to T and creating this positions sort of indices.

370
00:32:11,014 --> 00:32:16,002
And then we are making sure that they're on the same device as IDX, because we're not going to be training on only CPU.

371
00:32:16,018 --> 00:32:17,010
That's going to be too inefficient.

372
00:32:17,038 --> 00:32:20,002
We want to be training on GPU, and that's going to come in a bit.

373
00:32:20,066 --> 00:32:26,024
Then we have the position embeddings and the token embeddings, and the addition operation of those two.

374
00:32:26,078 --> 00:32:31,068
Now notice that the position embeddings are going to be identical for every single row of input.

375
00:32:32,028 --> 00:32:38,004
And so there's broadcasting hidden inside this plus, where we have to create an additional dimension here.

376
00:32:38,046 --> 00:32:44,062
And then these two add up, because the same position embeddings apply at every single row of our examples stacked up in a batch.

377
00:32:45,074 --> 00:32:49,096
Then we forward the transformer blocks, and finally the last layer norm and the LM head.

378
00:32:51,030 --> 00:32:53,054
So what comes out after forward is the logits.

379
00:32:53,090 --> 00:33:03,010
And if the input was B by T indices, then at every single B by T, we will calculate the logits for what

380
00:33:03,010 --> 00:33:04,080
token comes next in the sequence.

381
00:33:05,000 --> 00:33:10,038
So what is the token B, T plus one, the one on the right of this token.

382
00:33:11,004 --> 00:33:15,004
And vocab size here is the number of possible tokens.

383
00:33:15,074 --> 00:33:18,040
And so therefore, this is the tensor that we're going to obtain.

384
00:33:18,057 --> 00:33:22,040
And these logits are just a softmax away from becoming probabilities.

385
00:33:23,024 --> 00:33:26,054
So this is the forward pass of the network.

386
00:33:27,006 --> 00:33:28,006
And now we can get logits.

387
00:33:28,030 --> 00:33:30,060
And so we're going to be able to generate from the model imminently.

388
00:33:31,066 --> 00:33:37,004
Okay, so now we're going to try to set up the identical thing on the left here that matches HuggingFace on the right.

389
00:33:37,048 --> 00:33:45,014
So here we've sampled from the pipeline, and we sampled five times up to 30 tokens with a prefix of hello,

390
00:33:45,024 --> 00:33:45,082
I'm a language model.

391
00:33:46,028 --> 00:33:47,076
And these are the completions that we achieved.

392
00:33:48,020 --> 00:33:50,000
So we're going to try to replicate that on the left here.

393
00:33:50,048 --> 00:33:53,014
So number of turn sequences is five, max length is 30.

394
00:33:53,050 --> 00:33:57,074
So the first thing we do, of course, is we initialize our model, then we put it into evaluation mode.

395
00:33:58,014 --> 00:34:03,056
Now, this is a good practice to put the model into eval when you're not going to be training it, you're just going to be using it.

396
00:34:04,030 --> 00:34:08,036
And I don't actually know if this is doing anything right now for the following reason.

397
00:34:08,088 --> 00:34:16,044
Our model up above here contains no modules or layers that actually have a different behavior at training or evaluation time.

398
00:34:16,064 --> 00:34:20,066
So for example, dropout, batch norm, and a bunch of other layers have this kind of behavior.

399
00:34:21,002 --> 00:34:25,028
But all of these layers that we've used here should be identical in both training and evaluation time.

400
00:34:27,057 --> 00:34:37,026
So potentially model.eval does nothing, but then I'm not actually sure if this is the case, and maybe PyTorch internals do some clever things depending on the evaluation mode inside here.

401
00:34:38,062 --> 00:34:42,050
The next thing we're doing here is we are moving the entire model to CUDA.

402
00:34:42,068 --> 00:34:45,064
So we're moving this, all of the tensors to GPU.

403
00:34:46,028 --> 00:34:49,094
So I'm SSHed here to a cloud box, and I have a bunch of GPUs on this box.

404
00:34:49,094 --> 00:34:56,014
And here I'm moving the entire model and all of its members and all of its tensors and everything like that.

405
00:34:56,057 --> 00:35:02,008
Everything gets shipped off to basically a whole separate computer that is sitting on the GPU.

406
00:35:02,056 --> 00:35:08,074
And the GPU is connected to the CPU, and they can communicate, but it's basically a whole separate computer with its own computer architecture,

407
00:35:09,012 --> 00:35:13,034
and it's really well catered to parallel processing tasks like those of running neural networks.

408
00:35:13,096 --> 00:35:22,026
So I'm doing this so that the model lives on the GPU, a whole separate computer, and it's just going to make our code a lot more efficient because

409
00:35:22,026 --> 00:35:24,064
all of this stuff runs a lot more efficiently on the GPUs.

410
00:35:25,004 --> 00:35:28,038
So that's the model itself.

411
00:35:29,050 --> 00:35:35,084
Now, the next thing we want to do is we want to start with this as the prefix when we do the generation.

412
00:35:36,088 --> 00:35:39,034
So let's actually create those prefix tokens.

413
00:35:39,080 --> 00:35:41,076
So here's the code that I've written.

414
00:35:42,034 --> 00:35:47,050
We're going to import the TickToken library from OpenAI, and we're going to get the GPT-2 encoding.

415
00:35:47,070 --> 00:35:49,096
So that's the tokenizer for GPT-2.

416
00:35:50,046 --> 00:35:56,020
And then we're going to encode this string and get a list of integers, which are the tokens.

417
00:35:57,038 --> 00:36:03,026
Now, these integers here should actually be fairly straightforward because we can just copy-paste this string,

418
00:36:03,084 --> 00:36:06,018
and we can sort of inspect what it is in TickTokenizer.

419
00:36:06,060 --> 00:36:09,070
So just pasting that in, these are the tokens that are going to come out.

420
00:36:10,052 --> 00:36:14,018
So this list of integers is what we expect tokens to become.

421
00:36:15,006 --> 00:36:19,038
And as you recall, if you saw my video, of course, all the tokens, they're just little string chunks,

422
00:36:19,062 --> 00:36:19,080
right?

423
00:36:19,086 --> 00:36:24,062
So this is the chunkation of this string into GPT-2 tokens.

424
00:36:25,026 --> 00:36:31,008
So once we have those tokens, it's a list of integers, we can create a torch tensor out of it.

425
00:36:31,036 --> 00:36:32,068
In this case, it's eight tokens.

426
00:36:33,014 --> 00:36:38,080
And then we're going to replicate these eight tokens for five times to get five rows of eight tokens.

427
00:36:39,036 --> 00:36:44,054
And that is our initial input x, as I call it here.

428
00:36:45,004 --> 00:36:46,057
And it lives on the GPU as well.

429
00:36:46,078 --> 00:36:57,038
So x now is this IDX that we can pin to forward to get our logits so that we know what comes as the sixth token,

430
00:36:58,044 --> 00:37:02,014
sorry, as the ninth token in every one of these five rows.

431
00:37:03,004 --> 00:37:04,074
Okay, and we are now ready to generate.

432
00:37:04,094 --> 00:37:06,092
So let me paste in one more code block here.

433
00:37:08,074 --> 00:37:14,042
So what's happening here in this code block is we have this x, which is of size b by t, right?

434
00:37:14,048 --> 00:37:16,002
So batch by time.

435
00:37:16,060 --> 00:37:23,018
And we're going to be in every iteration of this loop, we're going to be adding a column of new indices into each one of these rows,

436
00:37:23,032 --> 00:37:23,054
right?

437
00:37:24,014 --> 00:37:28,066
And so these are the new indices, and we're appending them to the sequence as we're sampling.

438
00:37:29,012 --> 00:37:32,057
So with each loop iteration, we get one more column into x.

439
00:37:33,046 --> 00:37:36,080
And all of the operations happen in the context manager of torch.nograd.

440
00:37:37,012 --> 00:37:40,080
This is just telling PyTorch that we're not going to be calling dot backward on any of this.

441
00:37:41,014 --> 00:37:47,010
So it doesn't have to cache all the intermediate tensors, it's not going to have to prepare in any way for a potential backward later.

442
00:37:47,046 --> 00:37:51,006
And this saves a lot of space and also possibly some time.

443
00:37:51,074 --> 00:37:58,024
So we get our logits, we get the logits at only the last location, we throw away all the other logits,

444
00:37:58,042 --> 00:38:02,010
we don't need them, we only care about the last columns logits.

445
00:38:02,057 --> 00:38:03,092
So this is being wasteful.

446
00:38:04,050 --> 00:38:08,002
But this is just kind of like an inefficient implementation of sampling.

447
00:38:08,050 --> 00:38:10,048
So it's correct but inefficient.

448
00:38:11,054 --> 00:38:15,064
So we get the last column of logits, pass it through softmax to get our probabilities.

449
00:38:16,032 --> 00:38:18,032
Then here I'm doing top k sampling of 50.

450
00:38:18,076 --> 00:38:20,078
And I'm doing that because this is the HuggingFace default.

451
00:38:21,036 --> 00:38:30,044
So just looking at the HuggingFace docs here of a pipeline, there's a bunch of quarks that go into HuggingFace.

452
00:38:31,022 --> 00:38:34,042
And I mean, it's kind of a lot, honestly.

453
00:38:34,070 --> 00:38:39,050
But I guess the important one that I noticed is that they're using top k by default, which is 50.

454
00:38:40,010 --> 00:38:44,006
And what that does is that, so that's being used here as well.

455
00:38:44,050 --> 00:38:49,088
And what that does is basically we want to take our probabilities, and we only want to keep the top 50 probabilities.

456
00:38:50,034 --> 00:38:56,006
And anything that is lower than the 50th probability, we just clamp to zero and renormalize.

457
00:38:56,052 --> 00:38:59,078
And so that way, we are never sampling very rare tokens.

458
00:39:00,050 --> 00:39:04,036
The tokens we're going to be sampling are always in the top 50 of most likely tokens.

459
00:39:04,088 --> 00:39:09,034
And this helps keep the model kind of on track, and it doesn't blabber on, and it doesn't get lost, and

460
00:39:09,034 --> 00:39:10,072
doesn't go off the rails as easily.

461
00:39:11,022 --> 00:39:15,094
And it kind of like sticks in the vicinity of likely tokens a lot better.

462
00:39:16,060 --> 00:39:18,002
So this is the way to do it in PyTorch.

463
00:39:18,016 --> 00:39:19,032
And you can step through it if you like.

464
00:39:19,048 --> 00:39:21,082
I don't think it's super insightful, so I'll speed through it.

465
00:39:22,018 --> 00:39:25,016
But roughly speaking, we get this new column of tokens.

466
00:39:25,057 --> 00:39:26,090
We append them on x.

467
00:39:27,044 --> 00:39:31,096
And basically, the columns of x grow until this while loop gets tripped up.

468
00:39:32,078 --> 00:39:40,086
And then finally, we have an entire x of size 5 by 30 in this case, in this example.

469
00:39:41,030 --> 00:39:44,066
And we can just basically print all those individual rows.

470
00:39:45,006 --> 00:39:53,006
So I'm getting all the rows, I'm getting all the tokens that were sampled, and I'm using the decode function from TickTokenizer to get back the string,

471
00:39:53,052 --> 00:39:54,040
which we can print.

472
00:39:55,038 --> 00:39:57,016
And so terminal, new terminal.

473
00:39:59,020 --> 00:40:01,064
And let me Python train GPT2.

474
00:40:02,072 --> 00:40:10,000
Okay.

475
00:40:10,036 --> 00:40:12,002
So these are the generations that we're getting.

476
00:40:12,034 --> 00:40:14,068
Hello, I'm a language model, not a program.

477
00:40:15,084 --> 00:40:18,034
New line, new line, et cetera.

478
00:40:18,084 --> 00:40:24,064
Hello, I'm a language model, and one of the main things that bothers me when they create languages is how easy it becomes to create something that

479
00:40:25,070 --> 00:40:27,098
I mean, so this will just like blabber on, right, in all these cases.

480
00:40:28,042 --> 00:40:33,068
Now, one thing you will notice is that these generations are not the generations of HuggingFace here.

481
00:40:34,006 --> 00:40:42,078
And I can't find the discrepancy, to be honest, and I didn't fully go through all these options, but probably there's something else hiding on addition to the top P.

482
00:40:43,028 --> 00:40:44,050
So I'm not able to match it up.

483
00:40:44,078 --> 00:40:51,046
But just for correctness, down here below in the Jupyter Notebook, and using the HuggingFace model, so

484
00:40:51,046 --> 00:41:01,050
this is the HuggingFace model here, I replicated the code, and if I do this and I run that, then I am getting the same results.

485
00:41:02,020 --> 00:41:04,088
So basically, the model internals are not wrong.

486
00:41:05,026 --> 00:41:10,078
It's just I'm not 100% sure what the pipeline does in HuggingFace, and that's why we're not able to match them up.

487
00:41:11,040 --> 00:41:16,016
But otherwise, the code is correct, and we've loaded all the tensors correctly.

488
00:41:16,036 --> 00:41:18,088
So we're initializing the model correctly, and everything here works.

489
00:41:19,062 --> 00:41:26,026
So long story short, we've ported all the weights, we initialize the GPT-2, this is the exact opening of GPT-2,

490
00:41:26,060 --> 00:41:28,068
and it can generate sequences and they look sensible.

491
00:41:29,068 --> 00:41:33,046
And now, here, of course, we're initializing with GPT-2 model weights.

492
00:41:33,082 --> 00:41:41,036
But now we want to initialize from scratch, from random numbers, and we want to train the model that will give us sequences as good as,

493
00:41:41,050 --> 00:41:44,042
or better than these ones in quality.

494
00:41:45,020 --> 00:41:46,078
And so that's what we turn to next.

495
00:41:47,032 --> 00:41:53,054
So it turns out that using the random model is actually fairly straightforward, because PyTorch already initializes our model randomly and

496
00:41:53,054 --> 00:41:53,098
by default.

497
00:41:54,038 --> 00:42:04,048
So when we create the GPT model in the constructor, all of these layers and modules have random initializers that

498
00:42:04,048 --> 00:42:05,050
are there by default.

499
00:42:06,006 --> 00:42:12,016
So when these linear layers get created and so on, there's default constructors, for example, using the Javier initialization that

500
00:42:12,016 --> 00:42:15,096
we saw in the past to construct the weights of these layers.

501
00:42:16,064 --> 00:42:21,072
And so creating a random model instead of a GPT-2 model is actually fairly straightforward.

502
00:42:22,006 --> 00:42:28,094
And we would just come here, and instead we would create model equals GPT, and then we want to use the default config,

503
00:42:29,012 --> 00:42:33,042
GPT config, and the default config uses the 124m parameters.

504
00:42:33,066 --> 00:42:37,092
So this is the random model initialization, and we can run it.

505
00:42:43,010 --> 00:42:46,066
And we should be able to get results.

506
00:42:47,012 --> 00:42:51,012
Now the results here, of course, are total garbage garble, and that's because it's a random model.

507
00:42:51,062 --> 00:42:55,088
And so we're just getting all these random token string pieces chunked up totally at random.

508
00:42:56,034 --> 00:42:57,040
So that's what we have right now.

509
00:42:58,016 --> 00:43:03,008
Now one more thing I wanted to point out, by the way, is in case you do not have CUDA available, because

510
00:43:03,008 --> 00:43:08,050
you don't have a GPU, you can still follow along with what we're doing here to some extent.

511
00:43:09,050 --> 00:43:14,096
And probably not to the very end, because by the end we're going to be using multiple GPUs and actually doing a serious training run.

512
00:43:15,046 --> 00:43:17,068
But for now, you can actually follow along decently okay.

513
00:43:18,030 --> 00:43:23,036
So one thing that I like to do in PyTorch is I like to auto detect the device that is available to you.

514
00:43:24,004 --> 00:43:25,090
So in particular, you could do that like this.

515
00:43:27,092 --> 00:43:32,070
So here we are trying to detect the device to run on that has the highest compute capability.

516
00:43:33,000 --> 00:43:33,094
You can think about it that way.

517
00:43:34,052 --> 00:43:39,080
So by default, we start with CPU, which of course is available everywhere, because every single computer will have a CPU.

518
00:43:40,044 --> 00:43:42,066
But then we can try to detect, do you have a GPU?

519
00:43:42,094 --> 00:43:43,078
If so, use a CUDA.

520
00:43:44,034 --> 00:43:48,022
And then if you don't have a CUDA, do you at least have MPS?

521
00:43:48,048 --> 00:43:50,044
MPS is the backend for Apple Silicon.

522
00:43:50,098 --> 00:43:54,084
So if you have a MacBook that is fairly new, you probably have Apple Silicon on the inside.

523
00:43:55,048 --> 00:44:00,006
And then that has a GPU that is actually fairly capable, depending on which MacBook you have.

524
00:44:00,018 --> 00:44:03,002
And so you can use MPS, which will be potentially faster than CPU.

525
00:44:03,088 --> 00:44:05,008
And so we can print the device here.

526
00:44:05,064 --> 00:44:09,078
Now once we have the device, we can actually use it in place of CUDA.

527
00:44:10,052 --> 00:44:11,084
So we just swap it in.

528
00:44:12,056 --> 00:44:21,076
And notice that here when we call model on x, if this x here is on CPU instead of GPU, then it will work fine,

529
00:44:21,094 --> 00:44:31,076
because here in the forward, which is where PyTorch will come, when we create a pose, we are careful to use the device of IDX to create this tensor as well.

530
00:44:32,040 --> 00:44:38,002
And so there won't be any mismatch where one tensor is on CPU, one is on GPU, and you can't combine those.

531
00:44:38,030 --> 00:44:45,000
But here we are carefully initializing on the correct device as indicated by the input to this model.

532
00:44:45,057 --> 00:44:47,066
So this will auto detect device.

533
00:44:47,076 --> 00:44:50,010
For me, this will be, of course, GPU.

534
00:44:50,090 --> 00:44:53,008
So using device CUDA.

535
00:44:53,066 --> 00:44:59,048
But you can also run with, as I mentioned, another device.

536
00:44:59,082 --> 00:45:00,096
And it's not going to be too much slower.

537
00:45:01,006 --> 00:45:11,022
So if I override device here, if I override device equals CPU, then we'll still print CUDA, of course,

538
00:45:11,032 --> 00:45:12,050
but now we're actually using CPU.

539
00:45:13,000 --> 00:45:18,070
One, two, three, four, five, six.

540
00:45:19,010 --> 00:45:20,010
Okay, about six seconds.

541
00:45:20,086 --> 00:45:25,042
And actually, we're not using Torch compile and stuff like that, which will speed up everything a lot faster as well.

542
00:45:25,074 --> 00:45:29,032
But you can follow along even on the CPU, I think, to a decent extent.

543
00:45:30,006 --> 00:45:31,057
So that's a note on that.

544
00:45:32,004 --> 00:45:37,012
Okay, so I do want to loop around eventually into what it means to have different devices in PyTorch,

545
00:45:37,036 --> 00:45:43,054
and what it is exactly that PyTorch does in the background for you when you do something like module.to device,

546
00:45:43,094 --> 00:45:49,002
or where you take a Torch tensor and do a .to device, and what exactly happens and how that works.

547
00:45:49,046 --> 00:45:52,098
But for now, I'd like to get to training, and I'd like to start training the model.

548
00:45:53,048 --> 00:45:56,010
And for now, let's just say the device makes code go fast.

549
00:45:56,098 --> 00:45:59,038
And let's go into how we can actually train the model.

550
00:45:59,082 --> 00:46:01,094
So to train the model, we're going to need some dataset.

551
00:46:02,044 --> 00:46:07,022
And for me, the best debugging simplest dataset that I like to use is the tiny Shakespeare dataset.

552
00:46:08,002 --> 00:46:13,010
And it's available at this URL, so you can wget it, or you can just search tiny Shakespeare dataset.

553
00:46:14,044 --> 00:46:18,028
And so I have it in my file system as just lsinput.txt.

554
00:46:18,052 --> 00:46:20,084
So I already downloaded it.

555
00:46:21,054 --> 00:46:26,048
And here, I'm reading the dataset, getting the first 1000 characters, and printing the first 100.

556
00:46:27,042 --> 00:46:34,020
Now, remember that GPT-2 has roughly a compression ratio, the tokenizer has a compression ratio of roughly 3 to 1.

557
00:46:34,056 --> 00:46:40,066
So 1000 characters is roughly 300 tokens here that will come out of this in the slice that we're currently getting.

558
00:46:41,028 --> 00:46:43,072
So this is the first few characters.

559
00:46:45,054 --> 00:46:49,084
And if you want to get a few more statistics on this, we can do word count on input.txt.

560
00:46:50,068 --> 00:46:59,086
So we can see that this is 40,000 lines, about 200,000 words in this dataset, and about 1 million bytes in this file.

561
00:47:00,036 --> 00:47:04,068
And knowing that this file is only ASCII characters, there's no crazy Unicode here as far as I know.

562
00:47:05,012 --> 00:47:07,068
And so every ASCII character is encoded with one byte.

563
00:47:08,018 --> 00:47:13,000
And so this is the same number, roughly a million characters inside this dataset.

564
00:47:14,024 --> 00:47:18,000
So that's the dataset size by default, very small and minimal dataset for debugging.

565
00:47:19,012 --> 00:47:25,000
To get us off the ground, in order to tokenize this dataset, we're going to get TicToken encoding for GPT-2,

566
00:47:25,072 --> 00:47:32,092
encode the data, the first 1000 characters, and then I'm only going to print the first 24 tokens.

567
00:47:33,070 --> 00:47:36,000
So these are the tokens as a list of integers.

568
00:47:37,012 --> 00:47:42,056
And if you can read GPT-2 tokens, you will see that 198 here, you'll recognize that as the slash in character.

569
00:47:42,094 --> 00:47:43,088
So that is a new line.

570
00:47:44,048 --> 00:47:46,012
And then here, for example, we have two new lines.

571
00:47:46,038 --> 00:47:47,078
So that's 198 twice here.

572
00:47:48,082 --> 00:47:51,052
So this is just the tokenization of the first 24 tokens.

573
00:47:52,014 --> 00:47:58,034
So what we want to do now is we want to actually process these token sequences and feed them into a transformer.

574
00:47:58,084 --> 00:48:07,000
And in particular, we want to rearrange these tokens into this IDX variable that we're going to be feeding into the transformer.

575
00:48:07,032 --> 00:48:09,090
So we don't want a single, very long one-dimensional sequence.

576
00:48:10,028 --> 00:48:19,020
We want an entire batch where each sequence is up to, it's basically T tokens, and T cannot be larger than the maximum sequence length.

577
00:48:19,078 --> 00:48:27,080
And then we have these T long sequences of tokens, and we have B independent examples of sequences.

578
00:48:28,052 --> 00:48:33,078
So how can we create a B by T tensor that we can feed into the forward out of these one-dimensional sequences?

579
00:48:34,098 --> 00:48:37,042
So here's my favorite way to achieve this.

580
00:48:38,044 --> 00:48:44,004
So if we take Torch, and then we create a tensor object out of this list of integers, and just the first 24 tokens,

581
00:48:44,076 --> 00:48:53,028
my favorite way to do this is basically you do a dot view of, for example, 4 by 6, which multiply to 24.

582
00:48:53,074 --> 00:48:55,098
And so it's just a two-dimensional rearrangement of these tokens.

583
00:48:56,046 --> 00:49:06,086
And you'll notice that when you view this one-dimensional sequence as two-dimensional 4 by 6 here, the first six tokens up to here end up being the first row.

584
00:49:07,022 --> 00:49:11,072
The next six tokens here end up being the second row, and so on.

585
00:49:12,018 --> 00:49:22,054
And so basically it's just going to stack up every six tokens in this case as independent rows, and it creates a batch of tokens in this case.

586
00:49:23,020 --> 00:49:29,040
And so for example, if we are token 25 in the transformer, when we feed this in and this becomes the IDX,

587
00:49:29,088 --> 00:49:35,050
this token is going to see these three tokens and is going to try to predict that 198 comes next.

588
00:49:36,088 --> 00:49:40,066
So in this way, we are able to create this two-dimensional batch.

589
00:49:40,092 --> 00:49:41,082
That's quite nice.

590
00:49:42,044 --> 00:49:48,008
Now in terms of the label that we're going to need for the target to calculate the loss function, how do we get that?

591
00:49:48,070 --> 00:49:53,062
Well, we could write some code inside the forward pass, because we know that the next token in a sequence,

592
00:49:53,072 --> 00:49:56,002
which is the label, is just to the right of us.

593
00:49:56,064 --> 00:50:03,042
But you'll notice that actually for this token at the very end, 13, we don't actually have the next correct token,

594
00:50:03,042 --> 00:50:04,042
because we didn't load it.

595
00:50:05,022 --> 00:50:08,006
So we actually didn't get enough information here.

596
00:50:09,036 --> 00:50:12,096
So I'll show you my favorite way of basically getting these batches.

597
00:50:13,050 --> 00:50:18,064
And I like to personally have not just the input to the transformer, which I like to call x, but I also

598
00:50:18,064 --> 00:50:26,046
like to create the labels tensor, which is of the exact same size as x, but contains the targets at every single position.

599
00:50:27,030 --> 00:50:28,072
And so here's the way that I like to do that.

600
00:50:29,012 --> 00:50:34,050
I like to make sure that I fetch plus one token, because we need the ground truth for the very last token,

601
00:50:35,004 --> 00:50:36,022
for 13.

602
00:50:37,024 --> 00:50:44,018
And then when we're creating the input, we take everything up to the last token, not including, and view it as for y6.

603
00:50:44,057 --> 00:50:51,020
And when we're creating targets, we do the buffer, but starting at index one, not index zero.

604
00:50:51,038 --> 00:50:54,054
So we're skipping the first element, and we view it in the exact same size.

605
00:50:55,010 --> 00:51:00,086
And then when I print this, here's what happens, where we see that basically, as an example for this token 25,

606
00:51:03,014 --> 00:51:04,012
its target was 198.

607
00:51:04,057 --> 00:51:09,014
And that's now just stored at the exact same position in the target tensor, which is 198.

608
00:51:09,072 --> 00:51:14,072
And also, this last token 13 now has its label, which is 198.

609
00:51:14,072 --> 00:51:18,012
And that's just because we loaded this plus one here.

610
00:51:18,074 --> 00:51:20,080
So basically, this is the way I like to do it.

611
00:51:20,080 --> 00:51:27,004
You take long sequences, you view them in two dimensional terms, so that you get batches of time.

612
00:51:27,004 --> 00:51:30,060
And then we make sure to load one additional token.

613
00:51:30,094 --> 00:51:33,026
So we basically load a buffer of tokens of B times T plus one.

614
00:51:35,092 --> 00:51:37,070
And then we sort of offset things and view them.

615
00:51:38,014 --> 00:51:38,062
And then we have two tensors.

616
00:51:38,062 --> 00:51:41,044
One of them is the input to the transformer.

617
00:51:41,090 --> 00:51:43,074
And the other exactly is the labels.

618
00:51:44,030 --> 00:51:54,002
And so let's now reorganize this code and create a very simple data loader object that tries to basically load these tokens and

619
00:51:54,002 --> 00:51:56,066
feed them to the transformer and calculate the loss.

620
00:51:57,022 --> 00:51:59,052
Okay, so I reshuffled the code here accordingly.

621
00:52:00,032 --> 00:52:06,040
So as you can see here, I'm temporarily overriding to run on CPU and importing the token.

622
00:52:06,066 --> 00:52:07,064
And all of this should look familiar.

623
00:52:07,080 --> 00:52:09,016
We're loading 1000 characters.

624
00:52:09,052 --> 00:52:16,000
I'm setting BT to just be 4 and 32 right now, just because we're debugging, we just want to have a single batch that's very small.

625
00:52:16,078 --> 00:52:19,086
And all of this should now look familiar and follows what we did on the right.

626
00:52:20,082 --> 00:52:25,032
And then here, we get the we create the model and get the logits.

627
00:52:25,070 --> 00:52:30,034
And so here, as you see, I already ran this only runs in a few seconds.

628
00:52:30,084 --> 00:52:38,068
But because we have a batch of four by 32, our logits are now of size four by 32 by 50,257.

629
00:52:39,010 --> 00:52:42,016
So those are the logits for what comes next at every position.

630
00:52:42,078 --> 00:52:45,064
And now we have the labels which are stored in y.

631
00:52:45,094 --> 00:52:50,044
So now's the time to calculate the loss, and then do the backward pass and then the optimization.

632
00:52:50,090 --> 00:52:52,056
So let's first calculate the loss.

633
00:52:53,086 --> 00:52:58,054
Okay, so to calculate the loss, we're going to adjust the forward function of this and then module in the model.

634
00:52:58,092 --> 00:53:03,022
And in particular, we're not just going to be returning logits, but also we're going to return the loss.

635
00:53:04,006 --> 00:53:08,052
And we're going to not just pass in the input indices, but also the targets in y.

636
00:53:09,028 --> 00:53:16,006
And now we will print not logis.shape anymore, we're actually going to print the loss function, and then

637
00:53:16,006 --> 00:53:19,018
it's just that exit of zero so that we skip some of the sampling logic.

638
00:53:20,022 --> 00:53:27,024
So now let's swing up to the forward function, which gets called there, because now we also have these optional targets.

639
00:53:28,034 --> 00:53:32,052
And when we get the targets, we can also calculate the loss.

640
00:53:32,080 --> 00:53:37,066
And remember that we want to basically return logist loss, and loss by default is none.

641
00:53:38,002 --> 00:53:42,070
But let's put this here.

642
00:53:43,052 --> 00:53:49,006
If targets is not none, then we want to calculate the loss.

643
00:53:49,076 --> 00:53:54,052
And Copilot is already getting excited here and calculating the what looks to be correct loss.

644
00:53:54,082 --> 00:53:59,014
It is using the cross entropy loss as is documented here.

645
00:54:00,002 --> 00:54:03,036
So this is a function in PyTorch under the functional.

646
00:54:04,054 --> 00:54:11,090
Now, what is actually happening here, because it looks a little bit scary, basically, the F dot cross entropy does not like multi-dimensional inputs,

647
00:54:12,002 --> 00:54:14,022
it can't take a b by t by vocab size.

648
00:54:14,068 --> 00:54:19,098
So what's happening here is that we are flattening out this three-dimensional tensor into just two dimensions.

649
00:54:20,048 --> 00:54:24,048
The first dimension is going to be calculated automatically, and it's going to be b times t.

650
00:54:25,020 --> 00:54:27,056
And then the last dimension is vocab size.

651
00:54:28,008 --> 00:54:33,064
So basically, this is flattening out this three-dimensional tensor of logits to just be two-dimensional,

652
00:54:33,088 --> 00:54:40,086
b times t, all individual examples, and vocab size in terms of the length of each row.

653
00:54:41,044 --> 00:54:47,040
And then it's also flattening out the targets, which are also two-dimensional at this stage, but we're going to just flatten them out,

654
00:54:47,040 --> 00:54:49,056
so they're just a single tensor of b times t.

655
00:54:50,018 --> 00:54:53,074
And this can then pass into cross entropy to calculate a loss, which we return.

656
00:54:54,022 --> 00:54:58,084
So this should basically, at this point, run, because it's not too complicated.

657
00:54:58,098 --> 00:55:04,036
So let's run it, and let's see if we should be printing the loss.

658
00:55:10,048 --> 00:55:13,042
And here we see that we printed 11, roughly.

659
00:55:14,010 --> 00:55:20,098
And so, and notice that this is the tensor of a single element, which is this number 11.

660
00:55:21,062 --> 00:55:27,024
Now, we also want to be able to calculate a reasonable kind of starting point for a randomly initialized network.

661
00:55:27,072 --> 00:55:32,012
So we covered this in previous videos, but our vocabulary size is 50,257.

662
00:55:32,094 --> 00:55:40,096
At initialization of the network, you would hope that every vocab element is getting roughly a uniform probability,

663
00:55:41,064 --> 00:55:46,006
so that we're not favoring at initialization any token way too much.

664
00:55:46,016 --> 00:55:48,020
We're not confidently wrong at initialization.

665
00:55:48,086 --> 00:55:54,030
So what we're hoping is that the probability of any arbitrary token is roughly 1 over 50,257.

666
00:55:54,094 --> 00:56:01,080
And now we can sanity check the loss, because remember that the cross entropy loss is just basically the negative log likelihood.

667
00:56:02,036 --> 00:56:09,016
So if we now take this probability, and we take it through the natural logarithm, and then we do the negative,

668
00:56:09,066 --> 00:56:13,088
that is the loss we expect at initialization, and we covered this in previous videos.

669
00:56:14,048 --> 00:56:18,044
So I would expect something around 10.82, and we're seeing something around 11.

670
00:56:18,078 --> 00:56:19,090
So it's not way off.

671
00:56:20,010 --> 00:56:22,066
This is roughly the probability I expect at initialization.

672
00:56:23,032 --> 00:56:28,068
So that tells me that at initialization, our probability distribution is roughly diffuse, it's a good starting point,

673
00:56:28,092 --> 00:56:35,052
and we can now perform the optimization and tell the network which elements should follow correctly in what

674
00:56:35,052 --> 00:56:35,074
order.

675
00:56:36,034 --> 00:56:40,088
So at this point, we can do a loss step backward, calculate the gradients, and do an optimization.

676
00:56:41,034 --> 00:56:42,018
So let's get to that.

677
00:56:42,082 --> 00:56:44,066
Okay, so let's do the optimization now.

678
00:56:46,008 --> 00:56:51,022
So here we have the loss, this is how we get the loss.

679
00:56:52,016 --> 00:56:54,000
But now basically we want a little for loop here.

680
00:56:54,012 --> 00:56:57,032
So for i in range, let's do 50 steps or something like that.

681
00:56:57,078 --> 00:57:00,052
Let's create an optimizer object in PyTorch.

682
00:57:02,020 --> 00:57:09,084
And so here we are using the Adam optimizer, which is an alternative to the stochastic gradient descent optimizer,

683
00:57:09,096 --> 00:57:11,002
SGD, that we were using.

684
00:57:11,052 --> 00:57:13,062
So SGD is a lot simpler, Adam is a bit more involved.

685
00:57:14,002 --> 00:57:19,098
And I actually specifically like the Adam W variation, because in my opinion, it kind of just like fixes a bug.

686
00:57:21,026 --> 00:57:24,026
So Adam W is a bug fix of Adam, is what I would say.

687
00:57:24,064 --> 00:57:33,026
When we go to the documentation for Adam W, oh my gosh, we see that it takes a bunch of hyperparameters,

688
00:57:33,044 --> 00:57:36,028
and it's a little bit more complicated than the SGD we were looking at before.

689
00:57:36,094 --> 00:57:41,096
Because in addition to basically updating the parameters with the gradient scaled by the learning rate,

690
00:57:42,030 --> 00:57:43,094
it keeps these buffers around.

691
00:57:44,008 --> 00:57:48,082
And it keeps two buffers, the M and the V, which it calls the first and the second moment.

692
00:57:49,038 --> 00:57:54,034
So something that looks a bit like momentum, something that looks a bit like RMS prop, if you're familiar with it.

693
00:57:54,078 --> 00:57:59,040
But you don't have to be, it's just kind of like a normalization that happens on each gradient element individually,

694
00:57:59,094 --> 00:58:02,092
and speeds up the optimization, especially for language models.

695
00:58:03,036 --> 00:58:05,002
But I'm not going to go into the detail right here.

696
00:58:05,038 --> 00:58:12,088
We're going to treat this a bit of a black box, and it just optimizes the objective faster than SGD, which

697
00:58:12,088 --> 00:58:14,040
is what we've seen in the previous lectures.

698
00:58:15,002 --> 00:58:16,088
So let's use it as a black box in our case.

699
00:58:18,004 --> 00:58:23,060
Create the optimizer object, and then go through the optimization.

700
00:58:28,086 --> 00:58:33,002
The first thing to always make sure, the copilot did not forget to zero the gradients.

701
00:58:34,000 --> 00:58:37,038
So always remember that you have to start with a zero gradient.

702
00:58:37,090 --> 00:58:42,060
Then when you get your loss, and you do a dot backward, dot backward adds to gradients.

703
00:58:42,084 --> 00:58:44,020
So it deposits gradients.

704
00:58:44,046 --> 00:58:48,060
It always does a plus equals on whatever the gradients are, which is why you must set them to zero.

705
00:58:49,040 --> 00:58:57,052
So this accumulates the gradient from this loss, and then we call the step function on the optimizer to update the parameters,

706
00:58:58,072 --> 00:59:00,057
and to decrease the loss.

707
00:59:01,094 --> 00:59:07,057
Then we print the step, and the loss dot item is used here, because loss is a tensor with a single element.

708
00:59:07,080 --> 00:59:15,044
Dot item will actually convert that to a single float, and this float will live on the CPU.

709
00:59:16,008 --> 00:59:21,046
So this gets to some of the internals again of the devices, but loss is a tensor with a single element,

710
00:59:21,062 --> 00:59:24,092
and it lives on GPU for me, because I'm using GPUs.

711
00:59:25,046 --> 00:59:32,032
When you call dot item, PyTorch behind the scenes will take that one-dimensional tensor, ship it back to the CPU memory,

712
00:59:32,070 --> 00:59:34,076
and convert it into a float that we can just print.

713
00:59:35,078 --> 00:59:40,050
So this is the optimization, and this should probably just work.

714
00:59:41,080 --> 00:59:43,070
Let's see what happens.

715
00:59:46,010 --> 00:59:46,068
Actually, sorry.

716
00:59:46,082 --> 00:59:52,057
Instead of using CPU override, let me delete that, so this is a bit faster for me, and it runs on CUDA.

717
00:59:57,016 --> 01:00:04,024
Oh, expected all tensors to be on the same device, but found at least two devices, CUDA0 and CPU.

718
01:00:04,096 --> 01:00:12,030
So CUDA0 is the 0th GPU, because I actually have eight GPUs on this box, so the 0th GPU on my box and

719
01:00:12,030 --> 01:00:12,064
CPU.

720
01:00:13,044 --> 01:00:19,074
And model, we have moved to device, but when I was writing this code, I actually introduced a bug, because

721
01:00:19,074 --> 01:00:21,042
buff, we never moved to device.

722
01:00:22,028 --> 01:00:26,050
And you have to be careful, because you can't just do buff dot to of device.

723
01:00:27,076 --> 01:00:29,062
It's not stateful.

724
01:00:29,070 --> 01:00:31,028
It doesn't convert it to be a device.

725
01:00:31,044 --> 01:00:36,036
It instead returns pointer to a new memory, which is on the device.

726
01:00:36,088 --> 01:00:40,048
So you see how we can just do model dot to of device, but it does not apply to tensors.

727
01:00:40,088 --> 01:00:48,072
You have to do buff equals buff dot to device, and then this should work.

728
01:00:49,008 --> 01:00:49,010
Okay.

729
01:00:50,074 --> 01:00:51,096
So what do we expect to see?

730
01:00:52,006 --> 01:00:56,076
We expect to see a reasonable loss in the beginning, and then we continue to optimize just a single batch.

731
01:00:57,006 --> 01:00:59,042
And so we want to see that we can overfit this single batch.

732
01:00:59,057 --> 01:01:04,082
We can crush this little batch, and we can perfectly predict the indices on just this little batch.

733
01:01:05,046 --> 01:01:07,044
And in these, that is roughly what we're seeing here.

734
01:01:07,074 --> 01:01:16,088
So we started off at roughly 10.82, 11 in this case, and then as we continue optimizing on this single batch without loading new examples,

735
01:01:17,024 --> 01:01:21,066
we are making sure that we can overfit a single batch, and we are getting to very, very low loss.

736
01:01:21,086 --> 01:01:25,040
So the transformer is memorizing this single individual batch.

737
01:01:26,028 --> 01:01:34,016
And one more thing I didn't mention is the learning rate here is 3E negative 4, which is a pretty good default for most optimizations that

738
01:01:34,016 --> 01:01:36,074
you want to run at a very early debugging stage.

739
01:01:37,044 --> 01:01:43,044
So this is our simple inner loop, and we are overfitting a single batch, and this looks good.

740
01:01:43,098 --> 01:01:47,028
So now what comes next is we don't just want to overfit a single batch.

741
01:01:47,040 --> 01:01:48,086
We actually want to do an optimization.

742
01:01:49,042 --> 01:01:56,028
So we actually need to iterate these xy batches and create a little data loader that makes sure that we're always getting a fresh batch,

743
01:01:56,066 --> 01:01:58,062
and that we're actually optimizing a reasonable objective.

744
01:01:58,098 --> 01:01:59,076
So let's do that next.

745
01:02:00,030 --> 01:02:00,057
Okay.

746
01:02:00,066 --> 01:02:03,046
So this is what I came up with, and I wrote a little data loader light.

747
01:02:04,080 --> 01:02:12,032
So what this data loader does is we're importing the token up here, we're reading the entire text file from this single input.txt,

748
01:02:13,022 --> 01:02:21,060
tokenizing it, and then we're just printing the number of tokens in total, and the number of batches in a single epoch of iterating over this dataset.

749
01:02:22,002 --> 01:02:27,044
So how many unique batches do we output before we loop back around to the beginning of the document and

750
01:02:27,044 --> 01:02:28,040
start reading it again?

751
01:02:29,014 --> 01:02:34,094
So we start off at position 0, and then we simply walk the document in batches of b times t.

752
01:02:35,036 --> 01:02:39,032
So we take chunks of b times t, and then always advance by b times t.

753
01:02:40,038 --> 01:02:47,051
And it's important to note that we're always advancing our position by exactly b times t, but when we're fetching the tokens,

754
01:02:47,051 --> 01:02:51,052
we're actually fetching from current position to b times t plus 1.

755
01:02:52,006 --> 01:02:59,004
And we need that plus 1 because remember we need the target token for the last token in the current batch.

756
01:02:59,068 --> 01:03:04,060
And so that way we can do the xy exactly as we did it before.

757
01:03:05,028 --> 01:03:10,038
And if we are to run out of data, we'll just loop back around to 0.

758
01:03:11,040 --> 01:03:17,076
So this is one way to write a very, very simple data loader that simply just goes through the file in chunks,

759
01:03:18,008 --> 01:03:21,048
and is good enough for us for current purposes.

760
01:03:21,082 --> 01:03:23,057
And we're going to complexify it later.

761
01:03:23,090 --> 01:03:28,008
And now we'd like to come back around here, and we'd like to actually use our data loader.

762
01:03:28,028 --> 01:03:32,050
So the import tick token has moved up, and actually all of this is now useless.

763
01:03:33,018 --> 01:03:40,024
So instead we just want a train loader for the training data, and we want to use the same hyperparameters for 4.

764
01:03:40,050 --> 01:03:43,044
So batch size was 4, and time was 32.

765
01:03:44,018 --> 01:03:48,012
And then here we need to get the xy for the current batch.

766
01:03:48,057 --> 01:03:51,050
So let's see if Copilot gets it, because this is simple enough.

767
01:03:52,020 --> 01:04:01,076
So we call the next batch, and then we make sure that we have to move our tensors from CPU to the device.

768
01:04:02,080 --> 01:04:08,018
So here when I converted the tokens, notice that I didn't actually move these tokens to the GPU.

769
01:04:08,050 --> 01:04:10,084
I left them on CPU, which is default.

770
01:04:11,052 --> 01:04:15,066
And that's just because I'm trying not to waste too much memory on the GPU.

771
01:04:16,004 --> 01:04:23,040
In this case this is a tiny data set that it would fit, but it's fine to just ship it to GPU right now for our purposes right now.

772
01:04:24,034 --> 01:04:31,046
So we get the next batch, we keep the data loader simple CPU class, and then here we actually ship it to the GPU and

773
01:04:31,046 --> 01:04:32,030
do all the computation.

774
01:04:33,030 --> 01:04:35,056
And let's see if this runs.

775
01:04:36,000 --> 01:04:38,086
So python train gput2.py.

776
01:04:39,070 --> 01:04:41,090
And what do we expect to see before this actually happens?

777
01:04:42,044 --> 01:04:45,004
What we expect to see is now we're actually getting the next batch.

778
01:04:45,036 --> 01:04:48,008
So we expect to not overfit a single batch.

779
01:04:48,062 --> 01:04:53,012
And so I expect our loss to come down, but not too much.

780
01:04:53,084 --> 01:05:01,036
And that's because I still expect it to come down, because in the 50,257 tokens, many of those tokens never occur in our data set.

781
01:05:01,072 --> 01:05:08,002
So there's some very easy gains to be made here in the optimization, by for example taking the biases of all the logits that

782
01:05:08,002 --> 01:05:10,040
never occur and driving them to negative infinity.

783
01:05:11,024 --> 01:05:12,056
And that would basically just...

784
01:05:12,056 --> 01:05:18,046
it's just that all of these crazy Unicodes or different languages, those tokens never occur, so their probability should be very low.

785
01:05:18,094 --> 01:05:24,062
And so the gains that we should be seeing are along the lines of basically deleting the usage of tokens that

786
01:05:24,062 --> 01:05:25,032
never occur.

787
01:05:25,062 --> 01:05:29,010
That's probably most of the loss gain that we're going to see at this scale right now.

788
01:05:29,092 --> 01:05:38,068
But we shouldn't come to zero, because we are only doing 50 iterations, and I don't think that's enough to do an epoch right now.

789
01:05:39,032 --> 01:05:40,006
So let's see what we got.

790
01:05:41,074 --> 01:05:50,086
We have 338,000 tokens, which makes sense with our 3 to 1 compression ratio, because there are 1 million characters.

791
01:05:51,060 --> 01:05:56,086
So one epoch with the current setting of B and T will take 2,600 batches.

792
01:05:57,012 --> 01:06:00,070
And we're only doing 50 batches of optimization in here.

793
01:06:01,050 --> 01:06:07,096
So we start off in a familiar territory, as expected, and then we seem to come down to about 6.6.

794
01:06:08,044 --> 01:06:13,006
So basically, things seem to be working okay right now with respect to our expectations.

795
01:06:13,070 --> 01:06:14,010
So that's good.

796
01:06:14,056 --> 01:06:17,090
Okay, next I want to actually fix a bug that we have in our code.

797
01:06:18,062 --> 01:06:23,046
It's not a major bug, but it is a bug with respect to how GPT-2 training should happen.

798
01:06:24,092 --> 01:06:27,044
So the bug is the following.

799
01:06:28,000 --> 01:06:32,044
We were not being careful enough when we were loading the weights from HuggingFace, and we actually missed a little detail.

800
01:06:33,026 --> 01:06:40,022
So if we come here, notice that the shape of these two tensors is the same.

801
01:06:40,078 --> 01:06:51,014
So this one here is the token embedding at the bottom of the transformer, and this one here is the language modeling head at the top of the transformer.

802
01:06:52,040 --> 01:06:56,072
And both of these are basically two-dimensional tensors, and their shape is identical.

803
01:06:57,070 --> 01:07:04,057
So here, the first one is the output embedding, the token embedding, and the second one is this linear layer at the very top,

804
01:07:04,066 --> 01:07:05,038
the classifier layer.

805
01:07:05,088 --> 01:07:10,006
Both of them are of shape 50257x768.

806
01:07:11,094 --> 01:07:20,010
This one here is giving us our token embeddings at the bottom, and this one here is taking the 768 channels of the transformer and

807
01:07:20,010 --> 01:07:24,086
trying to upscale that to 50257 to get the loges for the next token.

808
01:07:25,070 --> 01:07:34,024
So they're both the same shape, but more than that, actually, if you look at comparing their elements in PyTorch,

809
01:07:34,036 --> 01:07:36,000
this is an element-wise equality.

810
01:07:36,020 --> 01:07:39,060
So then we use .all, and we see that every single element is identical.

811
01:07:40,038 --> 01:07:49,057
And more than that, we see that if we actually look at the data pointer, this is a way in PyTorch to get the actual pointer to the data and

812
01:07:49,057 --> 01:07:52,030
the storage, we see that actually the pointer is identical.

813
01:07:52,096 --> 01:07:59,046
So not only are these two separate tensors that happen to have the same shape and elements, they're actually pointing to the identical tensor.

814
01:08:00,052 --> 01:08:11,024
So what's happening here is that this is a common wait time scheme that actually comes from the original attention is all you need paper,

815
01:08:11,048 --> 01:08:13,010
and actually even the reference before it.

816
01:08:13,036 --> 01:08:25,076
So if we come here, embeddings in Softmax in the attention is all you need paper, they mention that in our model,

817
01:08:26,010 --> 01:08:32,040
we shared the same weight matrix between the two embedding layers and the pre-Softmax linear transformation similar to 30.

818
01:08:32,092 --> 01:08:39,038
So this is an awkward way to phrase that these two are shared and they're tied and they're the same matrix.

819
01:08:39,060 --> 01:08:42,010
And the 30 reference is this paper.

820
01:08:42,084 --> 01:08:45,020
So this came out in 2017.

821
01:08:46,002 --> 01:08:50,050
And you can read the full paper, but basically it argues for this wait time scheme.

822
01:08:51,012 --> 01:08:57,012
And I think intuitively the idea for why you might want to do this comes from this paragraph here.

823
01:08:57,072 --> 01:09:07,080
And basically, you can observe that you actually want these two matrices to behave similar in the following sense.

824
01:09:07,092 --> 01:09:15,068
If two tokens are very similar semantically, like maybe one of them is all lowercase and the other one is all uppercase,

825
01:09:15,068 --> 01:09:18,040
or it's the same token in a different language or something like that.

826
01:09:18,040 --> 01:09:24,012
If you have similarity between two tokens, presumably you would expect that they are nearby in the token embedding space.

827
01:09:24,086 --> 01:09:29,075
But in the exact same way, you'd expect that if you have two tokens that are similar semantically,

828
01:09:29,075 --> 01:09:36,067
 you'd expect them to get the same probabilities at the output of a transformer because they are semantically similar.

829
01:09:37,020 --> 01:09:48,002
And so both positions in the transformer at the very bottom and at the top have this property that similar tokens should have similar embeddings or similar weights.

830
01:09:48,062 --> 01:09:51,052
And so this is what motivates their exploration here.

831
01:09:51,072 --> 01:09:57,000
And they kind of, you know, I don't want to go through the entire paper, and you can go through it.

832
01:09:57,014 --> 01:09:58,026
But this is what they observe.

833
01:09:58,074 --> 01:10:03,016
They also observe that if you look at the output embeddings, they also behave like word embeddings.

834
01:10:04,040 --> 01:10:07,098
If you just kind of try to use those weights as word embeddings.

835
01:10:08,036 --> 01:10:16,050
So they kind of observe this similarity, they try to tie them, and they observe that they can get much better performance in that way.

836
01:10:16,050 --> 01:10:23,016
And so this was adopted in the attention is on paper, and then it was used again in GPT-2 as well.

837
01:10:23,056 --> 01:10:27,030
So I couldn't find it in the transformers implementation.

838
01:10:27,052 --> 01:10:29,084
I'm not sure where they tie those embeddings.

839
01:10:30,022 --> 01:10:35,026
But I can find it in the original GPT-2 code introduced by OpenAI.

840
01:10:35,050 --> 01:10:38,052
So this is OpenAI GPT-2 source model.

841
01:10:39,048 --> 01:10:44,012
And here where they are forwarding this model, and this is in TensorFlow, but that's okay.

842
01:10:44,056 --> 01:10:47,002
We see that they get the WTE token embeddings.

843
01:10:47,057 --> 01:10:51,082
And then here is the encoder of the token embeddings and the position.

844
01:10:52,056 --> 01:10:57,056
And then here at the bottom, they use the WTE again to do the logits.

845
01:10:57,092 --> 01:11:05,016
So when they get the logits, it's a mathmul of this output from the transformer, and the WTE tensor is reused.

846
01:11:06,098 --> 01:11:12,034
And so the WTE tensor basically is used twice on the bottom of the transformer and on the top of the transformer.

847
01:11:13,028 --> 01:11:18,032
And in the backward pass, we'll get gradients contributions from both branches, right?

848
01:11:18,040 --> 01:11:22,008
And these gradients will add up on the WTE tensor.

849
01:11:22,068 --> 01:11:25,040
So we'll get a contribution from the classifier layer.

850
01:11:25,086 --> 01:11:33,076
And then at the very end of the transformer, we'll get a contribution at the bottom of it, flowing again into the WTE tensor.

851
01:11:34,034 --> 01:11:40,008
So we want to, we are currently not sharing WTE in our code, but we want to do that.

852
01:11:41,028 --> 01:11:46,020
So weight sharing scheme.

853
01:11:46,062 --> 01:11:50,004
And one way to do this, let's see if Copilot gets it.

854
01:11:50,076 --> 01:11:51,034
Oh, it does.

855
01:11:51,054 --> 01:11:51,076
Okay.

856
01:11:52,032 --> 01:11:54,076
So this is one way to do it.

857
01:11:54,098 --> 01:11:58,082
Basically, relatively straightforward.

858
01:11:59,002 --> 01:12:07,036
What we're doing here is we're taking the wte.weight, and we're simply redirecting it to point to the lm_head.

859
01:12:07,086 --> 01:12:12,057
So this basically copies the data pointer, right?

860
01:12:12,062 --> 01:12:13,066
It copies the reference.

861
01:12:14,016 --> 01:12:22,076
And now the WTE.weight becomes orphaned, the old value of it, and PyTorch will clean it up, Python will clean it up.

862
01:12:23,030 --> 01:12:26,056
And so we are only left with a single tensor.

863
01:12:27,014 --> 01:12:29,050
And it's going to be used twice in the forward pass.

864
01:12:30,038 --> 01:12:33,070
And this is, to my knowledge, all that's required.

865
01:12:33,098 --> 01:12:36,012
So we should be able to use this.

866
01:12:36,034 --> 01:12:37,054
And this should probably train.

867
01:12:38,086 --> 01:12:41,070
We're just going to basically be using this exact same sensor twice.

868
01:12:42,010 --> 01:12:47,048
And we weren't being careful with tracking the likelihoods.

869
01:12:47,076 --> 01:12:52,038
But according to the paper, and according to the results, you'd actually expect slightly better results doing this.

870
01:12:52,096 --> 01:12:58,062
And in addition to that, one other reason that this is very, very nice for us is that this is a ton of parameters,

871
01:12:58,090 --> 01:12:59,006
right?

872
01:12:59,070 --> 01:13:00,074
What is the size of here?

873
01:13:00,090 --> 01:13:04,054
It's 768 times 50,257.

874
01:13:04,094 --> 01:13:06,080
So this is 40 million parameters.

875
01:13:07,012 --> 01:13:10,008
And this is a 124 million parameter model.

876
01:13:10,062 --> 01:13:12,012
So 40 divide 124.

877
01:13:12,030 --> 01:13:17,034
So this is like 30% of the parameters are being saved using this weight time scheme.

878
01:13:18,014 --> 01:13:21,026
And so this might be one of the reasons that this is working slightly better.

879
01:13:21,060 --> 01:13:26,096
If you're not training the model long enough, because of the weight time, you don't have to train as many parameters.

880
01:13:27,020 --> 01:13:31,010
And so you become more efficient in terms of the training process.

881
01:13:31,066 --> 01:13:39,000
Because you have fewer parameters, and you're putting in this inductive bias that these two embeddings should share similarities between tokens.

882
01:13:39,060 --> 01:13:41,048
So this is the weight time scheme.

883
01:13:41,090 --> 01:13:43,054
And we've saved a ton of parameters.

884
01:13:43,098 --> 01:13:46,056
And we expect our model to work slightly better because of this scheme.

885
01:13:47,034 --> 01:13:52,096
Okay, next, I would like us to be a bit more careful with the initialization and to try to follow the way GPT-2 initialized their model.

886
01:13:53,056 --> 01:13:59,038
Now, unfortunately, the GPT-2 paper and the GPT-3 paper are not very explicit about initialization.

887
01:13:59,050 --> 01:14:01,012
So we kind of have to read between the lines.

888
01:14:02,018 --> 01:14:08,080
And instead of going to the paper, which is quite vague, there's a bit of information in the code that OpenAI released.

889
01:14:08,080 --> 01:14:16,012
So when we go to the model.py, we see that when they initialize their weights, they are using the standard deviation of 0.02.

890
01:14:16,080 --> 01:14:23,032
And that's how they, so this is a normal distribution for the weights, and the standard deviation is 0.02.

891
01:14:24,010 --> 01:14:26,054
For the bias, they initialize that with zero.

892
01:14:27,076 --> 01:14:32,044
And then when we scroll down here, why is this not scrolling?

893
01:14:33,012 --> 01:14:40,072
The token embeddings are initialized at 0.02, and position embeddings at 0.01 for some reason.

894
01:14:41,044 --> 01:14:47,024
So those are the initializations, and we'd like to mirror that in GPT-2 in our module here.

895
01:14:47,066 --> 01:14:51,054
So here's a snippet of code that I sort of came up with very quickly.

896
01:14:52,094 --> 01:15:00,062
So what's happening here is at the end of our initializer for the GPT module, we're calling the apply function of an nn.Module.

897
01:15:01,000 --> 01:15:07,090
And that iterates all the sub-modules of this module and applies _init_weights function on them.

898
01:15:08,082 --> 01:15:16,036
And so what's happening here is that we're iterating all the modules here, and if they are an NN.linear module,

899
01:15:16,076 --> 01:15:21,046
then we're going to make sure to initialize the weight using a normal with a standard deviation of 0.02.

900
01:15:22,018 --> 01:15:25,072
If there's a bias in this layer, we will make sure to initialize that to zero.

901
01:15:26,036 --> 01:15:30,020
Note that zero initialization for the bias is not actually the PyTorch default.

902
01:15:31,028 --> 01:15:34,057
By default, the bias here is initialized with a uniform.

903
01:15:35,024 --> 01:15:36,046
So that's interesting.

904
01:15:37,004 --> 01:15:38,024
So we make sure to use zero.

905
01:15:38,096 --> 01:15:42,096
And for the embedding, we're just going to use 0.02 and keep it the same.

906
01:15:43,064 --> 01:15:47,036
So we're not going to change it to 0.01 for positional because it's about the same.

907
01:15:48,046 --> 01:15:54,036
And then if you look through our model, the only other layer that requires initialization and that has parameters is the layer norm.

908
01:15:54,076 --> 01:16:00,098
And the PyTorch default initialization sets the scale in the layer norm to be one and the offset in the layer norm to be zero.

909
01:16:01,002 --> 01:16:04,074
So that's exactly what we want, and so we're just going to keep it that way.

910
01:16:05,068 --> 01:16:16,026
And so this is the default initialization if we are following the GPT-2 source code that they released.

911
01:16:17,004 --> 01:16:22,060
I would like to point out, by the way, that typically the standard deviation here on this initialization,

912
01:16:22,074 --> 01:16:29,076
if you follow the He initialization, would be one over the square root of the number of features that are incoming into this layer.

913
01:16:29,076 --> 01:16:40,036
But if you'll notice, actually, 0.02 is basically consistent with that because the d model sizes inside these transformers for GPT-2 are roughly 768, 1600, etc.

914
01:16:40,036 --> 01:16:44,062
So one over the square root of, for example, 768 gives us 0.03.

915
01:16:45,052 --> 01:16:49,076
If we plug in 1600, we get 0.02.

916
01:16:50,028 --> 01:16:53,086
If we plug in three times that, 0.014, etc.

917
01:16:54,026 --> 01:17:02,074
So basically 0.02 is roughly in the vicinity of reasonable values for these initializations anyway.

918
01:17:03,088 --> 01:17:13,030
So it's not completely crazy to be hard coding 0.02 here, but you'd like typically something that grows with the model size instead.

919
01:17:13,074 --> 01:17:17,042
But we will keep this because that is the GPT-2 initialization per their source code.

920
01:17:17,098 --> 01:17:21,040
But we are not fully done yet on initialization because there's one more caveat here.

921
01:17:22,016 --> 01:17:29,046
So here, a modified initialization which accounts for the accumulation on the residual path with model depth is used.

922
01:17:30,000 --> 01:17:36,038
We scale the weight of residual layers initialization by a factor of one over square root of n, where n is the number of residual layers.

923
01:17:36,038 --> 01:17:38,000
So this is what GPT-2 paper says.

924
01:17:38,057 --> 01:17:41,078
So we have not implemented that yet, and we can do so now.

925
01:17:42,044 --> 01:17:46,044
Now I'd like to actually kind of like motivate a little bit what they mean here, I think.

926
01:17:47,034 --> 01:17:48,062
So here's roughly what they mean.

927
01:17:50,002 --> 01:17:59,044
If you start out with zeros in your residual stream, remember that each residual stream is of this form where we continue adding to it.

928
01:17:59,044 --> 01:18:02,044
x is x plus something, some kind of contribution.

929
01:18:02,096 --> 01:18:09,088
So every single block of the residual network contributes some amount and it gets added.

930
01:18:10,072 --> 01:18:17,018
And so what ends up happening is that the variance of the activations in the residual stream grows.

931
01:18:17,096 --> 01:18:19,018
So here's a small example.

932
01:18:19,032 --> 01:18:26,068
If we start at zero, and then we for 100 times, we have sort of this residual stream of 768 zeros.

933
01:18:27,036 --> 01:18:35,032
And then 100 times we add random, which is a normal distribution, zero mean, one standard deviation.

934
01:18:35,084 --> 01:18:40,012
If we add to it, then by the end, the residual stream has grown to have standard deviation of 10.

935
01:18:40,068 --> 01:18:46,096
And that's just because we're always adding these numbers.

936
01:18:47,057 --> 01:18:52,066
And so this scaling factor that they use here exactly compensates for that growth.

937
01:18:53,024 --> 01:19:02,000
So if we take n, and we basically scale down every one of these contributions into the residual stream by one over the square root of n.

938
01:19:02,070 --> 01:19:06,098
So one over the square root of n is n to the negative 0.5, right?

939
01:19:08,020 --> 01:19:13,080
Because n to the 0.5 is the square root, and then one over the square root is n negative 0.5.

940
01:19:14,006 --> 01:19:19,016
If we scale it in this way, then we see that we actually get one.

941
01:19:20,046 --> 01:19:27,092
So this is a way to control the growth of activations inside the residual stream in the forward pass.

942
01:19:28,064 --> 01:19:34,040
And so we'd like to initialize in the same way, where these weights that are at the end of each block,

943
01:19:34,062 --> 01:19:43,042
so this c_proj layer, the GPT paper proposes to scale down those weights by one over the square root of the number of residual layers.

944
01:19:44,052 --> 01:19:47,010
So one crude way to implement this is the following.

945
01:19:47,070 --> 01:19:51,016
I don't know if this is PyTorch sanctioned, but it works for me.

946
01:19:51,060 --> 01:19:53,072
It is we all do in the initialization.

947
01:19:54,030 --> 01:20:03,024
See that special nano-GPT scale in it is one.

948
01:20:03,078 --> 01:20:06,090
So we're setting kind of like a flag for this module.

949
01:20:07,084 --> 01:20:09,030
There must be a better way than PyTorch, right?

950
01:20:09,038 --> 01:20:11,002
But I don't know.

951
01:20:12,040 --> 01:20:17,096
Okay, so we're basically attaching this flag and trying to make sure that it doesn't conflict with anything previously.

952
01:20:19,004 --> 01:20:22,084
And then when we come down here, this std should be 0.02 by default.

953
01:20:25,002 --> 01:20:33,006
But then if hasAtter module of this thing, then std times equals...

954
01:20:33,006 --> 01:20:34,002
Std times equals...

955
01:20:37,044 --> 01:20:38,074
Copilot is not guessing correctly.

956
01:20:39,036 --> 01:20:41,092
So we want one over the square root of the number of layers.

957
01:20:41,092 --> 01:20:55,038
So the number of residual layers here is twice times self.config_n_layers, and then this times negative 0.5.

958
01:20:55,076 --> 01:21:02,014
So we want to scale down that standard deviation, and this should be correct and implement that.

959
01:21:02,064 --> 01:21:10,032
I should clarify, by the way, that the two times number of layers comes from the fact that every single one of our layers in the transformer actually has two blocks that

960
01:21:10,032 --> 01:21:11,098
add to the residual pathway, right?

961
01:21:12,022 --> 01:21:14,000
We have the attention and then the MLP.

962
01:21:14,026 --> 01:21:15,082
So that's where the two times comes from.

963
01:21:16,088 --> 01:21:23,002
And the other thing to mention is that what's slightly awkward, but we're not going to fix it, is that

964
01:21:23,002 --> 01:21:33,018
because we are weight sharing the wte and the lm_head, in this iteration over all submodules, we're going to actually come around to that tensor twice.

965
01:21:33,018 --> 01:21:36,073
So we're going to first initialize it as an embedding with 0.02, 

966
01:21:36,073 --> 01:21:41,098
and then we're going to come back around it again in a linear and initialize it again using 0.02.

967
01:21:42,054 --> 01:21:47,056
And it's going to be 0.02 because the lm_head is of course not scaled, so it's not going to come here.

968
01:21:48,008 --> 01:21:53,038
It's just going to be basically initialized twice using the identical same initialization, but that's okay.

969
01:21:54,034 --> 01:22:03,040
And then scrolling over here, I added some code here so that we have reproducibility to set the seeds.

970
01:22:03,062 --> 01:22:08,042
And now we should be able to Python train GPT-2.py and let this running.

971
01:22:08,066 --> 01:22:13,064
And as far as I know, this is the GPT-2 initialization in the way we've implemented it right now.

972
01:22:14,038 --> 01:22:18,012
So this looks reasonable to me.

973
01:22:19,000 --> 01:22:21,038
Okay, so at this point, we have the GPT-2 model.

974
01:22:21,057 --> 01:22:23,052
We have some confidence that it's correctly implemented.

975
01:22:23,084 --> 01:22:28,068
We've initialized it properly, and we have a data loader that's iterating through data batches, and we can train.

976
01:22:29,022 --> 01:22:30,046
So now comes the fun part.

977
01:22:30,072 --> 01:22:32,096
I'd like us to speed up the training by a lot.

978
01:22:33,028 --> 01:22:36,096
So we're getting our money's worth with respect to the hardware that we are using here.

979
01:22:37,072 --> 01:22:39,074
And we're going to speed up the training by quite a bit.

980
01:22:40,018 --> 01:22:46,056
Now, you always want to start with what hardware do you have, what does it offer, and are you fully utilizing it?

981
01:22:47,012 --> 01:23:00,052
So in my case, if we go to NVIDIA-SMI, we can see that I have eight GPUs, and each one of those GPUs is an A100 SXM 80 gigabytes.

982
01:23:01,022 --> 01:23:04,052
So this is the GPU that I have available to me in this box.

983
01:23:05,026 --> 01:23:12,086
Now, when I use to spin up these kinds of boxes, by the way, my favorite place to go to is Lambda Labs.

984
01:23:13,084 --> 01:23:19,044
They do sponsor my development and that of my projects, but this is my favorite place to go.

985
01:23:19,092 --> 01:23:24,010
And this is where you can spin up one of these machines, and you pay per hour, and it's very, very simple.

986
01:23:25,002 --> 01:23:28,030
So I like to spin them up and then connect VS Code to it, and that's how I develop.

987
01:23:29,026 --> 01:23:38,030
Now, when we look at the A100s that are available here, A100 80 gigabyte SXM is the GPU that I have here.

988
01:23:38,086 --> 01:23:43,076
And we have a bunch of numbers here for how many calculations you can expect out of this GPU.

989
01:23:44,064 --> 01:23:51,046
So when I come over here and I break in right after here, so Python train GPT.

990
01:23:52,006 --> 01:23:54,092
So I'm breaking in right after we calculate the logits and the loss.

991
01:23:56,044 --> 01:24:04,060
And the interesting thing I'd like you to note is when I do logits.dtype, this prints a torch.float32.

992
01:24:05,008 --> 01:24:11,090
So by default, in PyTorch, when you create tensors, and this is the case for all the activations and for the parameters of the network and

993
01:24:11,090 --> 01:24:14,048
so on, by default, everything is in float32.

994
01:24:15,022 --> 01:24:22,048
That means that every single number, activation or weight and so on, is using a float representation that

995
01:24:22,048 --> 01:24:23,057
has 32 bits.

996
01:24:24,066 --> 01:24:26,050
And that's actually quite a bit of memory.

997
01:24:26,064 --> 01:24:30,086
And it turns out empirically that for deep learning as a computational workload, this is way too much.

998
01:24:31,042 --> 01:24:36,000
And deep learning and the training of these networks can tolerate significantly lower precisions.

999
01:24:36,084 --> 01:24:40,018
Not all computational workloads can tolerate small precision.

1000
01:24:40,060 --> 01:24:48,052
So for example, if we go back to the data sheet, you'll see that actually these GPUs support up to FP64.

1001
01:24:49,010 --> 01:24:53,062
And this is quite useful, I understand, for a lot of scientific computing applications.

1002
01:24:54,004 --> 01:24:55,024
And there, they really need this.

1003
01:24:55,064 --> 01:24:58,018
But we don't need that much precision for deep learning training.

1004
01:24:58,072 --> 01:25:01,028
So currently, we are here, FP32.

1005
01:25:02,014 --> 01:25:09,084
And with this code as it is right now, we expect to get at most 19.5 TFLOPS of performance.

1006
01:25:10,024 --> 01:25:14,044
That means we're doing 19.5 trillion operations, floating point operations.

1007
01:25:14,088 --> 01:25:19,088
So this is floating point multiply add most likely.

1008
01:25:21,028 --> 01:25:23,066
And so these are the floating point operations.

1009
01:25:25,056 --> 01:25:33,014
Now, notice that if we are willing to go down in precision, so TF32 is a lower precision format we're going to see in a second,

1010
01:25:33,054 --> 01:25:35,064
you can actually get an 8x improvement here.

1011
01:25:36,012 --> 01:25:42,028
And if you're willing to go down to float 16 or bfloat 16, you can actually get times 16x performance,

1012
01:25:42,052 --> 01:25:44,074
all the way to 312 TFLOPS.

1013
01:25:45,066 --> 01:25:48,090
You see here that NVIDIA likes to cite numbers that have an asterisk here.

1014
01:25:49,032 --> 01:25:54,080
This asterisk says with sparsity, but we are not going to be using sparsity in our code.

1015
01:25:55,008 --> 01:25:58,020
And I don't know that this is very widely used in the industry right now.

1016
01:25:58,052 --> 01:26:01,078
So most people look at this number here without sparsity.

1017
01:26:02,042 --> 01:26:04,090
And you'll notice that we could get even more here.

1018
01:26:05,028 --> 01:26:06,056
But this is int8.

1019
01:26:06,094 --> 01:26:18,042
And int8 is used for inference, not for training, because int8 has a, it basically has uniform spacing.

1020
01:26:18,096 --> 01:26:29,040
And we actually require a float so that we get a better match to the normal distributions that occur during training of neural networks,

1021
01:26:29,064 --> 01:26:33,000
where both activations and weights are distributed as a normal distribution.

1022
01:26:33,078 --> 01:26:38,030
And so floating points are really important to match that representation.

1023
01:26:39,020 --> 01:26:43,057
So we're not typically using int8 for training, but we are using it for inference.

1024
01:26:44,038 --> 01:26:50,074
And if we bring down the precision, we can get a lot more TFLOPS out of the tensor cores available in the GPUs.

1025
01:26:50,094 --> 01:26:52,057
We'll talk about that in a second.

1026
01:26:52,057 --> 01:26:59,070
But in addition to that, if all of these numbers have fewer bits of representation, it's going to be much easier to move them around.

1027
01:27:00,036 --> 01:27:03,094
And that's where we start to get into the memory bandwidth and the memory of the model.

1028
01:27:04,064 --> 01:27:10,078
So not only do we have a finite capacity of the number of bits that our GPU can store, but in addition to that,

1029
01:27:11,000 --> 01:27:13,044
there's a speed with which you can access this memory.

1030
01:27:13,096 --> 01:27:16,064
And you have a certain memory bandwidth.

1031
01:27:16,088 --> 01:27:18,034
It's a very precious resource.

1032
01:27:19,020 --> 01:27:23,062
And in fact, many of the deep learning workloads for training are memory bound.

1033
01:27:24,012 --> 01:27:29,008
And what that means is actually that the tensor cores that do all these extremely fast multiplications,

1034
01:27:29,046 --> 01:27:30,082
most of the time they're waiting around.

1035
01:27:30,096 --> 01:27:31,057
They're idle.

1036
01:27:32,026 --> 01:27:35,038
Because we can't feed them with data fast enough.

1037
01:27:35,078 --> 01:27:37,082
We can't load the data fast enough for memory.

1038
01:27:38,034 --> 01:27:44,030
So typical utilizations of your hardware, if you're getting 60% utilization, you're actually doing extremely well.

1039
01:27:45,046 --> 01:27:51,084
So half of the time in a well-tuned application, your tensor cores are not doing multiplies because the data is not available.

1040
01:27:52,054 --> 01:27:54,086
So the memory bandwidth here is extremely important as well.

1041
01:27:55,038 --> 01:28:02,032
And if we come down in the precision for all the floats, all the numbers, weights, and activations suddenly require less memory.

1042
01:28:03,010 --> 01:28:05,064
So we can store more and we can access it faster.

1043
01:28:06,018 --> 01:28:07,098
So everything speeds up and it's amazing.

1044
01:28:08,054 --> 01:28:10,060
And now let's reap the benefits of it.

1045
01:28:11,018 --> 01:28:14,020
And let's first look at the tensor float 32 format.

1046
01:28:15,062 --> 01:28:17,008
So first of all, what are tensor cores?

1047
01:28:17,074 --> 01:28:22,076
Well, tensor cores is just an instruction in the A100 architecture.

1048
01:28:23,022 --> 01:28:28,002
So what it does is it does basically a little 4x4 matrix multiply.

1049
01:28:28,066 --> 01:28:33,064
So this is just matrix multiplication here of 4x4 matrices.

1050
01:28:34,002 --> 01:28:43,057
And there are multiple configurations as to what precision any of these matrices are, in what precision the internal accumulate happens,

1051
01:28:44,028 --> 01:28:46,074
and then what is the output precision, input precision, etc.

1052
01:28:47,024 --> 01:28:50,006
So there's a few switches, but it's basically a 4x4 multiply.

1053
01:28:50,056 --> 01:29:00,050
And then any time we have any operations that require matrix multiplication, they get broken up into this instruction of a little 4x4 multiply.

1054
01:29:00,092 --> 01:29:05,002
And so everything gets broken up into this instruction because it's the fastest way to multiply matrices.

1055
01:29:05,057 --> 01:29:11,076
And it turns out that most of the computational work that we're doing up above, all of it really is matrix multiplication.

1056
01:29:12,038 --> 01:29:19,056
Most of the work computationally happens in the linear layers, linear, linear, etc.

1057
01:29:20,006 --> 01:29:21,098
There's a few things sandwiched in between.

1058
01:29:22,030 --> 01:29:27,044
So there's some additions in residuals, there's some GELU nonlinearities, there's some layer norms, etc.

1059
01:29:27,054 --> 01:29:30,057
But if you just time them, you'll see that these are nothing.

1060
01:29:30,088 --> 01:29:35,068
Like basically, the entire transformer is just a bunch of matrix multiplications, really.

1061
01:29:36,086 --> 01:29:45,064
And especially at this small scale, 124 million parameter model, actually, the biggest matrix multiplication by far is the classifier layer at the top.

1062
01:29:46,002 --> 01:29:51,034
That is a massive matrix multiply of going from 768 to 50,257.

1063
01:29:51,076 --> 01:29:56,032
And that matrix multiply dominates anything else that happens in that network, roughly speaking.

1064
01:29:57,042 --> 01:30:02,082
So it's matrix multiplies that become a lot faster, which are hidden inside our linear layers.

1065
01:30:03,070 --> 01:30:05,070
And they're accelerated through Tensor Cores.

1066
01:30:06,024 --> 01:30:13,044
Now, the best reference I would say for Tensor Cores is basically just go to the A100 architecture white paper.

1067
01:30:14,008 --> 01:30:15,070
And then it's pretty detailed.

1068
01:30:16,044 --> 01:30:21,030
But I think people, it's like relatively readable mostly, if you half understand what's happening.

1069
01:30:23,030 --> 01:30:25,042
So figure nine, TensorFloat 32.

1070
01:30:26,028 --> 01:30:30,012
So this is the explanation basically for TF 32 and what happens here.

1071
01:30:30,094 --> 01:30:33,066
And you see that there's many configuration options here available.

1072
01:30:33,096 --> 01:30:48,042
So the input operands and what precisions are they in, the accumulator and what basically the internal representation within the instruction when you do the accumulate of this matrix multiplication.

1073
01:30:49,062 --> 01:30:56,068
So the intermediate plus equals of the intermediate little vector multiplies here, that all happens in FP 32.

1074
01:30:57,024 --> 01:31:02,046
And then this is an Apex improvement, as I mentioned, to the tops that we got.

1075
01:31:02,098 --> 01:31:05,050
So TF 32 specifically, we're looking at this row here.

1076
01:31:06,002 --> 01:31:13,024
And the way this works is normally FP 32 has 32 bits.

1077
01:31:14,008 --> 01:31:17,052
TF 32 is the exact same bits.

1078
01:31:17,078 --> 01:31:24,074
We have one sign bit, we have eight exponent bits, except the mantissa bits get cropped in the float.

1079
01:31:24,090 --> 01:31:33,063
And so basically, we end up with just 19 bits instead of 32 bits, because the last 13 bits get truncated,

1080
01:31:33,063 --> 01:31:34,038
they get dropped.

1081
01:31:35,020 --> 01:31:37,070
And all this is internal to the instruction.

1082
01:31:38,024 --> 01:31:41,006
So none of it is visible to anything in our PyTorch.

1083
01:31:41,082 --> 01:31:45,020
None of our PyTorch code will change, all of the numbers will look identical.

1084
01:31:45,068 --> 01:31:53,070
It's just that when you call the tensor core instruction, internally in the hardware, it will crop out these 13 bits.

1085
01:31:53,094 --> 01:32:00,062
And that allows it to calculate this little matrix multiply significantly faster, 8x faster.

1086
01:32:01,036 --> 01:32:03,080
Now, of course, this speed up comes at a cost.

1087
01:32:04,016 --> 01:32:10,026
And the cost is that we are reducing the precision, our accumulate is still in FP 32, our output is FP 32,

1088
01:32:10,057 --> 01:32:11,090
our inputs are FP 32.

1089
01:32:12,040 --> 01:32:17,018
But internally, things get truncated in the operands to perform the operation faster.

1090
01:32:17,062 --> 01:32:19,086
And so our results are starting to be a bit more approximate.

1091
01:32:20,038 --> 01:32:23,062
But empirically, when you actually train with this, you basically can't tell the difference.

1092
01:32:24,042 --> 01:32:32,002
So the reason I like TF 32 is because if you can tolerate a little bit of a precision fudge, then this is free,

1093
01:32:32,042 --> 01:32:39,020
like none of your code sees this, it's fully internal to the operation, and the operation to you just go 8x faster,

1094
01:32:39,020 --> 01:32:40,044
and it's a bit more approximate.

1095
01:32:41,010 --> 01:32:43,094
And so it's a pretty sweet spot, I would say in optimization.

1096
01:32:44,064 --> 01:32:46,064
And let's see what that looks like first.

1097
01:32:46,096 --> 01:32:50,040
So I've set up our codes to just time the iterations.

1098
01:32:50,082 --> 01:32:57,038
So import time, I changed the hyper parameters so that we have something a bit more that reflects a kind of workload that

1099
01:32:57,038 --> 01:33:00,092
we want to run, because we want to do a fairly large run at the end of this.

1100
01:33:01,024 --> 01:33:02,092
So let's use batch size 16.

1101
01:33:03,042 --> 01:33:08,052
And let's now use the actual GPT to maximum sequence length of 1024 tokens.

1102
01:33:09,068 --> 01:33:11,050
So this is the configuration.

1103
01:33:12,050 --> 01:33:19,096
And then for 50 iterations, I'm just doing something very lazy here, I'm doing time.time to get the current time.

1104
01:33:20,042 --> 01:33:22,032
And then this is the optimization loop.

1105
01:33:23,002 --> 01:33:25,010
And now I want to time how long this takes.

1106
01:33:25,046 --> 01:33:37,016
Now, one issue with working with GPUs is that as your CPU, when your CPU runs, it's just scheduling work on GPU,

1107
01:33:37,034 --> 01:33:38,096
it's ordering some work, right?

1108
01:33:39,036 --> 01:33:41,074
And so it sends a request, and then it continues running.

1109
01:33:42,030 --> 01:33:47,050
And so we can actually, it can happen sometimes that we sort of speed through this.

1110
01:33:47,064 --> 01:33:51,008
And we queue up a lot of kernels to run on the GPU.

1111
01:33:51,040 --> 01:33:54,020
And then the CPU sort of like gets here and takes time to time.

1112
01:33:54,048 --> 01:33:59,050
But actually, the GPU is still running, because it takes a time to actually work through the work that

1113
01:33:59,050 --> 01:34:00,062
was scheduled to run.

1114
01:34:01,054 --> 01:34:04,000
And so you're just building up a queue for the GPU.

1115
01:34:04,042 --> 01:34:08,050
And so actually, if you need to, you want to wait, Torch.cuda.synchronize.

1116
01:34:08,062 --> 01:34:14,050
And this will wait for the GPU to finish all the work that was scheduled to run up above here.

1117
01:34:14,074 --> 01:34:16,070
And then we can actually take the time.

1118
01:34:17,024 --> 01:34:23,024
So basically, we're waiting for the GPU to stop this iteration, take time, and then we're going to just print it.

1119
01:34:23,048 --> 01:34:27,008
So here, I'm going to run the training loop.

1120
01:34:27,036 --> 01:34:30,034
And here on the right, I'm watching NVIDIA SMI.

1121
01:34:30,050 --> 01:34:32,008
So we start off at zero.

1122
01:34:32,066 --> 01:34:34,014
We're not using the GPU.

1123
01:34:34,074 --> 01:34:36,062
And then by default, PyTorch will use GPU zero.

1124
01:34:36,082 --> 01:34:38,002
So we see that it gets filled up.

1125
01:34:38,062 --> 01:34:42,012
And we're using 35 gigabytes out of 80 gigabytes available.

1126
01:34:43,006 --> 01:34:53,002
And then here on the left, we see that because we cranked up the batch size, now it's only 20 batches to do a single epoch on our tiny Shakespeare.

1127
01:34:53,084 --> 01:34:57,086
And we see that we're seeing roughly a thousand milliseconds per iteration here.

1128
01:34:58,016 --> 01:35:03,032
So the first iteration sometimes is slower.

1129
01:35:03,084 --> 01:35:07,022
And that's because PyTorch might be doing a lot of initializations here on the very first iteration.

1130
01:35:07,094 --> 01:35:11,094
And so it's probably initializing all these tensors and buffers to hold all the gradients.

1131
01:35:12,016 --> 01:35:17,012
And I'm not 100% sure all the work that happens here, but this could be a slower iteration.

1132
01:35:17,040 --> 01:35:19,088
When you're timing your logic, you always want to be careful with that.

1133
01:35:20,030 --> 01:35:23,034
But basically, we're seeing a thousand milliseconds per iteration.

1134
01:35:24,080 --> 01:35:28,004
And so this will run for roughly 50 seconds as we have it right now.

1135
01:35:28,064 --> 01:35:31,006
So that's our baseline in Float32.

1136
01:35:31,068 --> 01:35:37,092
One more thing I wanted to mention is that if this doesn't fit into your GPU and you're getting out of memory errors,

1137
01:35:38,026 --> 01:35:40,062
then start decreasing your batch size until things fit.

1138
01:35:40,094 --> 01:35:47,008
So instead of 16, try 8 or 4 or whatever you need to fit the batch into your GPU.

1139
01:35:47,066 --> 01:35:51,016
And if you have a bigger GPU, you can actually potentially get away with 32 and so on.

1140
01:35:51,086 --> 01:35:56,068
By default, you want to basically max out the batch size that fits on your GPU.

1141
01:35:57,048 --> 01:35:59,008
And you want to keep it nice numbers.

1142
01:35:59,042 --> 01:36:02,070
So use numbers that have lots of powers of 2 in them.

1143
01:36:03,022 --> 01:36:04,038
So 16 is a good number.

1144
01:36:04,062 --> 01:36:08,070
8, 24, 32, 48, these are nice numbers.

1145
01:36:09,004 --> 01:36:12,082
But don't use something like 17, because that will run very inefficiently on the GPU.

1146
01:36:13,070 --> 01:36:15,052
And we're going to see that a bit later as well.

1147
01:36:16,052 --> 01:36:18,070
So for now, let's just stick with 16, 1024.

1148
01:36:19,078 --> 01:36:28,056
And the one thing that I added also here, and I ran it again, is I'm calculating a tokens per second throughput during training.

1149
01:36:29,030 --> 01:36:35,062
Because we might end up changing the batch size around over time, but tokens per second is the objective measure that

1150
01:36:35,062 --> 01:36:36,052
we actually really care about.

1151
01:36:36,070 --> 01:36:39,032
How many tokens of data are we training on?

1152
01:36:39,046 --> 01:36:42,014
And what is the throughput of tokens that we're getting in our optimization?

1153
01:36:42,086 --> 01:36:48,002
So right now, we're processing and training on 163,000 tokens per second, roughly.

1154
01:36:48,006 --> 01:36:50,020
And that's a bit more objective metric.

1155
01:36:51,034 --> 01:36:53,024
Okay, so let's now enable TF32.

1156
01:36:53,048 --> 01:36:56,060
Now, luckily, PyTorch makes this fairly easy for us.

1157
01:36:57,024 --> 01:37:00,080
And to enable TF32, you just need to do a single line.

1158
01:37:00,092 --> 01:37:02,010
And it's this.

1159
01:37:02,076 --> 01:37:09,062
And when we go to the PyTorch documentation here for this function, basically, this tells PyTorch what kind of kernels to run.

1160
01:37:09,084 --> 01:37:14,028
And by default, I believe it is highest, highest precision for MatMul.

1161
01:37:14,074 --> 01:37:18,080
And that means that everything happens in float32, just like it did before.

1162
01:37:19,033 --> 01:37:26,056
But if we set it to high, as we do right now, matrix multiplications will now use TensorFloat32 when it's available.

1163
01:37:27,032 --> 01:37:29,022
My GPU is the A100.

1164
01:37:29,060 --> 01:37:31,044
So it's an Ampere series.

1165
01:37:31,078 --> 01:37:33,050
And therefore, TF32 is available.

1166
01:37:33,082 --> 01:37:36,084
If you have an older GPU, this might not be available for you.

1167
01:37:37,072 --> 01:37:39,002
But for my GPU, it's available.

1168
01:37:39,054 --> 01:37:44,093
And so what I expect PyTorch to do is that every single place where we see an nn.linear, inside there,

1169
01:37:44,093 --> 01:37:46,008
there's a matrix multiplication.

1170
01:37:46,052 --> 01:37:53,060
And I expect that matrix multiplication now to be running on Tensor Cores, utilizing the TF32 precision.

1171
01:37:54,077 --> 01:37:59,015
So this is the single line of change that is, I believe, necessary.

1172
01:37:59,036 --> 01:38:00,016
And let's rerun this.

1173
01:38:00,072 --> 01:38:07,057
Now, we saw that in terms of the throughput that is promised to us, we're supposed to be getting 8x, roughly.

1174
01:38:08,006 --> 01:38:09,010
So let's see what happens.

1175
01:38:11,076 --> 01:38:14,074
And that 8x came from here, right?

1176
01:38:15,057 --> 01:38:16,084
8x.

1177
01:38:17,030 --> 01:38:24,078
And it also came from looking at it here, 156 TFLOPS instead of 19.5.

1178
01:38:25,052 --> 01:38:26,066
Okay, so what actually happened?

1179
01:38:27,050 --> 01:38:31,088
So we're seeing that our throughput, roughly 3x, not 8x.

1180
01:38:32,046 --> 01:38:40,096
So we are going, we're from 1000 milliseconds, we're going down to 300 milliseconds, and our throughput is now about 50,000 tokens per second.

1181
01:38:41,020 --> 01:38:43,046
So we have a roughly 3x instead of 8x.

1182
01:38:43,060 --> 01:38:44,034
So what happened?

1183
01:38:45,014 --> 01:38:49,048
And basically, what's happening here is, again, a lot of these workloads are memory bound.

1184
01:38:50,010 --> 01:39:01,038
And so even though the TF32 offers, in principle, a lot faster throughput, all of these numbers everywhere are still float 32s.

1185
01:39:01,038 --> 01:39:05,048
And it's float 32 numbers that are being shipped all over the place through the memory system.

1186
01:39:05,068 --> 01:39:09,014
And it's just costing us way too much time to shuttle around all this data.

1187
01:39:09,070 --> 01:39:15,098
And so even though we've made the multiply itself much faster, we are memory bound, and we're not actually seeing the full benefit that 

1188
01:39:15,098 --> 01:39:18,086
would come from this napkin math here.

1189
01:39:19,060 --> 01:39:22,044
That said, we are getting one, a 3x faster throughput.

1190
01:39:22,066 --> 01:39:23,076
And this is free.

1191
01:39:25,032 --> 01:39:30,006
Single line of code in PyTorch, all your variables are still float 32 everywhere.

1192
01:39:30,054 --> 01:39:35,026
It just runs faster, and it's slightly more approximate, but we're not going to notice it, basically.

1193
01:39:35,096 --> 01:39:37,057
So that's TF32.

1194
01:39:38,040 --> 01:39:39,042
Okay, so let's now continue.

1195
01:39:39,098 --> 01:39:42,048
So we've exercised this row.

1196
01:39:42,062 --> 01:39:47,096
And we saw that we can crop out some of the precision inside the operation itself.

1197
01:39:48,057 --> 01:39:50,006
But we saw that we're still memory bound.

1198
01:39:50,016 --> 01:39:52,018
We're still moving around all these floats, right?

1199
01:39:52,028 --> 01:39:52,060
Otherwise.

1200
01:39:52,078 --> 01:39:54,078
And we're paying that cost because of this.

1201
01:39:55,056 --> 01:39:58,070
So let's now decrease the amount of stuff that we're going to be moving around.

1202
01:39:59,036 --> 01:40:02,056
And we're going to do that by dropping down to Bfloat16.

1203
01:40:03,040 --> 01:40:06,034
So we're only going to be maintaining 16 bits per float.

1204
01:40:06,098 --> 01:40:08,066
And we're going to use the Bfloat16.

1205
01:40:08,072 --> 01:40:11,018
I'll explain in a bit FP16 difference.

1206
01:40:11,072 --> 01:40:13,016
And we're going to be in this row.

1207
01:40:13,070 --> 01:40:23,074
So when we go back to the documentation here for the A100, we see here the precisions that are available.

1208
01:40:24,056 --> 01:40:25,092
And this is the original FP32.

1209
01:40:26,016 --> 01:40:28,042
The TF32 crops out the precision.

1210
01:40:29,034 --> 01:40:38,006
And then here in Bfloat16, you see that it is very similar to TF32, but it's even more aggressive in cropping off the precision,

1211
01:40:38,030 --> 01:40:39,068
the mantissa of this float.

1212
01:40:40,036 --> 01:40:45,098
So the important thing with Bfloat16 is that the exponent bits and the sign bit, of course, remain unchanged.

1213
01:40:46,048 --> 01:40:53,030
So if you're familiar with your float numbers, and I think this should probably be an entire video by itself,

1214
01:40:53,080 --> 01:40:57,046
the exponent sets the range that you can represent of your numbers.

1215
01:40:57,088 --> 01:41:01,060
And the precision is how much precision you have for your numbers.

1216
01:41:02,042 --> 01:41:12,068
And so the range of numbers is identical, but we have fewer possibilities within that range, because we are truncating the mantissa.

1217
01:41:12,084 --> 01:41:14,054
So we have less precision in that range.

1218
01:41:15,028 --> 01:41:21,030
What that means is that things are actually fairly nice, because we have the original range of numbers that

1219
01:41:21,030 --> 01:41:25,028
are representable in float, but we just have less precision for it.

1220
01:41:26,006 --> 01:41:30,036
And the difference with FP16 is that they actually touch and change the range.

1221
01:41:30,096 --> 01:41:34,068
So FP16 cannot represent the full range of FP32.

1222
01:41:34,090 --> 01:41:36,034
It has a reduced range.

1223
01:41:36,076 --> 01:41:42,027
And that's where you start to actually run into issues, because now you need these gradient scalars and

1224
01:41:42,027 --> 01:41:43,002
things like that.

1225
01:41:43,030 --> 01:41:48,062
And I'm not going to go into the detail of that in this video, because that's a whole video by itself.

1226
01:41:49,026 --> 01:41:51,040
But FP16 actually historically came first.

1227
01:41:51,066 --> 01:41:54,094
That was available in the Volta series before Ampere.

1228
01:41:55,054 --> 01:41:58,068
And so FP16 came first, and everyone started to train FP16.

1229
01:41:59,006 --> 01:42:06,014
But everyone had to use all these gradient scaling operations, which are kind of annoying, and it's an additional source of state and complexity.

1230
01:42:06,014 --> 01:42:10,012
And the reason for that was because the exponent range was reduced in FP16.

1231
01:42:10,098 --> 01:42:13,024
So that's the IEEE FP16's spec.

1232
01:42:13,064 --> 01:42:16,022
And then they came out with BF16 and the Ampere.

1233
01:42:16,044 --> 01:42:21,054
And they made it much simpler, because we're just truncating mantissa, we have the exact same range, and

1234
01:42:21,054 --> 01:42:23,002
we do not need gradient scalars.

1235
01:42:23,038 --> 01:42:24,082
So everything is much, much simpler.

1236
01:42:25,064 --> 01:42:32,016
Now, when we do use BF16, though, we are impacting the numbers that we might be seeing in our PyTorch code.

1237
01:42:33,028 --> 01:42:36,042
This change is not just local to the operation itself.

1238
01:42:36,096 --> 01:42:38,028
So let's see how that works.

1239
01:42:39,094 --> 01:42:43,013
There's some documentation here that...

1240
01:42:43,013 --> 01:42:50,064
So I think this is probably the best page to explain how to use mixed precision in PyTorch, because there are many other tutorials and

1241
01:42:50,064 --> 01:42:54,024
so on, even within PyTorch documentation, that are a lot more confusing.

1242
01:42:54,080 --> 01:42:59,078
And so I recommend specifically this one, because there's five other copies that I would not recommend.

1243
01:43:00,072 --> 01:43:04,092
And then when we come here, ignore everything about everything.

1244
01:43:05,012 --> 01:43:06,064
Ignore everything about gradient scalars.

1245
01:43:08,002 --> 01:43:10,088
And only look at Torch.autocast.

1246
01:43:11,068 --> 01:43:15,004
And basically, also, this comes to a single line of code at the end.

1247
01:43:15,044 --> 01:43:18,002
So this is the context manager that we want.

1248
01:43:18,072 --> 01:43:22,004
And we want to use that in our network.

1249
01:43:22,056 --> 01:43:28,040
When you click into the Torch.autocast, autocasting, it has a few more...

1250
01:43:28,096 --> 01:43:30,026
a bit more guideline for you.

1251
01:43:30,056 --> 01:43:34,092
So it's telling you, do not call BFloat16 on any of your tensors.

1252
01:43:35,032 --> 01:43:41,022
Just use autocast, and only surround the forward pass of the model and the loss calculation.

1253
01:43:41,064 --> 01:43:43,090
And that's the only two things that you should be surrounding.

1254
01:43:44,036 --> 01:43:46,046
Leave the backward and the optimizer step alone.

1255
01:43:47,030 --> 01:43:49,052
So that's the guidance that comes from the PyTorch team.

1256
01:43:50,004 --> 01:43:51,034
So we're going to follow that guidance.

1257
01:43:51,062 --> 01:43:57,057
And for us, because the loss calculation is inside of the model forward pass for us, we are going to be doing this.

1258
01:43:59,006 --> 01:44:04,050
And then we don't want to be using Torch.float16, because if we do that, we need to start using gradient scalars as well.

1259
01:44:04,092 --> 01:44:06,052
So we are going to be using BFloat16.

1260
01:44:07,028 --> 01:44:09,018
This is only possible to do in Ampere.

1261
01:44:09,052 --> 01:44:14,052
But this means that the changes are extremely minimal, like basically just this one line of code.

1262
01:44:15,086 --> 01:44:22,004
Let me first break in to here before we actually run this.

1263
01:44:22,072 --> 01:44:30,062
So right after logits, I'd like to show you that different from the TF32 that we saw, this is actually going to impact our tensors.

1264
01:44:32,022 --> 01:44:40,040
So this logits tensor, if we now look at this, and we look at the D type, we suddenly see that this is now BFloat16.

1265
01:44:40,088 --> 01:44:43,030
It's not float32 anymore.

1266
01:44:43,032 --> 01:44:45,026
So our activations have been changed.

1267
01:44:45,044 --> 01:44:47,072
The activations tensor is now BFloat16.

1268
01:44:48,012 --> 01:44:49,050
But not everything has changed.

1269
01:44:49,074 --> 01:45:00,050
So model.transformer.wte, this is the weight token embedding table, it has a dot weight inside it.

1270
01:45:01,004 --> 01:45:05,057
And the dtype of this weight, this parameter, is still Torchfloat32.

1271
01:45:06,036 --> 01:45:08,060
So our parameters seem to still be on float32.

1272
01:45:09,000 --> 01:45:11,072
But our activations, the logits are now on BFloat16.

1273
01:45:12,054 --> 01:45:15,024
So clearly, this is why we get the mixed precision.

1274
01:45:15,076 --> 01:45:21,000
Some things PyTorch is keeping in float32, some things PyTorch is converting to lower precision.

1275
01:45:22,056 --> 01:45:26,078
And what gets converted at what point is not super clear.

1276
01:45:27,050 --> 01:45:29,012
I remember scrolling down.

1277
01:45:30,066 --> 01:45:31,096
Is it here?

1278
01:45:35,060 --> 01:45:36,057
Okay, I can't find it.

1279
01:45:37,038 --> 01:45:39,048
I thought it was here.

1280
01:45:40,012 --> 01:45:40,082
Okay, there we go.

1281
01:45:41,064 --> 01:45:48,080
So there are a few docs on when you're using this autocast, what gets converted to BFloat16 and when.

1282
01:45:49,022 --> 01:45:53,048
So for example, only these matrix multiply like operations get converted to BFloat16.

1283
01:45:53,096 --> 01:45:56,046
But a lot of operations remain in float32.

1284
01:45:56,086 --> 01:46:02,010
So in particular, a lot of normalizations like layer norms and things like that, not all of those layers might be converted.

1285
01:46:03,018 --> 01:46:07,012
So only some layers selectively would be running BFloat16.

1286
01:46:07,040 --> 01:46:17,028
But things like softmax, layer norms, log softmax, so loss function calculations, a lot of those things might remain in float32 because

1287
01:46:17,028 --> 01:46:19,048
they are more susceptible to precision changes.

1288
01:46:20,006 --> 01:46:25,040
Matrix multiplies are fairly robust to precision changes.

1289
01:46:26,032 --> 01:46:31,006
So some parts of the network are impacted more or less by the precision change.

1290
01:46:32,086 --> 01:46:36,060
So basically only some parts of the model are running in reduced precision.

1291
01:46:37,008 --> 01:46:42,028
Let's take it for a spin and let's actually see what kind of improvement we achieve here.

1292
01:46:49,074 --> 01:46:53,036
Okay, so we used to be 333 milliseconds, we're now 300.

1293
01:46:53,036 --> 01:46:56,080
And we used to be somewhere around 50,000 tokens per second, we're now 55.

1294
01:46:57,034 --> 01:47:01,038
So we're definitely running faster, but maybe not a lot faster.

1295
01:47:01,092 --> 01:47:06,026
And that's because there are still many, many bottlenecks in our GPT-2, we're just getting started.

1296
01:47:06,084 --> 01:47:11,034
But we have dropped down the precision as far as we can with my current GPU, which is A100.

1297
01:47:12,010 --> 01:47:13,082
We're using PyTorch AutoCast.

1298
01:47:14,036 --> 01:47:18,094
Unfortunately, I don't actually exactly know what PyTorch AutoCast does.

1299
01:47:19,020 --> 01:47:22,086
I don't actually know exactly what's in BFloat16, what's in float32.

1300
01:47:23,004 --> 01:47:25,024
We could go in and we could start to scrutinize it.

1301
01:47:26,006 --> 01:47:28,098
But these are the kinds of rules that PyTorch has internally.

1302
01:47:29,012 --> 01:47:31,057
And unfortunately, they don't document it very well.

1303
01:47:32,014 --> 01:47:35,084
So we're not going to go into that in too much detail.

1304
01:47:36,020 --> 01:47:38,008
But for now, we are training in BFloat16.

1305
01:47:38,044 --> 01:47:40,004
We do not need a gradient scaler.

1306
01:47:40,044 --> 01:47:47,094
And the reason things are running faster is because we are able to run Tensorcores in BFloat16 now.

1307
01:47:47,098 --> 01:47:50,016
That means we are in this row.

1308
01:47:50,060 --> 01:47:54,004
But we are also paying in precision for this.

1309
01:47:54,026 --> 01:47:59,076
So we expect slightly less accurate results with respect to the original FP32.

1310
01:48:00,056 --> 01:48:06,048
But empirically, in many cases, this is a worth it trade-off because it allows you to run faster.

1311
01:48:06,084 --> 01:48:11,062
And you could, for example, train longer and make up for that precision decrease.

1312
01:48:12,044 --> 01:48:15,016
So that's BFloat16 for now.

1313
01:48:15,054 --> 01:48:15,080
Okay.

1314
01:48:15,088 --> 01:48:20,040
So as we can see, we are currently at about 300 milliseconds per iteration.

1315
01:48:20,066 --> 01:48:24,006
And we're now going to reach for some really heavy weapons in the PyTorch arsenal.

1316
01:48:24,054 --> 01:48:26,084
And in particular, we're going to introduce Torch.compile.

1317
01:48:27,068 --> 01:48:31,064
Torch.compile is an incredible infrastructure from the PyTorch team.

1318
01:48:32,000 --> 01:48:34,024
And it's basically a compiler for neural networks.

1319
01:48:34,084 --> 01:48:37,062
It's almost like GCC for C and C++ code.

1320
01:48:38,002 --> 01:48:40,030
This is just GCC of neural nets.

1321
01:48:41,014 --> 01:48:45,032
So came out a while ago and extremely simple to use.

1322
01:48:46,042 --> 01:48:48,070
The way to use Torch.compile is to do this.

1323
01:48:49,044 --> 01:48:53,036
It's a single line of code to compile your model and return it.

1324
01:48:53,096 --> 01:48:56,040
Now, this line of code will cost you compilation time.

1325
01:48:56,092 --> 01:48:59,010
But as you might guess, it's going to make the code a lot faster.

1326
01:48:59,074 --> 01:49:02,084
So let's actually run that because this will take some time to run.

1327
01:49:03,026 --> 01:49:04,096
But currently, remember, we're at 300 milliseconds.

1328
01:49:05,056 --> 01:49:06,030
And we'll see what happens.

1329
01:49:07,000 --> 01:49:12,002
Now, while this is running, I'd like to explain a little bit of what Torch.compile does under the hood.

1330
01:49:12,078 --> 01:49:15,068
So feel free to read this page of PyTorch.

1331
01:49:16,014 --> 01:49:20,088
But basically, there's no real good reason for you to not use Torch.compile in your PyTorch.

1332
01:49:21,006 --> 01:49:25,066
I kind of feel like you should be using it almost by default if you're not unless you're debugging and

1333
01:49:25,066 --> 01:49:27,008
you want your code to run really fast.

1334
01:49:28,016 --> 01:49:32,094
And there's one line here in Torch.compile that I found that actually kind of like gets to why this is faster.

1335
01:49:33,094 --> 01:49:38,002
Speed up mainly comes from reducing Python overhead and GPU read writes.

1336
01:49:38,050 --> 01:49:39,084
So let me unpack that a little bit.

1337
01:49:40,064 --> 01:49:41,060
Okay.

1338
01:49:41,068 --> 01:49:42,012
Here we are.

1339
01:49:42,057 --> 01:49:42,074
Okay.

1340
01:49:42,078 --> 01:49:44,018
So we went from 300 milliseconds.

1341
01:49:44,066 --> 01:49:46,048
We're now running at 129 milliseconds.

1342
01:49:47,084 --> 01:49:54,020
So this is 300 divided by 129, about 2.3x improvement from a single line of code in PyTorch.

1343
01:49:55,012 --> 01:49:56,034
So quite incredible.

1344
01:49:56,074 --> 01:49:57,046
So what is happening?

1345
01:49:57,052 --> 01:49:58,034
What's happening under the hood?

1346
01:49:59,004 --> 01:50:08,018
Well, when you pass the model to Torch.compile, what we have here in this NN module, this is really just the algorithmic description of what

1347
01:50:08,018 --> 01:50:09,086
we'd like to happen in our network.

1348
01:50:10,024 --> 01:50:16,048
And Torch.compile will analyze the entire thing and it will look at what operations you'd like to use.

1349
01:50:17,002 --> 01:50:23,012
And with the benefit of knowing exactly what's going to happen, it doesn't have to run in what's called the eager mode.

1350
01:50:23,046 --> 01:50:26,010
It doesn't have to just kind of like go layer by layer.

1351
01:50:26,052 --> 01:50:34,010
Like the Python interpreter normally would start at the forward and the Python interpreter will go, okay,

1352
01:50:34,018 --> 01:50:37,006
let's do this operation and then let's do that operation.

1353
01:50:37,060 --> 01:50:40,076
And it kind of materializes all the operations as it goes through.

1354
01:50:41,044 --> 01:50:44,098
So these calculations are dispatched and run in this order.

1355
01:50:45,044 --> 01:50:50,002
And the Python interpreter and this code doesn't know what kind of operations are going to happen later.

1356
01:50:50,064 --> 01:50:57,038
But Torch.compile sees your entire code at the same time and it's able to know what operations you intend to run and

1357
01:50:57,038 --> 01:50:59,036
it will kind of optimize that process.

1358
01:50:59,084 --> 01:51:04,080
The first thing it will do is it will take out the Python interpreter from the forward pass entirely and

1359
01:51:04,080 --> 01:51:10,002
it will kind of compile this entire neural net as a single object with no Python interpreter involved.

1360
01:51:10,048 --> 01:51:14,076
So it knows exactly what's going to run and it will just run that and it's all going to be running in efficient code.

1361
01:51:16,074 --> 01:51:22,044
The second thing that happens is this read write that they mentioned very briefly.

1362
01:51:22,090 --> 01:51:26,032
So a good example of that I think is the Gelu nonlinearity that we've been looking at.

1363
01:51:26,092 --> 01:51:28,026
So here we use the nn.gelu.

1364
01:51:28,084 --> 01:51:36,078
Now this here is me basically just breaking up the nn.gelu which you remember has this formula.

1365
01:51:37,032 --> 01:51:40,088
So this here is the equivalent implementation to what's happening inside Gelu.

1366
01:51:40,098 --> 01:51:42,016
Algorithmically it's identical.

1367
01:51:43,020 --> 01:51:50,076
Now by default if we just were using this instead of nn.gelu here.  what would happen without Torch.compile?

1368
01:51:51,020 --> 01:51:55,050
Well the Python interpreter would make its way here and then it would be okay well there's an input.

1369
01:51:56,028 --> 01:52:04,042
Well, let me first let me raise this input to the third power, and it's going to dispatch a kernel that takes your input and raises it to the third power.

1370
01:52:04,042 --> 01:52:13,081
That kernel will run, and when this kernel runs, what ends up happening is this input is stored in the memory of the GPU.

1371
01:52:13,082 --> 01:52:17,070
So here's a helpful example of the layout of what's happening right.

1372
01:52:18,042 --> 01:52:19,012
You have your CPU.

1373
01:52:19,054 --> 01:52:20,072
This is in every single computer.

1374
01:52:21,014 --> 01:52:29,045
There are a few cores in there, and you have your RAM, your memory. The CPU can talk to the memory, and this is all well-known. 

1375
01:52:29,045 --> 01:52:33,012
But now we've added the GPU, and the GPU is a slightly different architecture, of course.

1376
01:52:33,036 --> 01:52:38,006
They can communicate, and it's different in that it's got a lot more cores than a CPU. 

1377
01:52:38,074 --> 01:52:43,057
All of those cores are individually a lot simpler too, but it also has memory, right?

1378
01:52:43,086 --> 01:52:45,084
This high bandwidth memory.

1379
01:52:46,038 --> 01:52:48,054
I'm sorry if I'm botching it.

1380
01:52:48,068 --> 01:52:49,000
HBM.

1381
01:52:49,024 --> 01:52:50,080
I don't even know what that stands for.

1382
01:52:51,018 --> 01:52:53,002
I'm just realizing now. 

1383
01:52:53,003 --> 01:52:59,066
but this is the memory, and it's very equivalent to RAM basically in the computer. 

1384
01:52:59,066 --> 01:53:15,077
What's happening is that input is living in the memory, and when you do input cubed, this has to travel to the GPU to the cores and to all the caches and registers on the actual chip of this GPU.

1385
01:53:15,085 --> 01:53:26,008
It has to calculate all the elements to the third and then it saves the result back to the memory. It's this travel time that actually causes a lot of issues.

1386
01:53:26,036 --> 01:53:28,096
So here, remember this memory bandwidth.

1387
01:53:29,046 --> 01:53:37,024
We can communicate about two terabytes per second, which is a lot, but also, we have to traverse this link, and it's very slow.

1388
01:53:37,024 --> 01:53:42,096
So here on the GPU, we're on-chip, and everything is super fast within the chip, but going to the memory is extremely expensive.

1389
01:53:43,014 --> 01:53:50,070
It takes an extremely long amount of time, and so we load the input, do the calculations, and load back the output.

1390
01:53:50,070 --> 01:53:56,077
this round trip takes a lot of time, and now right after we do that, we multiply by this constant. 

1391
01:53:56,088 --> 01:54:04,080
So what happens then is we dispatch another kernel, and then the result travels back, all the elements get multiplied by a constant, and

1392
01:54:04,080 --> 01:54:11,020
then the results travel back to the memory, and then we take the result and we add back input. 

1393
01:54:11,020 --> 01:54:17,060
And so this entire thing again travels to the GPU as the inputs, and gets written back.



1394
01:54:18,018 --> 01:54:23,064
So we're making all these round trips from the memory to actually where the computation happens, because

1395
01:54:23,064 --> 01:54:28,012
all the tensor cores and the ALUs and everything like that is all stored on the chip in the GPU.

1396
01:54:28,078 --> 01:54:36,002
So we're doing a ton of round trips, and PyTorch without using Torch Compile doesn't know to optimize this, because

1397
01:54:36,002 --> 01:54:38,004
it doesn't know what kind of operations you're running later.

1398
01:54:38,042 --> 01:54:44,023
You're just telling it raise the power to the third, then do this, then do that, and it will just do that in that sequence.

1399
01:54:44,066 --> 01:54:51,055
But Torch Compile sees your entire code. It will come here and it will realize, wait, all of these are element-wise operations, and actually 

1400
01:54:51,067 --> 01:54:56,015
what I'm going to do is I'm going to do a single trip of input to the GPU.

1401
01:54:56,015 --> 01:55:07,012
Then, for every single element, I'm going to do all of these operations while that memory is on the GPU, or chunks of it rather, and then I'm going to write back a single time.

1402
01:55:07,028 --> 01:55:11,054
So we're not going to have these round trips, and that's one example of what's called kernel fusion, and

1403
01:55:11,054 --> 01:55:13,084
is a major way in which everything is sped up.

1404
01:55:14,056 --> 01:55:17,085
So basically, if you have your benefit of headstart, and you know exactly what you're going to compute, 

1405
01:55:17,096 --> 01:55:23,027
you can optimize your round trips to the memory, and you're not going to pay the memory bandwidth cost, 

1406
01:55:23,027 --> 01:55:30,066
That's fundamentally what makes some of these operations a lot faster and what they mean by read-writes here.

1407
01:55:30,066 --> 01:55:40,046
So let me erase this because we are not using it, and yeah, we should be using Torch Compile, and our code is now significantly faster, and

1408
01:55:40,046 --> 01:55:45,078
we're doing about 125,000 tokens per second, but we still have a long way to go.

1409
01:55:46,010 --> 01:55:52,098
Before we move on, I wanted to supplement the discussion a little bit with a few more figures because this is a complicated topic.

1410
01:55:52,098 --> 01:56:01,045
but  It's worth understanding on a high level what's happening here. I could probably spend an entire video of like two hours on this, but just a preview of that basically.

1411
01:56:02,006 --> 01:56:09,020
So this chip here that is the GPU, this chip is where all the calculations happen mostly.

1412
01:56:09,055 --> 01:56:18,080
But this chip also does have some memory in it, but most of the memory by far is here in the high bandwidth memory HBM and is connected. 

1413
01:56:19,000 --> 01:56:23,014
 They're connected, but these are two separate chips basically.

1414
01:56:23,014 --> 01:56:32,090
Now, here, this is a zoom in of kind of this cartoon diagram of a GPU. What we're seeing here is, number one, you see this HBM.

1415
01:56:33,009 --> 01:56:39,030
I realize it's probably very small for you, but on the sides here, it says HBM. So that's the links to the HBM.

1416
01:56:39,030 --> 01:56:41,064
Now the HBM is again off-chip.

1417
01:56:42,020 --> 01:56:46,086
On the chip, there are a large number of these streaming multi-processors. 

1418
01:56:47,014 --> 01:56:54,018
Every one of these is an SM; there's 120 of them in total, and this is where a lot of the calculations happen, and

1419
01:56:54,018 --> 01:56:56,076
this is a zoom-in of a single individual SM.

1420
01:56:57,020 --> 01:57:00,015
It has these four quadrants and see, for example, tensor core.

1421
01:57:00,018 --> 01:57:09,060
This is where a lot of the matrix multiply stuff happens, but there's all these other units to do all different kinds of calculations for FB64, FB32, and for integers and so on.

1422
01:57:10,020 --> 01:57:19,088
Now, so we have all this logic here to do the calculations, but in addition to that, on the chip, there is memory sprinkled throughout the chip.

1423
01:57:19,088 --> 01:57:27,033
 L2 cache is some amount of memory that lives on the chip, and then on the SMs themselves, there's L1 cache.

1424
01:57:27,033 --> 01:57:32,050
I realize it's probably very small for you, but this blue bar is L1, and there are also registers. 

1425
01:57:32,059 --> 01:57:41,000
So there is memory stored here, but the way this memory is stored is very different from the way memory stored in HBM.

1426
01:57:41,000 --> 01:57:51,064
This is a very different implementation using just in terms of like what the silicon looks like. It's a very different implementation, 

1427
01:57:51,064 --> 01:57:58,017
 So here, you would be using transistors and capacitors, and here it's a very different implementation with SRAM and what that looks like. 

1428
01:57:58,055 --> 01:58:08,042
 But long story short, there is memory inside the chip, but it's not a lot of memory. That's the critical point.

1429
01:58:08,042 --> 01:58:14,096
This is some... This is an example diagram of a slightly different GPU. Just like here where it shows that, 

1430
01:58:14,096 --> 01:58:19,045
for example, typical numbers for CPU DRAM memory, which is this thing here.

1431
01:58:19,057 --> 01:58:23,025
You might have one terabyte of disk, right? 

1432
01:58:23,025 --> 01:58:27,010
But it would be extremely expensive to access, especially for a GPU. You have to go through the CPU here.

1433
01:58:27,098 --> 01:58:37,026
Now, next we have the HBM, so we have tens of gigabytes of HBM memory on a typical GPU here, but it's, as I mentioned, very expensive to access.

1434
01:58:37,026 --> 01:58:47,056
And then on the chip itself, everything is extremely fast within the chip, but we only have a couple of tens of megabytes of memory collectively throughout the chip. 

1435
01:58:47,056 --> 01:58:56,083
So there's just not enough space because the memory is very expensive on the chip, and so there's not a lot of it. But it is lightning fast to access in relative terms.

1436
01:58:56,090 --> 01:58:58,095
So basically, whenever we have these kernels, 

1437
01:58:59,010 --> 01:59:07,042
the more accurate picture of what's happening here is that we take these inputs which live by default on the global memory.

1438
01:59:07,047 --> 01:59:15,060
 And now we need to perform some calculation, so we start streaming the data from the global memory to the chip.

1439
01:59:15,067 --> 01:59:22,038
We perform the calculations on the chip and then stream it back and store it back to the global memory, right?

1440
01:59:22,038 --> 01:59:30,042
And so if we are...  if we don't have Torch Compile, we are streaming the data through the chip during the calculations and saving to the memory, and we're doing those round trips many, many times.

1441
01:59:30,047 --> 01:59:35,045
But if it's Torch Compiled, then we start streaming the memory as before, 

1442
01:59:35,045 --> 01:59:44,008
but then while we're on the chip where we have a chunk of the data that we're trying to process, so that chunk now lives on the chip.

1443
01:59:44,018 --> 01:59:47,005
While it's on the chip, it's extremely fast to operate on. 

1444
01:59:47,005 --> 01:59:53,080
if we have kernel fusion, we can do all the operations right there in an element-wise fashion, and those are very cheap, 

1445
01:59:53,080 --> 01:59:58,060
And then we do a single round trip back to the global memory. 

1446
01:59:58,060 --> 02:00:07,012
So operator fusion basically allows you to keep your chunk of data on the chip and do lots of calculations on it before you write it back, and that gives huge savings. 

1447
02:00:07,026 --> 02:00:13,042
 And that's why Torch Compile ends up being a lot faster, or that's one of the major reasons.

1448
02:00:13,045 --> 02:00:19,023
Again, just a very brief intro to the memory hierarchy and roughly what Torch Compile does for you.

1449
02:00:19,023 --> 02:00:27,095
Now Torch Compile is amazing, but there are operations that Torch Compile will not find. And an amazing example of that is FlashAttention, to which we turn next. 

1450
02:00:28,015 --> 02:00:34,050
So, FlashAttention comes from this paper from Stanford in 2022, 

1451
02:00:34,061 --> 02:00:40,077
and it's this incredible algorithm for performing attention  and running it a lot faster,

1452
02:00:40,077 --> 02:00:47,066
 so FlashAttention will come here, and we will take out these four lines, and

1453
02:00:47,066 --> 02:00:53,035
FlashAttention implements these four lines really, really quickly, and how does it do that?

1454
02:00:53,035 --> 02:01:04,038
FlashAttention is a kernel fusion operation. So you see here, we have in this diagram they're showing PyTorch, and you have these four operations. 

1455
02:01:04,057 --> 02:01:07,040
They're including Dropout, but we are not using Dropout here.

1456
02:01:07,047 --> 02:01:16,008
So, we just have these four lines of code here, and instead of those, we are fusing them into a single fused kernel of FlashAttention. 

1457
02:01:16,008 --> 02:01:22,057
So, it's a kernel fusion algorithm, but it's a kernel fusion that Torch Compile cannot find. 

1458
02:01:22,057 --> 02:01:31,022
And the reason that it cannot find it is that it requires an algorithmic rewrite of how attention is actually implemented here in this case.

1459
02:01:31,022 --> 02:01:41,030
What's remarkable about it is that FlashAttention actually, if you just count the number of flops, FlashAttention does more flops than this attention here.

1460
02:01:41,030 --> 02:01:54,008
But FlashAttention is actually significantly faster. In fact, they cite 7.6 times faster potentially, and that's because it is very mindful of the memory hierarchy, as I described it just now. 

1461
02:01:54,052 --> 02:01:59,093
and so it's very mindful about what's in high bandwidth memory, what's in the shared memory, 

1462
02:01:59,093 --> 02:02:07,070
and it is very careful with how it orchestrates the computation such that we have fewer reads and writes to the high bandwidth memory.

1463
02:02:07,070 --> 02:02:13,080
So, even though we're doing more flops, the expensive part is the load and store into HBM, and that's what they avoid.

1464
02:02:13,080 --> 02:02:20,090
In particular, they do not ever materialize this n-by-n attention matrix, this ATT here. 

1465
02:02:20,090 --> 02:02:31,038
FlashAttention is designed such that this matrix never gets materialized at any point and it never gets read or written to the HBM, and this is a very large matrix, right?

1466
02:02:31,038 --> 02:02:43,052
So because this is where all the queries and keys interact, and we're sort of getting for each head, for each batch element, we're getting a t by t matrix of attention,

1467
02:02:43,052 --> 02:02:51,096
 which is a million numbers even for a single head at a single batch index. So basically, this is a ton of memory.

1468
02:02:51,096 --> 02:02:56,073
This is never materialized, and the way that this is  achieved is  that

1469
02:02:56,073 --> 02:03:03,038
basically the fundamental algorithmic rewrite here relies on this online softmax trick which was proposed previously. 

1470
02:03:03,038 --> 02:03:06,025
I'll show you the paper in a bit.

1471
02:03:06,025 --> 02:03:19,018
The online softmax trick, coming from a previous paper, shows how you can incrementally evaluate a softmax without having to sort of realize all of the inputs to the softmax of the normalization. 

1472
02:03:19,018 --> 02:03:27,000
You do that by having these intermediate variables M and L, and there's an update to them that allows you to evaluate the softmax in an online manner. 

1473
02:03:27,095 --> 02:03:38,084
Now, FlashAttention actually—recently FlashAttention 2 came out as well, so I have that paper up here as well. It has additional gains to how it calculates FlashAttention.

1474
02:03:38,084 --> 02:03:44,057
The original paper that this is based on basically is this online normalizing calculation for softmax,

1475
02:03:44,057 --> 02:03:53,002
Remarkably, it came out of NVIDIA, and it came out of it really early 2018, so this is four years before FlashAttention. 

1476
02:03:53,002 --> 02:03:59,002
This paper says that we propose a way to compute the classical softmax with fewer memory accesses, 

1477
02:03:59,002 --> 02:04:04,088
and hypothesize that this reduction in memory accesses should improve softmax performance on actual hardware.

1478
02:04:04,088 --> 02:04:11,092
 They are extremely correct in this hypothesis, but it's really fascinating to me that they're from NVIDIA 

1479
02:04:11,092 --> 02:04:19,088
and that they had this realization, but they didn't actually take it to the actual FlashAttention that had to come four years later from Stanford. 

1480
02:04:19,088 --> 02:04:29,038
So, I don't fully understand the historical how this happened historically, but they do basically propose this online update to the softmax right here, 

1481
02:04:29,038 --> 02:04:34,075
this is fundamentally what they reuse here to calculate the softmax in a streaming manner.

1482
02:04:34,075 --> 02:04:42,054
And then they realize that they can actually fuse all the other operations with the online softmax calculation into a single fused kernel, FlashAttention.

1483
02:04:42,054 --> 02:04:44,010
And that's what we are about to use.

1484
02:04:44,032 --> 02:04:49,032
A great example, I think, of being aware of memory hierarchy, 

1485
02:04:49,057 --> 02:04:55,023
the fact that flops don't matter, the entire memory access pattern matters, and that Torch Compile is amazing, 

1486
02:04:55,023 --> 02:04:59,095
but there are many optimizations that are still available to us that potentially Torch Compile cannot find. 

1487
02:04:59,095 --> 02:05:05,083
Maybe one day it could, but right now it seems like a lot to ask. So, here's what we're going to do.

1488
02:05:05,088 --> 02:05:17,094
We're going to use FlashAttention, and the way to do that basically in PyTorch is we are going to comment out these four lines and we're going to replace them with a single line. 

1489
02:05:17,094 --> 02:05:31,005
And here we are calling this compound operation in PyTorch called scaled dot product attention, and PyTorch will call FlashAttention when you use it in this way.

1490
02:05:31,005 --> 02:05:38,090
 I'm not actually 100% sure why Torch Compile doesn't realize that these four lines should just call FlashAttention in this exact way we have to do it again for it, 

1491
02:05:38,095 --> 02:05:46,020
which in my opinion is a little bit odd, but here we are. 

1492
02:05:46,020 --> 02:05:53,020
So, you have to use this compound op, and let's wait for a few moments before Torch Compile gets around to it,

1493
02:05:53,020 --> 02:06:01,074
 and then let's remember that we achieved a loss of 6.05661. I have it here, that's the loss we were expecting to see

1494
02:06:01,074 --> 02:06:10,000
We took 130 milliseconds before this change, so we're expecting to see the exact same result by iteration 49,

1495
02:06:10,000 --> 02:06:16,062
but we expect to see faster runtime because FlashAttention is just an algorithmic rewrite and it's a faster kernel. 

1496
02:06:16,062 --> 02:06:21,046
But it doesn't actually change any of the computation, and we should have the exact same optimization.

1497
02:06:21,046 --> 02:06:29,002
Okay, so we're a lot faster, we're at about 95 milliseconds, and we achieved 6.058. 

1498
02:06:29,002 --> 02:06:34,023
Okay, so they're basically identical up to a floating point fudge factor.

1499
02:06:34,038 --> 02:06:39,080
 So it's the identical computation, but it's significantly faster, going from 130 to roughly 95-96. 

1500
02:06:39,085 --> 02:06:55,000
So this is 96 divided by 130-ish, so this is maybe a 27-ish percent improvement. Really interesting, and that is FlashAttention.

1501
02:06:55,000 --> 02:07:05,088
We are now getting to one of my favorite optimizations, and it is simultaneously the most brilliant optimization, and it's always a little bit surprising to me.

1502
02:07:05,088 --> 02:07:12,057
anyway, so basically, I mentioned a few minutes ago that there are some numbers that are nice and some numbers that are ugly.

1503
02:07:12,057 --> 02:07:19,062
64 is a beautiful nice number, 128 is even nicer, 256 is beautiful. 

1504
02:07:19,065 --> 02:07:26,083
What makes these numbers beautiful is that there are many powers of two inside them. You can divide by two many times. 

1505
02:07:26,083 --> 02:07:32,070
Examples of ugly numbers are like 13, 17, and prime numbers, numbers that are not even, and so on.

1506
02:07:32,080 --> 02:07:42,036
Pretty much, you always want to use nice numbers in all of your code that deals with neural networks or CUDA because everything in CUDA works in sort of like powers of two. 

1507
02:07:42,036 --> 02:07:49,095
And lots of kernels are written in terms of powers of two, and there are lots of blocks of sizes 16 and 64 and so on. 

1508
02:07:49,095 --> 02:07:59,095
So everything is written in those terms, and you always have special case handling for all kinds of logic that when your inputs are not made of nice numbers.

1509
02:07:59,095 --> 02:08:02,017
So, let's see what that looks like. 

1510
02:08:02,017 --> 02:08:09,027
Basically, scan your code and look for ugly numbers is roughly the heuristic.  So, three times is kind of ugly.

1511
02:08:09,040 --> 02:08:15,040
I'm not 100% sure; maybe this can be improved, but this is ugly and not ideal. 

1512
02:08:15,040 --> 02:08:29,042
Four times is nice, so that's nice. 1024 is very nice; that's a power of two. 12 is a little bit suspicious, not too many powers of two. 

1513
02:08:29,042 --> 02:08:37,050
768 is great. 50,257 is a really, really ugly number. First of all, it's odd, so...

1514
02:08:37,050 --> 02:08:44,056
And there's not too many powers of two in there, so this is a very ugly number, and it's highly suspicious.

1515
02:08:44,056 --> 02:08:53,008
Then, when we scroll down, all these numbers are nice, and then here we have mostly nice numbers except for 25. 

1516
02:08:53,008 --> 02:08:59,050
 So, in this configuration of GPT-2 XL, the number of heads is 25. That's a really ugly number; that's an odd number.

1517
02:08:59,050 --> 02:09:08,042
And actually, this did cause a lot of headaches for us recently when we were trying to optimize some kernels to run this fast and required a bunch of special case handling

1518
02:09:08,042 --> 02:09:13,098
So, basically, these numbers are... we have some ugly numbers, and some of them are easier to fix than others. 

1519
02:09:14,008 --> 02:09:20,033
And in particular, the vocab size being 50,257, that's a very ugly number, very suspicious. We're going to fix it.

1520
02:09:20,033 --> 02:09:31,000
When you fix these things, one of the easy ways to do that is you basically increase the number until it's the nearest power of two that you like. 

1521
02:09:31,000 --> 02:09:43,035
So, here's a much nicer number; it's 50,304. And why is that? Because 50,304 can be divided by 8, or by 16, or by 32, or 64. 

1522
02:09:43,048 --> 02:09:49,055
It can even be divided by 128, I think, yeah. So, it's a very nice number.

1523
02:09:49,080 --> 02:09:52,052
So what we're going to do here is... this is the GPT config and

1524
02:09:52,052 --> 02:10:04,016
you see that we initialize vocab size to 50,257. Let's override just that element to be 50,304, okay? 

1525
02:10:04,016 --> 02:10:12,040
So everything else stays the same. We're just increasing our vocabulary size. So we're adding; it's almost like we're adding fake tokens. 

1526
02:10:12,040 --> 02:10:14,057
so that vocab size has powers of two inside it .

1527
02:10:14,069 --> 02:10:20,098
Now, actually what I'm doing here, by the way, is I'm increasing the amount of computation that our network will be doing.

1528
02:10:20,098 --> 02:10:26,015
 If you just count the flops, like do the math of how many flops we're doing, we're going to be doing more flops, 

1529
02:10:26,030 --> 02:10:31,023
and we still have to think through whether this doesn't break anything. 

1530
02:10:31,026 --> 02:10:34,065
But if I just run this, let's see what we get. 

1531
02:10:34,065 --> 02:10:43,038
Currently, this ran in maybe 96.5 milliseconds per step. I'm just kind of like eyeballing it, and let's see what kind of a result we're going to get.

1532
02:10:43,038 --> 02:10:52,017
Let's see what kind of a result we're going to get . While this is compiling. Let's think through whether our code actually works, okay? 

1533
02:10:52,028 --> 02:10:58,027
Okay, when we increase the vocab size like this, let's look at where vocab size is actually used. 

1534
02:10:58,027 --> 02:11:02,035
So, we swing up to the init and we see that it's used inside the embedding table, of course.

1535
02:11:02,042 --> 02:11:08,004
Of course, All the way at the bottom of the transformer, and it's used at the classifier layer all the way at the top of the transformer. 

1536
02:11:08,004 --> 02:11:15,052
So in two places, let's take a look. We're running at 93, so 93 milliseconds instead of 96.5. 

1537
02:11:15,054 --> 02:11:24,048
So we are seeing a roughly, yeah, four percent improvement here by doing more calculations.

1538
02:11:24,048 --> 02:11:29,010
And the reason for this is we fixed, we made an ugly number into a nice number. 

1539
02:11:29,021 --> 02:11:36,096
Let's, I'm going to come into the explanation for that a little bit again, but for now let's just convince ourselves that we're not breaking anything when we do this.

1540
02:11:36,096 --> 02:11:41,090
First of all, we've made the WTE, the embedding table for the tokens, we've made it larger.

1541
02:11:41,097 --> 02:11:52,065
It's almost like we introduced more tokens at the bottom, and these tokens are never used because the GPT tokenizer only has tokens up to 50,256. 

1542
02:11:52,075 --> 02:11:56,055
And so we'll never index into the rows that we've added. 

1543
02:11:56,055 --> 02:12:03,038
 So we're wasting a little bit of space here by creating memory that's never going to be accessed, never going to be used, etc. 

1544
02:12:03,038 --> 02:12:10,098
 Now, that's not fully correct because this WTE weight ends up being shared and ends up being used in the classifier here at the end.

1545
02:12:11,002 --> 02:12:14,070
So what is that doing to the classifier right here?

1546
02:12:14,070 --> 02:12:18,073
Well, what that's doing is we're predicting additional dimensions of the classifier now, 

1547
02:12:18,083 --> 02:12:24,026
and we're predicting probabilities for tokens that will, of course, never be present in the training set. 

1548
02:12:24,026 --> 02:12:30,088
And so therefore, the network has to learn that these probabilities have to be driven to zero. 

1549
02:12:30,088 --> 02:12:35,098
And so the logits that the network produces have to drive those dimensions of the output to negative infinity.

1550
02:12:36,003 --> 02:12:43,060
But that's no different from all the other tokens that are already in our dataset, or rather that are not in our dataset. 

1551
02:12:44,027 --> 02:12:49,035
So Shakespeare only probably uses, let's say, a thousand tokens out of 50,257 tokens. 

1552
02:12:49,039 --> 02:12:56,005
So most of the tokens are already being driven to zero probability by the optimization we've just introduced a few more tokens now that, 

1553
02:12:56,005 --> 02:13:04,008
in a similar manner, will never be used and have to be driven to zero in probability.Functionally, though, nothing breaks.

1554
02:13:04,008 --> 02:13:11,005
We're using a bit more extra memory, but otherwise, this is a harmless operation as far as I can tell. 

1555
02:13:11,041 --> 02:13:14,060
But and we're adding calculation, but it's running faster. 

1556
02:13:14,065 --> 02:13:23,073
And It's running faster because, as I mentioned, in CUDA, so many kernels use block tiles, and these block tiles are usually nice numbers.

1557
02:13:23,081 --> 02:13:24,085
powers of two.

1558
02:13:24,085 --> 02:13:28,050
So calculations are done in like chunks of 64 or chunks of 32. 

1559
02:13:28,062 --> 02:13:41,070
And when your desired calculation doesn't neatly fit into those block tiles, there are all kinds of boundary kernels that can kick in to like do the last part. 

1560
02:13:42,023 --> 02:13:45,092
So basically, in a lot of kernels, they will truncate up your input.

1561
02:13:45,092 --> 02:13:56,042
and they will do the nice part first and then they have a whole second phase where they come back to anything that like remains and then they process the remaining part. 

1562
02:13:56,060 --> 02:14:03,092
And the kernels for that could be very inefficient. So you're basically spinning up all this extra compute, and it's extremely inefficient.

1563
02:14:04,020 --> 02:14:11,052
So you might as well pad your inputs and make it fit nicely. And usually, that empirically ends up actually running faster. 

1564
02:14:11,057 --> 02:14:17,085
So this is another example of a four percent improvement that we've added. 

1565
02:14:17,090 --> 02:14:21,002
And this is something that also Torch Compile did not find for us. 

1566
02:14:21,012 --> 02:14:27,057
You would hope that Torch Compile at some point could figure an optimization like this out. But for now, this is it.

1567
02:14:27,080 --> 02:14:32,023
 And I also have to point out that we're using PyTorch nightly. So that's why we're only seeing four percent.

1568
02:14:32,033 --> 02:14:45,050
 If you're using PyTorch 2.3.1 or earlier, you would actually see something like 30 improvement just from this change from changing it to from 50,000 to 57 to 5304. 

1569
02:14:45,050 --> 02:14:51,042
 So again, one of my favorite examples also of having to understand the under the hood and how it all works 

1570
02:14:51,042 --> 02:14:54,068
and to know what kinds of things to tinker with to push the performance of your code.

1571
02:14:54,068 --> 02:14:58,019
Okay, so at this point, we have improved the performance by about 11x, right? 

1572
02:14:59,017 --> 02:15:05,048
Because we started at about 1000 milliseconds per step, and we're now down to like 93 milliseconds.

1573
02:15:05,048 --> 02:15:10,071
So that's quite good, and we're doing a much better job of utilizing our GPU resources.

1574
02:15:10,092 --> 02:15:17,050
 So I'm going to now turn to more algorithmic changes and improvements to the actual optimization itself.

1575
02:15:17,050 --> 02:15:23,028
What we would like to do is follow the hyperparameters that are mentioned in the GPT-2 or GPT-3 paper. 

1576
02:15:24,016 --> 02:15:27,092
Now, sadly, GPT-2 doesn't actually say too much.

1577
02:15:28,000 --> 02:15:35,033
 It's very nice of them that they released the model weights and the code, but the paper itself is extremely vague regarding the optimization details.

1578
02:15:35,062 --> 02:15:41,034
The code itself that they released, as well as the code we've been looking at, is just the inference code, 

1579
02:15:41,034 --> 02:15:45,067
so there's no training code here and very few hyperparameters. So this doesn't tell us too much. 

1580
02:15:45,067 --> 02:15:57,002
For that, we have to turn to the GPT-3 paper. In the appendix of the GPT-3 paper, they have a lot more hyperparameters for us to use. 

1581
02:15:57,002 --> 02:16:05,026
 The GPT-3 paper, in general, is a lot more detailed regarding all the small details that go into the model training. 

1582
02:16:05,026 --> 02:16:13,027
But GPT-3 models were never released. So, for GPT-2, we have the weights but no details, and for GPT-3, we have lots of details but no weights. 

1583
02:16:14,070 --> 02:16:22,048
 Roughly speaking, the GPT-2 and GPT-3 architectures are very similar. There are very few changes. 

1584
02:16:22,048 --> 02:16:27,085
The context length was expanded from 1024 to 2048, and that's kind of the major change. 

1585
02:16:27,085 --> 02:16:33,010
Some of the hyperparameters around the transformer have changed, but otherwise, they're pretty much the same model. 

1586
02:16:33,022 --> 02:16:39,095
GPT-3 was just trained for a lot longer on a bigger dataset and has a lot more thorough evaluations. 

1587
02:16:39,095 --> 02:16:47,036
 and the GPT-3 model is 175 billion parameters instead of 1.6 billion in GPT-2. 

1588
02:16:47,036 --> 02:16:51,070
Long story short, we're going to follow the GPT-3 paper to follow along with some of the parameters.

1589
02:16:52,044 --> 02:16:59,038
So to train all the versions of GPT-3, we use Adam with beta 1 and beta 2 of 0.9 and 0.95. 

1590
02:16:59,038 --> 02:17:11,000
Let's swing over here and make sure that the betas parameter, which you can see here defaults to 0.9 and 0.999, is actually set to 0.9 and 0.95.

1591
02:17:11,035 --> 02:17:22,022
The epsilon parameter, you can see, is the default is 1e-8. This is also 1e-8. Let's just put it in so that we're explicit.

1592
02:17:23,014 --> 02:17:28,047
Now Next up, they say we clip the global norm of the gradient at 1.0. 

1593
02:17:28,055 --> 02:17:38,052
What this is referring to is that once we calculate the gradients, right after loss.backward, we basically have the gradients at all the parameter tensors.

1594
02:17:38,052 --> 02:17:43,080
What people like to do is basically clip them to have some kind of a maximum norm. 

1595
02:17:45,006 --> 02:17:51,047
So in PyTorch, this is fairly easy to do; it's one line of code that we have to insert right after we calculate the gradients.

1596
02:17:51,075 --> 02:17:58,085
What this utility function is doing is it's calculating the global norm of the parameters. 

1597
02:17:59,002 --> 02:18:06,070
So, every single gradient on all the parameters, you square it and add it all up, then you take a big square root of that. 

1598
02:18:06,090 --> 02:18:15,072
That's the norm of the parameter vector. Basically, it's the length of it if you'd like to look at it that way.

1599
02:18:15,072 --> 02:18:21,052
We are basically making sure that its length is no more than 1.0, and we're going to clip it.

1600
02:18:21,052 --> 02:18:28,098
The reason that people like to use this is that sometimes you can get unlucky during the optimization. Maybe it's a bad data batch or something like that.

1601
02:18:29,038 --> 02:18:36,060
If you get very unlucky in the batch, you might get really high loss, and really high loss could lead to a really high gradient. 

1602
02:18:36,075 --> 02:18:41,042
This could basically shock your model and shock the optimization.

1603
02:18:41,042 --> 02:18:52,042
People like to use gradient norm clipping to prevent the model from getting too big of shocks in terms of the gradient magnitude. 

1604
02:18:52,042 --> 02:19:02,005
 And the upper bound is in this way.  It's a bit of a hacky solution, it's about like a patch on top of like deeper issues,  but people still do it fairly frequently. 

1605
02:19:02,005 --> 02:19:11,015
now, the clip_grad_norm returns the norm of the gradient, which I like to always visualize because it is useful information .

1606
02:19:11,015 --> 02:19:19,062
Sometimes, you can look at the norm of the gradient. If it's well-behaved, things are good; if it's climbing, things are bad and destabilizing during training. 

1607
02:19:19,068 --> 02:19:24,068
 Sometimes, you could get a spike in the norm, which means there's some kind of issue or instability.

1608
02:19:24,082 --> 02:19:34,062
The norm here will be a norm, and let's do a 0.4f or something like that. 

1609
02:19:34,067 --> 02:19:44,055
I believe this is just a float, so we should be able to print that. That's global gradient clipping.

1610
02:19:45,062 --> 02:19:49,042
Now, they go into the details of the learning rate scheduler. 

1611
02:19:49,042 --> 02:19:58,012
They don't just use a fixed learning rate like we do here for 3e-4, but there's actually basically a cosine decay learning rate schedule

1612
02:19:58,025 --> 02:20:10,053
It's got a warm-up and it's got a cosine decay to 10 percent over some horizon, and so we're going to implement this in a second.

1613
02:20:10,063 --> 02:20:14,056
I just like to see norm printed here. Okay, there we go. 

1614
02:20:14,056 --> 02:20:27,054
So what happened here is the norm is actually really high in the beginning, 30 or so, and and you see that as we continue training, it kind of stabilizes at values below one.

1615
02:20:27,054 --> 02:20:32,090
This is not that crazy uncommon for the norm to be high in the very first few stages. 

1616
02:20:32,090 --> 02:20:38,047
Basically, what's happening here is the model is completely random, so there's a ton of learning happening very early in the network.

1617
02:20:38,056 --> 02:20:44,097
That learning is mostly learning the biases of the output tokens. 

1618
02:20:44,097 --> 02:20:49,030
so it's a bit of an unstable time, but the network usually stabilizes in the very few iterations. 

1619
02:20:49,037 --> 02:20:54,092
This looks relatively reasonable to me, except usually I would expect this to look a little bit funky. 

1620
02:20:54,092 --> 02:21:02,005
We go from 28 to 6 to 2 and then to 10. It's not completely insane, but it's just kind of a little bit funky.

1621
02:21:04,065 --> 02:21:14,057
So, Let's now get to the learning rate scheduler. So, the learning rate schedule that's used here in GPT-3 is what's called a cosine decay learning schedule with warm-up.

1622
02:21:14,057 --> 02:21:27,005
The way this looks is that the learning rate basically starts right at around zero, linearly ramps up over some amount of time, and then comes down with this cosine sort of form. 

1623
02:21:27,012 --> 02:21:32,008
It comes down to some kind of a minimum learning rate that's up to you. Here, the minimum learning rate is zero.

1624
02:21:32,064 --> 02:21:42,004
 But here in the paper, they said that they use cosine decay for learning rate down to 10% of its value over the first 260 billion tokens. 

1625
02:21:42,004 --> 02:21:49,097
Then training continues after, and there's a linear warm-up over the first 375 million tokens.

1626
02:21:49,097 --> 02:21:56,083
So, here's the learning rate. Let's now implement this.So I already implemented it here, and the way this works is

1627
02:21:56,085 --> 02:22:00,095
 - let me scroll down first. Here I changed our training loop a little bit.

1628
02:22:00,095 --> 02:22:10,095
 So instead of "for i in max steps," I just changed it to "step" now, so that we have the notion of a step as a single optimization step in the for loop.

1629
02:22:10,095 --> 02:22:17,022
Then, here I get the learning rate for this step of the optimization using a new function I call "get_lr." 

1630
02:22:17,030 --> 02:22:22,016
In PyTorch, to set the learning rate— I think this is the way to set the learning rate. 

1631
02:22:22,016 --> 02:22:28,082
It's a little bit gnarly because you have to basically, there's a notion of different parameter groups that could exist in the optimizer. 

1632
02:22:28,082 --> 02:22:33,080
So you actually have to iterate over them, even though we currently have a single param group only.

1633
02:22:33,080 --> 02:22:39,028
And so, you have to set the LR in this for-loop kind of style, is my impression right now. 

1634
02:22:39,028 --> 02:22:44,028
So, we have this local LR, we set the learning rate, and then at the bottom, I'm also printing it.

1635
02:22:45,083 --> 02:22:50,053
So that's all the changes I made to this loop.  And of course, the "get_lr" is my scheduler.

1636
02:22:50,059 --> 02:22:58,022
Now, it's worth pointing out that PyTorch actually has learning rate schedulers, and you can use them. And I believe there's a cosine learning rate schedule in PyTorch.

1637
02:22:58,035 --> 02:23:07,090
 I just don't really love using that code because, honestly, it's like five lines of code and I fully understand what's happening inside these lines. 

1638
02:23:07,090 --> 02:23:13,082
So, I don't love to use abstractions where they're kind of inscrutable, and then I don't know what they're doing. 

1639
02:23:13,082 --> 02:23:15,062
So, personal style.

1640
02:23:15,065 --> 02:23:28,060
The max learning rate here is, let's say, 3e-4. But we're gonna see that in GPT-3, they have a table of what the maximum learning rate is for every model size.

1641
02:23:28,060 --> 02:23:41,010
So, for this one, basically 12-layer 768 GPT-3, so the GPT-3 Small is roughly like a GPT-2 124M.

1642
02:23:41,020 --> 02:23:44,012
We see that here, they use a learning rate of 6e-4. 

1643
02:23:44,012 --> 02:23:51,074
So, we could actually go higher. In fact, we may want to try to follow that and just set the max LR here at 6.

1644
02:23:51,074 --> 02:23:58,085
That's the maximum learning rate. The min learning rate is 10% of that per description in the paper.

1645
02:23:59,088 --> 02:24:07,008
Some number of steps that we're going to warm up over and then the maximum steps of the optimization, which I now use also in the for loop down here.

1646
02:24:07,008 --> 02:24:11,092
And then, you can go over this code if you like. it's not it's not terribly inside floor interesting.

1647
02:24:11,097 --> 02:24:17,095
 I'm just modulating based on the iteration number of which learning rate there should be. 

1648
02:24:18,003 --> 02:24:29,038
So, this is the warm-up region, this is the region after the optimization, and then this is the region sort of in between, and this is where I calculate the cosine learning rate schedule.


1649
02:24:29,038 --> 02:24:39,045
You can step through this in detail if you'd like, but this is basically implementing this curve. I ran this already, and this is what that looks like.

1650
02:24:42,003 --> 02:24:51,072
When we now run, we start at some very low number. Now, note that we don't start exactly at zero, because that would be not useful to update with a learning rate of zero.

1651
02:24:51,092 --> 02:24:59,000
That's why there's an 'it + 1', so that on the zeroth iteration, we are not using exactly zero; we're using something very, very low. 

1652
02:24:59,005 --> 02:25:06,083
Then we linearly warm up to maximum learning rate which in this case was 3e-4 when I ran it, but now it would be 6e-4. 

1653
02:25:06,087 --> 02:25:15,067
And then it starts to decay all the way down to 3e-5, which was at the time 10% of the original learning rate.

1654
02:25:15,067 --> 02:25:24,040
Now one thing we are not following exactly is that they mentioned... let me see if I can find it again...

1655
02:25:24,053 --> 02:25:33,040
We're not exactly following what they did, because they mentioned that their training horizon is 300 billion tokens, 

1656
02:25:33,040 --> 02:25:40,083
and they come down to 10% of the initial learning rate at 260 billion, and then they train after 260 with 10%.

1657
02:25:40,083 --> 02:25:46,097
So basically their decay time is less than the max steps time, whereas for us they're exactly equal. 

1658
02:25:46,097 --> 02:25:55,030
So it's not exactly faithful, but it's okay. This is okay for us and for our purposes right now. 

1659
02:25:55,030 --> 02:26:01,055
And we're just gonna use this ourselves. I don't think it makes too, too big of a difference honestly.

1660
02:26:01,055 --> 02:26:06,010
I should point out that what learning rate schedule you use is totally up to you. 

1661
02:26:06,015 --> 02:26:11,015
There are many different types. Cosine learning rate has been popularized a lot by GPT-2 and GPT-3, 

1662
02:26:11,054 --> 02:26:14,097
but people have come up with all kinds of other learning rate schedules. 

1663
02:26:15,004 --> 02:26:20,067
This is kind of like an active area of research as to which one is the most effective at training these networks.

1664
02:26:20,067 --> 02:26:22,035
Okay, next step.

1665
02:26:22,035 --> 02:26:26,070
The paper talks about the gradual batch size increase.

1666
02:26:26,075 --> 02:26:34,072
So there's a ramp on the batch size that is linear, and you start with very small batch size and you ramp up to a big batch size over time.

1667
02:26:34,072 --> 02:26:37,062
We're going to actually skip this and we're not going to work with it.

1668
02:26:37,065 --> 02:26:46,012
The reason I don't love to use it is that it complicates a lot of the arithmetic because you are changing the number of tokens that you're processing at every single step of the optimization.

1669
02:26:46,012 --> 02:26:49,022
 I like to keep that math very very simple.

1670
02:26:49,022 --> 02:26:57,070
 Also, my understanding is that this is not like a major improvement. And also my understanding is that this is not like an algorithmic optimization improvement.

1671
02:26:57,070 --> 02:27:00,030
It's more of a systems and speed improvement. 

1672
02:27:01,002 --> 02:27:06,002
Roughly speaking,this is because in the early stages of the optimization, again, 

1673
02:27:06,047 --> 02:27:16,080
the model is in a very atypical setting, and mostly what you're learning is to ignore the tokens that don't come up in your training set very often.

1674
02:27:16,092 --> 02:27:21,092
 You're learning very simple biases and that kind of a thing.

1675
02:27:22,050 --> 02:27:29,025
So, every single example that you put through your network is basically just telling you, "Use these tokens and don't use these tokens."

1676
02:27:29,025 --> 02:27:33,038
So the gradients from every single example are actually extremely highly correlated. 

1677
02:27:33,038 --> 02:27:42,000
They all look roughly the same in the original parts of the optimization, because they're all just telling you that these tokens don't appear and  these tokens do appear.

1678
02:27:42,000 --> 02:27:49,033
So, because the gradients are all very similar and highly correlated, then why are you doing batch sizes of millions, 

1679
02:27:49,081 --> 02:27:54,081
when if you do a batch size of 32k, you're basically getting the exact same gradient early on in the training.

1680
02:27:55,006 --> 02:28:01,045
And then later in the optimization, once you've learned all the simple stuff, that's where the actual work starts. 

1681
02:28:01,045 --> 02:28:09,083
That's where the gradients become more de-correlated per examples and that's where they actually offer you sort of statistical power in some sense.

1682
02:28:09,096 --> 02:28:13,057
So we're gonna skip this just because it kind of complicates things. 

1683
02:28:14,044 --> 02:28:19,048
And we're going to go to "data are sampled without replacement during training".

1684
02:28:21,016 --> 02:28:22,084
So until an epoch boundary is reached. 

1685
02:28:23,010 --> 02:28:29,075
So "without replacement" means that they're not sampling from some fixed pool and then take a sequence, 

1686
02:28:29,083 --> 02:28:34,072
train on it, but then also like return the sequence to the pool. They are exhausting a pool.

1687
02:28:34,072 --> 02:28:38,090
So when they draw a sequence, it's gone until the next epoch of training. 

1688
02:28:39,056 --> 02:28:44,098
So we're already doing that because our data loader iterates over chunks of data.

1689
02:28:45,048 --> 02:28:50,042
So there's no replacement, they don't become eligible to be drawn again until the next epoch. 

1690
02:28:50,074 --> 02:28:52,040
So we're basically already doing that.

1691
02:28:53,062 --> 02:28:59,030
All models use a weight decay of 0.1 to provide a small amount of regularization. 

1692
02:28:59,094 --> 02:29:01,064
So let's implement a weight decay.

1693
02:29:02,012 --> 02:29:04,026
And you see here that I've already kind of made the changes. 

1694
02:29:04,072 --> 02:29:13,032
In particular, instead of creating the optimizer right here, I'm creating a new configure optimizers function inside the model, 

1695
02:29:13,046 --> 02:29:15,086
and I'm passing in some of the hyper parameters instead.

1696
02:29:16,040 --> 02:29:20,038
So let's look at the configure optimizers, which is supposed to return the optimizer object.

1697
02:29:25,060 --> 02:29:28,032
Okay, so it looks complicated, but it's actually really simple. 

1698
02:29:28,056 --> 02:29:31,040
And it's just, we're just being very careful.  

1699
02:29:31,062 --> 02:29:32,090
And there's a few settings here to go through.

1700
02:29:33,028 --> 02:29:38,038
The most important thing with respect to this line is that you see there's a weight decay parameter here. 

1701
02:29:38,048 --> 02:29:48,064
And I'm passing that into, well, I'm passing that into something called optim groups that eventually ends up going into the AdamW optimizer.

1702
02:29:49,057 --> 02:29:59,070
And the weight decay that's by default used in AdamW here is 0.01. So it's 10 times lower than what's used in GPT-3 paper here.

1703
02:30:01,030 --> 02:30:04,096
So the weight decay basically ends up making its way into the AdamW3 optimizer groups.

1704
02:30:05,034 --> 02:30:07,076
Now, what else is going on here in this function?

1705
02:30:08,032 --> 02:30:13,044
So the two things that are happening here that are important is that I'm splitting up the parameters into those 

1706
02:30:13,044 --> 02:30:16,024
should be weight decayed and those that should not be weight decayed.

1707
02:30:16,054 --> 02:30:24,044
So in particular, it is common to not weight decay biases and any other sort of one-dimensional tensors.

1708
02:30:24,094 --> 02:30:28,030
So the one-dimensional tensors are in the node decay params.

1709
02:30:28,086 --> 02:30:32,090
And these are also things like layer norm, scales, and biases.

1710
02:30:32,092 --> 02:30:35,008
It doesn't really make sense to weight decay those.

1711
02:30:35,046 --> 02:30:40,036
You mostly want to weight decay the weights that participate in matrix multiplications.

1712
02:30:40,076 --> 02:30:43,048
And you want to potentially weight decay the embeddings.

1713
02:30:44,042 --> 02:30:51,002
And we've covered in a previous video why it makes sense to decay the weights, because you can sort of think of it as a regularization.

1714
02:30:51,032 --> 02:30:56,012
Because when you're pulling down all the weights, you're forcing the optimization to use more of the weights.

1715
02:30:56,078 --> 02:31:00,094
And you're not allowing any one of the weights individually to be way too large.

1716
02:31:02,006 --> 02:31:10,064
You're forcing the network to kind of like distribute the work across more channels, because there's sort of like a pull of gravity on the weights themselves.

1717
02:31:12,000 --> 02:31:15,042
So that's why we are separating it in those ways here.

1718
02:31:15,074 --> 02:31:20,008
We're only decaying the embeddings and the matmul participating weights.

1719
02:31:20,068 --> 02:31:24,018
We're printing the number of parameters that we're decaying and not.

1720
02:31:24,038 --> 02:31:25,094
Most of the parameters will be decayed.

1721
02:31:26,048 --> 02:31:30,066
And then one more thing that we're doing here is I'm doing another optimization here.

1722
02:31:32,008 --> 02:31:37,076
And previous AdamW did not have this option, but later parts of PyTorch introduced it.

1723
02:31:38,010 --> 02:31:47,074
And that's why I'm guarding it with an inspect.signature, which is basically checking if this fused quark is present inside AdamW.

1724
02:31:48,010 --> 02:31:52,066
And then if it is present, I'm going to end up using it and passing it in here.

1725
02:31:53,042 --> 02:31:55,092
Because some earlier versions do not have fused equals.

1726
02:31:56,070 --> 02:31:59,016
So here's AdamW fused equals.

1727
02:31:59,042 --> 02:32:01,030
It did not used to exist, and it was added later.

1728
02:32:02,008 --> 02:32:04,016
And there's some docs here for what's happening.

1729
02:32:05,014 --> 02:32:11,082
And basically they say that by default they do not use fused, because it is relatively new and we want to give it sufficient bake time.

1730
02:32:12,022 --> 02:32:17,066
So by default they don't use fused, but fused is a lot faster when it is available and when you're running on CUDA.

1731
02:32:18,016 --> 02:32:24,054
And what that does is instead of iterating in a for loop over all the parameter tensors and updating them,

1732
02:32:25,010 --> 02:32:26,096
that would launch a lot of kernels, right?

1733
02:32:27,026 --> 02:32:31,096
And so fused just means that all those kernels are fused into a single kernel.

1734
02:32:32,034 --> 02:32:39,078
You get rid of a lot of overhead, and you a single time on all the parameters call a kernel that updates them.

1735
02:32:40,057 --> 02:32:46,074
And so it's just basically a kernel fusion for the AdamW update instead of iterating over all the tensors.

1736
02:32:48,014 --> 02:32:51,018
So that's the configure optimizers function that I like to use.

1737
02:32:51,052 --> 02:32:52,066
And we can rerun.

1738
02:32:53,026 --> 02:32:59,040
And we're not going to see any major differences from what we saw before, but we are going to see some prints coming from here.

1739
02:32:59,092 --> 02:33:01,018
So let's just take a look at what they look like.

1740
02:33:02,068 --> 02:33:07,016
So we see that number of decay tensors is 50, and it's most of the primers.

1741
02:33:07,066 --> 02:33:12,030
And number of non-decay tensors is 98, and these are the biases in the layer norm parameters mostly.

1742
02:33:12,057 --> 02:33:15,010
And there's only 100,000 of those.

1743
02:33:15,060 --> 02:33:16,084
So most of it is decayed.

1744
02:33:17,030 --> 02:33:21,072
And then we are using the fused implementation of AdamW, which will be a lot faster.

1745
02:33:22,026 --> 02:33:25,024
So if you have it available, I would advise you to use it.

1746
02:33:25,034 --> 02:33:27,060
I'm not actually 100% sure why they don't default to it.

1747
02:33:27,064 --> 02:33:29,030
It seems fairly benign and harmless.

1748
02:33:30,057 --> 02:33:35,012
And also because we are using the fused implementation, I think this is why we have dropped.

1749
02:33:36,094 --> 02:33:42,078
Notice that the running time used to be 93 milliseconds per step, and we're now down to 90 milliseconds per step because

1750
02:33:42,078 --> 02:33:44,088
of using the fused AdamW optimizer.

1751
02:33:45,030 --> 02:33:54,094
So in a single commit here, we are introducing fused Adam, getting improvements on the time, and we're adding or changing the weight decay.

1752
02:33:55,040 --> 02:34:00,078
But we're only weight decaying the two-dimensional parameters, the embeddings, and the matrices that participate in linear.

1753
02:34:01,057 --> 02:34:04,092
So that is this, and we can take this out.

1754
02:34:05,096 --> 02:34:08,090
And yeah, that is it for this line.

1755
02:34:09,062 --> 02:34:11,042
One more quick note before we continue here.

1756
02:34:11,070 --> 02:34:17,048
I just want to point out that the relationship between weight decay, learning rate, batch size, the Adam parameters,

1757
02:34:17,064 --> 02:34:24,066
beta1, beta2, the epsilon, and so on, these are very complicated mathematical relationships in the optimization literature.

1758
02:34:25,050 --> 02:34:31,014
And for the most part in this video, I'm just trying to copy-paste the settings that OpenAI used.

1759
02:34:31,034 --> 02:34:33,080
But this is a complicated topic, quite deep.

1760
02:34:34,048 --> 02:34:40,004
And yeah, in this video, I just want to copy the parameters because it's a whole different video to really talk about that

1761
02:34:40,004 --> 02:34:43,036
in detail and give it a proper justice instead of just high-level intuitions.

1762
02:34:44,068 --> 02:34:52,054
Now, the next thing that I want to move on to is that this paragraph here, by the way, we're going to turn back around to when we improve our data loader.

1763
02:34:52,074 --> 02:35:08,084
For now, I want to swing back around to this table, where you will notice that for different models, we of course have different hyperparameters for the transformer that

1764
02:35:08,084 --> 02:35:10,074
dictate the size of the transformer network.

1765
02:35:11,042 --> 02:35:12,080
We also have a different learning rate.

1766
02:35:12,094 --> 02:35:16,057
So we're seeing the pattern that the bigger networks are trained with slightly lower learning rates.

1767
02:35:17,064 --> 02:35:24,016
And we also see this batch size, where in the small networks, they use a smaller batch size, and in the bigger networks,

1768
02:35:24,028 --> 02:35:25,038
they use a bigger batch size.

1769
02:35:26,030 --> 02:35:33,044
Now, the problem for us is we can't just use 0.5 million batch size, because if I just try to come in here and

1770
02:35:33,044 --> 02:35:36,094
I try to set this B, where is my B?

1771
02:35:38,012 --> 02:35:47,056
B equals 16.

1772
02:35:47,096 --> 02:35:52,004
If I try to set, well, we have to be careful.

1773
02:35:52,016 --> 02:35:56,014
It's not 0.5 million, because this is the batch size in the number of tokens.

1774
02:35:56,048 --> 02:35:59,064
Every single one of our rows is 1024 tokens.

1775
02:36:00,024 --> 02:36:03,038
So 0.5E6, 1 million divided by 1024.

1776
02:36:04,000 --> 02:36:07,092
This would need about a 488 batch size.

1777
02:36:08,040 --> 02:36:14,002
So the problem is I can't come in here and set this to 488, because my GPU would explode.

1778
02:36:15,012 --> 02:36:16,048
This would not fit for sure.

1779
02:36:17,034 --> 02:36:26,034
But we still want to use this batch size, because again, as I mentioned, the batch size is correlated with all the other optimization hyperparameters and

1780
02:36:26,034 --> 02:36:27,034
the learning rates and so on.

1781
02:36:27,078 --> 02:36:34,054
So we want to have a faithful representation of all the hyperparameters, and therefore we need to use a batch size of 0.5 million,

1782
02:36:34,074 --> 02:36:35,000
roughly.

1783
02:36:35,066 --> 02:36:39,074
But the question is, how do we use 0.5 million if we only have a small GPU?

1784
02:36:40,038 --> 02:36:43,010
Well, for that, we need to use what's called gradient accumulation.

1785
02:36:43,092 --> 02:36:50,020
So we're going to turn to that next, and it allows us to simulate in a serial way any arbitrary batch size that

1786
02:36:50,020 --> 02:36:50,062
we set.

1787
02:36:51,024 --> 02:36:58,062
And so we can do a batch size of 0.5 million, we just have to run longer, and we have to process multiple sequences and

1788
02:36:58,062 --> 02:37:03,064
basically add up all the gradients from them to simulate a batch size of 0.5 million.

1789
02:37:04,012 --> 02:37:05,018
So let's turn to that next.

1790
02:37:05,066 --> 02:37:08,088
Okay, so I started the implementation right here, just by adding these lines of code.

1791
02:37:09,072 --> 02:37:13,088
And basically what I did is first, I set the total batch size that we desire.

1792
02:37:14,038 --> 02:37:22,026
So this is exactly 0.5 million, and I used a nice number, a power of 2, because 2 to the 19 is 524288,

1793
02:37:22,064 --> 02:37:24,048
so it's roughly 0.5 million, and it's a nice number.

1794
02:37:25,042 --> 02:37:28,078
Now, our micro-batch size, as we call it now, is 16.

1795
02:37:29,050 --> 02:37:35,088
So this is going to be, we still have B by T indices that go into the transformer and do forward, backward,

1796
02:37:36,038 --> 02:37:37,090
but we're not going to do an update, right?

1797
02:37:38,016 --> 02:37:44,040
We're going to do many forward, backwards, and those gradients are all going to plus equals on the parameter gradients.

1798
02:37:44,056 --> 02:37:45,044
They're all going to add up.

1799
02:37:46,002 --> 02:37:52,057
So we're going to do forward, backward, grad, accoum, steps, number of times, and then we're going to do a single update once all of that

1800
02:37:52,057 --> 02:37:53,014
is accumulated.

1801
02:37:54,000 --> 02:38:01,050
So in particular, our micro-batch size is just now controlling how many tokens, how many rows we're processing in a single go of a forward,

1802
02:38:01,057 --> 02:38:01,080
backward.

1803
02:38:02,038 --> 02:38:07,042
So here, we are doing 16 times 124.

1804
02:38:07,076 --> 02:38:17,018
We're doing 16384 tokens per forward, backward, and we are supposed to be doing 2 to the 19.

1805
02:38:17,064 --> 02:38:18,066
Whoops, what am I doing?

1806
02:38:19,004 --> 02:38:23,098
2 to the 19 in total, so the grad, accoum will be 32.

1807
02:38:27,026 --> 02:38:34,012
So therefore, grad, accoum here will work out to 32, and we have to do 32 forward, backward, and then

1808
02:38:34,012 --> 02:38:34,072
a single update.

1809
02:38:35,030 --> 02:38:45,064
Now, we see that we have about 100 milliseconds for a single forward, backward, so doing 32 of them will make every step roughly three seconds in just napkin math.

1810
02:38:47,014 --> 02:38:50,004
So that's grad, accoum, steps, but now we actually have to implement that.

1811
02:38:50,032 --> 02:38:59,032
So we're going to swing over to our training loop, because now this part here and this part here, the forward and

1812
02:38:59,032 --> 02:39:04,084
the backward, we have to now repeat this 32 times before we do everything else that follows.

1813
02:39:05,014 --> 02:39:07,008
So let's see how we can implement that.

1814
02:39:07,074 --> 02:39:12,042
So let's come over here, and actually, we do have to load a new batch every single time, so let me move that

1815
02:39:12,042 --> 02:39:14,088
over here, and now this is where we have the inner loop.

1816
02:39:15,046 --> 02:39:26,014
So for micro step in range, grad, accoum, steps, we do this, and remember that last step backward always deposits gradients,

1817
02:39:26,030 --> 02:39:30,004
so we're doing inside last step backward, there's always a plus equals on the gradients.

1818
02:39:30,062 --> 02:39:34,074
So in every single last step backward, gradients will add up on the gradient tensors.

1819
02:39:35,088 --> 02:39:44,072
So we lost that backward, and then we get all the gradients over there, and then we normalize, and everything else should just follow.

1820
02:39:45,084 --> 02:39:53,000
So we're very close, but actually there's like subtle and deep issue here, and this is actually incorrect.

1821
02:39:53,050 --> 02:39:59,000
So I invite you to think about why this is not yet sufficient, and let me fix it then.

1822
02:39:59,074 --> 02:40:04,080
Okay, so I brought back the Jupyter notebook, so we can think about this carefully in a simple toy setting,

1823
02:40:05,006 --> 02:40:05,074
and see what's happening.

1824
02:40:06,080 --> 02:40:12,042
So let's create a very simple neural net that takes a 16, vector of 16 numbers, and returns a single number,

1825
02:40:12,090 --> 02:40:25,018
and then here I'm creating some random examples x, and some targets y, and then we are using the mean squared loss here to calculate the loss.

1826
02:40:25,086 --> 02:40:34,018
So basically what this is, is four individual examples, and we're just doing simple regression with the mean squared loss over those four examples.

1827
02:40:35,012 --> 02:40:40,036
Now when we calculate the loss, and we lost that backward, and look at the gradient, this is the gradient that

1828
02:40:40,036 --> 02:40:40,078
we achieve.

1829
02:40:42,018 --> 02:40:49,042
Now the loss objective here, notice that in MSC loss, the default for the loss function is reduction is mean.

1830
02:40:49,086 --> 02:40:57,028
So we're calculating the average mean loss, the mean loss here, over the four examples.

1831
02:40:57,068 --> 02:41:04,057
So this is the exact loss objective, and this is the average, the one over four, because there are four independent examples here,

1832
02:41:04,088 --> 02:41:12,060
and then we have the four examples, and their mean squared error, the squared error, and then this makes it the mean squared error.

1833
02:41:13,086 --> 02:41:20,008
So therefore, we are, we calculate the squared error, and then we normalize it to make it the mean over the examples,

1834
02:41:20,028 --> 02:41:21,030
and there's four examples here.

1835
02:41:22,008 --> 02:41:30,098
So now when we come to the gradient accumulation version of it, this here is the gradient accumulation version of it,

1836
02:41:31,014 --> 02:41:37,056
where we have grad-account steps of four, and I reset the gradient, with grad-account steps of four, and

1837
02:41:37,056 --> 02:41:42,020
now I'm evaluating all the examples individually instead, and calling loss.backward on them many times,

1838
02:41:42,056 --> 02:41:44,078
and then we're looking at the gradient that we achieve from that.

1839
02:41:45,024 --> 02:41:51,032
So basically now we forward our function, calculate the exact same loss, do it backward, and we do that

1840
02:41:51,032 --> 02:41:56,070
four times, and when we look at the gradient, you'll notice that the gradients don't match.

1841
02:41:57,090 --> 02:42:05,026
So here we did a single batch of four, and here we did four gradient accumulation steps of batch size one,

1842
02:42:05,060 --> 02:42:07,042
and the gradients are not the same.

1843
02:42:08,034 --> 02:42:13,074
And basically the reason that they're not the same is exactly because this mean squared error gets lost.

1844
02:42:13,094 --> 02:42:23,004
This one quarter in this loss gets lost, because what happens here is the loss objective for every one of the loops is just a mean squared error,

1845
02:42:23,046 --> 02:42:28,020
which in this case, because there's only a single example, is just this term here.

1846
02:42:28,046 --> 02:42:32,046
So that was the loss in the zeroth iteration, same in the first, third, and so on.

1847
02:42:33,012 --> 02:42:44,034
And then when you do the loss.backward, we're accumulating gradients, and what happens is that accumulation in the gradient is basically equivalent to doing a sum in the loss.

1848
02:42:45,012 --> 02:42:52,052
So our loss actually here is this, without the factor of one quarter outside of it.

1849
02:42:53,022 --> 02:42:56,078
So we're missing the normalizer, and therefore our gradients are off.

1850
02:42:57,024 --> 02:43:03,062
And so the way to fix this, or one of them, is basically we can actually come here and we can say loss equals loss divide four.

1851
02:43:06,008 --> 02:43:12,054
And what happens now is that we're introducing, we're scaling our loss, we're introducing a one quarter in front of all of these places.

1852
02:43:16,008 --> 02:43:22,068
So all the individual losses are now scaled by one quarter, and then when we backward, all of these accumulate with a sum,

1853
02:43:22,098 --> 02:43:28,048
but now there's a one quarter inside every one of these components, and now our losses will be equivalent.

1854
02:43:29,038 --> 02:43:34,052
So when I run this, you see that the gradients are now identical.

1855
02:43:35,050 --> 02:43:41,048
So long story short, with this simple example, when you step through it, you can see that basically the reason that

1856
02:43:41,048 --> 02:43:55,006
this is not correct is because in the same way as here in the MSC loss, the loss that we're calculating here in the model is using a reduction of mean as well.

1857
02:43:55,084 --> 02:43:56,062
So where is the loss?

1858
02:43:56,088 --> 02:43:57,072
F dot cross entropy.

1859
02:43:58,074 --> 02:44:04,084
And by default, the reduction here in cross entropy is also, I don't know why they don't show it, but

1860
02:44:04,084 --> 02:44:09,042
it's the mean loss at all the b by t elements.

1861
02:44:12,002 --> 02:44:17,014
So there's a reduction by mean in there, and if we're just doing this gradient accumulation here, we're missing that.

1862
02:44:17,078 --> 02:44:22,080
And so the way to fix this is to simply compensate for the number of gradient accumulation steps, and

1863
02:44:22,080 --> 02:44:24,050
we can in the same way divide this loss.

1864
02:44:24,090 --> 02:44:32,016
So in particular here, the number of steps that we're doing is loss equals loss divided gradient accumulation steps.

1865
02:44:32,072 --> 02:44:35,044
So even Copilot gets the modification.

1866
02:44:36,014 --> 02:44:41,050
But in the same way exactly, we are scaling down the loss so that when we do loss step backward, which

1867
02:44:41,050 --> 02:44:48,050
basically corresponds to a sum in the objective, we are summing up the already normalized loss.

1868
02:44:48,090 --> 02:44:54,086
And therefore when we sum up the losses divided by grad-accum steps, we are recovering the additional normalizer.

1869
02:44:55,074 --> 02:45:02,068
And so now these two will be, now this will be equivalent to the original sort of optimization because

1870
02:45:02,068 --> 02:45:03,070
the gradient will come out the same.

1871
02:45:04,092 --> 02:45:08,080
Okay, so I had to do a few more touch-ups, and I launched the optimization here.

1872
02:45:09,018 --> 02:45:14,014
So in particular, one thing we want to do because we want to print things nicely is, well, first of all,

1873
02:45:14,020 --> 02:45:16,074
we need to create like an accumulator over the loss.

1874
02:45:16,088 --> 02:45:21,032
We can't just print the loss because we'd be printing only the final loss at the final micro step.

1875
02:45:22,012 --> 02:45:27,057
So instead we have a loss-accum which I initialized at zero, and then I accumulate the loss into it.

1876
02:45:28,030 --> 02:45:36,086
And I'm using detach so that I'm detaching the tensor from the graph, and I'm just trying to keep track of the values.

1877
02:45:37,012 --> 02:45:39,057
So I'm making these leaf nodes when I add them.

1878
02:45:40,044 --> 02:45:43,054
So that's loss-accum, and then we're printing that here instead of loss.

1879
02:45:43,098 --> 02:45:49,054
And then in addition to that, I had to account for the grad-accum steps inside the tokens processed, because

1880
02:45:49,054 --> 02:45:53,082
now the token's processed per step is b times t times gradient accumulation.

1881
02:45:55,024 --> 02:45:57,074
So long story short, here we have the optimization.

1882
02:45:58,024 --> 02:46:00,030
It looks reasonable, right?

1883
02:46:00,032 --> 02:46:01,054
We're starting at a good spot.

1884
02:46:02,002 --> 02:46:07,054
We calculated the grad-accum steps to be 32, and we're getting about three seconds here, right?

1885
02:46:10,078 --> 02:46:13,098
And so this looks pretty good.

1886
02:46:14,016 --> 02:46:17,014
Now if you'd like to verify that your optimization and the implementation here is correct, and you're working on a side,

1887
02:46:17,032 --> 02:46:24,068
well now because we have the total batch size and the gradient accumulation steps, our setting of b is purely a performance optimization kind of setting.

1888
02:46:24,068 --> 02:46:33,044
So if you have a big GPU, you can actually increase this to 32, and you'll probably go a bit faster.

1889
02:46:33,086 --> 02:46:39,078
If you have a very small GPU, you can try eight or four, but in any case, you should be getting the exact same optimization and

1890
02:46:39,078 --> 02:46:47,012
the same answers up to like a floating point error, because the gradient accumulation kicks in and can handle everything serially as necessary.

1891
02:46:50,020 --> 02:46:52,060
So that's it for gradient accumulation, I think.

1892
02:46:52,064 --> 02:46:55,034
Okay, so now is the time to bring out the heavy weapons.

1893
02:46:56,010 --> 02:47:02,030
You've noticed that so far we've only been using a single GPU for training, but actually I am paying for eight GPUs here,

1894
02:47:02,086 --> 02:47:04,057
and so we should be putting all of them to work.

1895
02:47:04,057 --> 02:47:13,042
And in particular, they're all going to collaborate and optimize over tokens at the same time and communicate

1896
02:47:13,042 --> 02:47:17,020
so that they're all kind of collaborating on the optimization.

1897
02:47:17,020 --> 02:47:20,092
For this, we are going to be using the distributed data parallel from PyTorch.

1898
02:47:21,000 --> 02:47:26,022
There's also a legacy data parallel, which I recommend you not use, and that's kind of like legacy.

1899
02:47:27,006 --> 02:47:29,032
Distributed data parallel is a very simple way.

1900
02:47:29,072 --> 02:47:37,016
We have eight GPUs, so we're going to launch eight processes, and each process is going to be assigned a GPU,

1901
02:47:38,008 --> 02:47:42,098
and for each process, the training loop and everything we've worked on so far is going to look pretty much the same.

1902
02:47:43,050 --> 02:47:48,096
Each GPU, as far as it's concerned, is just working on exactly what we've built so far, but now secretly,

1903
02:47:49,042 --> 02:47:53,034
there's eight of them, and they're all going to be processing slightly different parts of the data.

1904
02:47:54,042 --> 02:47:58,092
And we're going to add one more part where once they all calculate their gradients, 

1905
02:47:58,097 --> 02:48:02,084
there's one more part where we do an average of those gradients.

1906
02:48:04,048 --> 02:48:08,082
And so that's how they're going to be collaborating on the computational workload here.

1907
02:48:09,062 --> 02:48:17,056
So to use all eight of them, we're not going to be launching our script anymore with just PyTorch train GPT2.py.

1908
02:48:17,084 --> 02:48:21,046
We're going to be running it with a special command called torchrun in PyTorch.

1909
02:48:21,056 --> 02:48:22,022
We'll see that in a bit.

1910
02:48:23,038 --> 02:48:30,032
And torchrun, when it runs our Python script, will actually make sure to run eight of them in parallel,

1911
02:48:30,086 --> 02:48:41,034
and it creates these environmental variables where each of these processes can look up which basically which one of the processes it is.

1912
02:48:41,034 --> 02:48:47,004
So for example, torchrun will set rank, local rank, and world size, environmental variables.

1913
02:48:47,004 --> 02:48:50,092
And so this is a bad way to detect whether DDP is running.

1914
02:48:51,018 --> 02:48:58,028
So if we're using torchrun, if DDP is running, then we have to make sure that CUDA is available, because

1915
02:48:58,028 --> 02:49:02,044
I don't know that you can run this on CPU anymore, or that that makes sense to do.

1916
02:49:03,054 --> 02:49:06,072
This is some setup code here.

1917
02:49:06,090 --> 02:49:11,026
The important part is that there's a world size, which for us will be eight.

1918
02:49:11,042 --> 02:49:13,002
That's the total number of processes running.

1919
02:49:13,082 --> 02:49:21,020
There's a rank, which is each process will basically run the exact same code at the exact same time, roughly.

1920
02:49:21,092 --> 02:49:27,008
But the only difference between these processes is that they all have a different DDP rank.

1921
02:49:27,088 --> 02:49:34,096
So the GPU 0 will have DDP rank of 0, GPU 1 will have a rank of 1, etc.

1922
02:49:35,064 --> 02:49:37,088
So otherwise, they're all running the exact same script.

1923
02:49:38,026 --> 02:49:40,074
It's just that DDP rank will be a slightly different integer.

1924
02:49:41,040 --> 02:49:45,054
And that is the way for us to coordinate that they don't, for example, run on the same data.

1925
02:49:45,078 --> 02:49:48,064
We want them to run on different parts of the data, and so on.

1926
02:49:49,084 --> 02:49:53,096
Now, local rank is something that is only used in a multi-node setting.

1927
02:49:54,020 --> 02:49:56,022
We only have a single node with eight GPUs.

1928
02:49:56,080 --> 02:50:00,084
And so local rank is the rank of the GPU on a single node.

1929
02:50:01,054 --> 02:50:04,008
So from 0 to 7, as an example.

1930
02:50:05,020 --> 02:50:07,028
But for us, we're mostly going to be running on a single box.

1931
02:50:07,046 --> 02:50:10,012
So the things we care about are rank and world size.

1932
02:50:10,052 --> 02:50:18,052
This is 8, and this will be whatever it is, depending on the GPU, that this particular instantiation of the script runs on.

1933
02:50:19,034 --> 02:50:29,036
Now here, we make sure that according to the local rank, we are setting the device to be CUDA colon, and

1934
02:50:29,036 --> 02:50:33,052
colon indicates which GPU to use if there are more than one GPUs.

1935
02:50:34,012 --> 02:50:40,020
So depending on the local rank of this process, it's going to use just the appropriate GPU.

1936
02:50:40,048 --> 02:50:43,000
So there's no collisions on which GPU is being used by which process.

1937
02:50:44,040 --> 02:50:49,022
And finally, there's a Boolean variable that I like to create, which is the DDP rank equals equals zero.

1938
02:50:49,080 --> 02:50:57,020
So the master process is arbitrarily process number zero, and it does a lot of the printing, logging, checkpointing, etc.

1939
02:50:57,020 --> 02:51:00,078
And the other processes are thought of mostly as compute processes that are assisting.

1940
02:51:01,054 --> 02:51:04,052
And so master process zero will have some additional work to do.

1941
02:51:04,084 --> 02:51:07,084
All the other processes will mostly just be doing forward and backwards.

1942
02:51:08,026 --> 02:51:13,068
And if we're not using DDP, and none of these variables are set, we revert back to single GPU training.

1943
02:51:14,004 --> 02:51:20,022
So that means that we only have rank zero, the world size is just one, and we are the master process,

1944
02:51:20,064 --> 02:51:24,068
and we try to auto detect the device, and this is world as normal.

1945
02:51:25,092 --> 02:51:32,024
So so far, all we've done is we've initialized DDP, and in the case where we're running with Torchrun,

1946
02:51:32,038 --> 02:51:37,088
which we'll see in a bit, there's gonna be eight copies running in parallel, each one of them will have a different rank.

1947
02:51:38,078 --> 02:51:42,090
And now we have to make sure that everything happens correctly afterwards.

1948
02:51:43,038 --> 02:51:51,028
So the tricky thing with running multiple processes is you always have to imagine that there's going to be eight processes running in parallel.

1949
02:51:51,074 --> 02:51:59,020
So as you read the code, now you have to imagine there's eight, you know, eight Python interpreters running down these lines of code.

1950
02:51:59,036 --> 02:52:02,057
And the only difference between them is that they have a different DDP rank.

1951
02:52:03,012 --> 02:52:11,010
So they all come here, they all pick the exact same seed, they all make all of these calculations, completely unaware of the other copies running,

1952
02:52:11,030 --> 02:52:12,014
roughly speaking, right.

1953
02:52:12,072 --> 02:52:15,020
So they all make the exact same calculations.

1954
02:52:15,052 --> 02:52:23,036
And now we have to adjust these calculations to take into account that there's actually like a certain world size and certain ranks.

1955
02:52:23,036 --> 02:52:28,094
So in particular, these micro batches and sequence lengths, these are all just per GPU, right.

1956
02:52:29,064 --> 02:52:33,060
So now there's going to be num processes of them running in parallel.

1957
02:52:34,012 --> 02:52:43,025
So we have to adjust this, right, because the grad_accum_steps now is going to be total batch size divided by B times T times DDP world size,

1958
02:52:43,078 --> 02:52:50,086
because each process will do B times T, and there's this many often.

1959
02:52:51,082 --> 02:52:59,020
And so in addition to that, we want to make sure that this fits nicely into total batch size, which for us it will because

1960
02:52:59,020 --> 02:53:05,072
16 times 1024 times eight GPUs is 131k.

1961
02:53:06,002 --> 02:53:12,094
And so 524288, this means that our grad_accum_steps will be four with the current settings, right.

1962
02:53:13,050 --> 02:53:18,082
So there's going to be 16 times 124 process in each GPU, and then there's eight GPUs.

1963
02:53:18,090 --> 02:53:25,090
So we're going to be doing 131,000 tokens in a single forward backward on the eight GPUs.

1964
02:53:27,038 --> 02:53:31,092
So we want to make sure that this fits nicely so that we can derive a nice gradient accumulation steps.

1965
02:53:33,050 --> 02:53:40,008
And yeah, let's just adjust the comments here times DDP world size.

1966
02:53:40,088 --> 02:53:44,054
Okay, so each GPU calculates this.

1967
02:53:45,002 --> 02:53:47,000
Now this is where we start to get run into issues, right.

1968
02:53:47,008 --> 02:53:52,040
So we are, each process is going to come by a print, and they're all going to print.

1969
02:53:52,066 --> 02:53:55,056
So we're going to have eight copies of these prints.

1970
02:53:56,026 --> 02:53:59,046
So one way to deal with this is exactly this master process variable that we have.

1971
02:53:59,046 --> 02:54:03,008
So if master process, then guard this.

1972
02:54:03,052 --> 02:54:09,042
And that's just so that we just print this a single time, because otherwise, all the processes would have computed the exact same variables,

1973
02:54:09,054 --> 02:54:11,034
and there's no need to print this eight times.

1974
02:54:12,078 --> 02:54:20,046
Before getting into the data loader, and we're going to have to refactor it, obviously, maybe at this point is we should do some prints,

1975
02:54:20,072 --> 02:54:24,024
and just take it out for a spin and exit at this point.

1976
02:54:24,052 --> 02:54:37,026
So import sys and sys.exit and print imgpudp rank.

1977
02:54:39,048 --> 02:54:44,060
i'm gpu, ddp_rank and print bye.

1978
02:54:45,026 --> 02:54:50,086
So now let's try to run this and just see how this works.

1979
02:54:51,006 --> 02:54:53,034
So let's take it for a spin, just so we see what it looks like.

1980
02:54:53,090 --> 02:54:57,012
So normally, we used to launch Python train gpt2.py like this.

1981
02:54:57,057 --> 02:55:00,034
Now we're going to run with torchrun, and this is what it looks like.

1982
02:55:00,070 --> 02:55:06,000
So torchrun, standalone, number of processes, for example, is eight for us, because we have eight GPUs,

1983
02:55:06,082 --> 02:55:08,024
and then train gpt2.py.

1984
02:55:08,070 --> 02:55:11,038
So this is what the command would look like.

1985
02:55:11,076 --> 02:55:13,078
And torch run, again, we'll run eight of these.

1986
02:55:14,086 --> 02:55:15,078
So let's just see what happens.

1987
02:55:16,028 --> 02:55:19,060
So first, it gets a little busy.

1988
02:55:19,070 --> 02:55:20,066
So there's a lot going on here.

1989
02:55:20,092 --> 02:55:26,052
So first of all, there's some warnings from distributed, and I don't actually know that these mean anything.

1990
02:55:26,066 --> 02:55:34,064
I think this is just like, the code is setting up and the processes are coming online, and we're seeing some preliminary failure to collect while the processes come up.

1991
02:55:34,078 --> 02:55:36,054
I'm not 100% sure about that.

1992
02:55:37,028 --> 02:55:39,098
But we start to then get into actual prints.

1993
02:55:40,064 --> 02:55:43,060
So all the processes went down.

1994
02:55:43,096 --> 02:55:49,078
And then the first print actually comes from process five, just by chance.

1995
02:55:50,040 --> 02:55:51,010
And then it printed.

1996
02:55:51,046 --> 02:55:53,008
So process five basically got here first.

1997
02:55:53,024 --> 02:55:55,068
It said, on process, on GPU five, bye.

1998
02:55:56,054 --> 02:56:00,082
And then these prints come from the master process.

1999
02:56:02,048 --> 02:56:05,010
So process five just finished first, for whatever reason.

2000
02:56:05,018 --> 02:56:08,002
It just depends on how the operating system scheduled the processes to run.

2001
02:56:08,082 --> 02:56:11,060
Then GPU zero ended, then GPU three and two.

2002
02:56:12,070 --> 02:56:17,008
And then probably process five or something like that has exited.

2003
02:56:17,088 --> 02:56:25,086
And DDP really doesn't like that, because we didn't properly dispose of the multi GPUs setting.

2004
02:56:26,046 --> 02:56:29,066
And so process group has not been destroyed before we destruct.

2005
02:56:30,048 --> 02:56:32,012
So it really doesn't like that.

2006
02:56:32,042 --> 02:56:38,080
And in actual application, we would want to call destroyed process group, so that we clean up DDP properly.

2007
02:56:39,060 --> 02:56:40,096
And so it doesn't like that too much.

2008
02:56:41,030 --> 02:56:43,018
And then the rest of the GPUs finish.

2009
02:56:43,046 --> 02:56:44,054
And that's it.

2010
02:56:45,020 --> 02:56:49,088
So basically, we can't guarantee when these processes are running is totally arbitrary, but they are running in parallel.

2011
02:56:50,040 --> 02:56:51,052
We don't want that to be printing.

2012
02:56:53,022 --> 02:56:56,020
And next up, let's erase this.

2013
02:56:58,006 --> 02:57:04,092
Next up, we want to make sure that when we create DataLoaderLite, we need to now make it aware of this multi process setting,

2014
02:57:05,038 --> 02:57:12,000
because we don't want all the processes to be loading the exact same data, we want every process to get its own chunk of data,

2015
02:57:12,034 --> 02:57:15,014
so that they're all working on different parts of the data set, of course.

2016
02:57:15,076 --> 02:57:16,096
So let's adjust that.

2017
02:57:17,040 --> 02:57:25,088
So one particularly simple and a naive way to do this is we have to make sure that we pass in the rank and the size to the data loader.

2018
02:57:26,012 --> 02:57:30,064
And then we come up here, we see that we now take rank and processes and we save them.

2019
02:57:31,010 --> 02:57:33,074
Now, the current position will not be zero.

2020
02:57:34,040 --> 02:57:38,060
Because what we want is we want to stride out all the processes.

2021
02:57:39,050 --> 02:57:45,086
So one way to do this is we basically take self.B times self.T, and then multiply it by the process rank.

2022
02:57:46,084 --> 02:57:52,094
So process rank zero will start at zero, but process rank one now starts at B * T.

2023
02:57:53,040 --> 02:57:56,098
Process rank two is starts at 2 * B * T，, etc.

2024
02:57:57,054 --> 02:57:58,086
So that is the initialization.

2025
02:57:59,070 --> 02:58:02,066
Now, we still, they still do this identically.

2026
02:58:03,034 --> 02:58:09,060
But now when we advance, we don't advance by B * T, we advance by B * T * num_processes.

2027
02:58:11,010 --> 02:58:18,070
So basically, the total number of tokens that we're consuming is B * T * num_processes.

2028
02:58:19,010 --> 02:58:22,016
And they all go off to a different rank.

2029
02:58:22,084 --> 02:58:25,060
And the position has to advance by the entire chunk.

2030
02:58:27,020 --> 02:58:34,090
And then here at B * T * self.num_processes +1 would be to exceed number of tokens, then

2031
02:58:34,090 --> 02:58:35,044
we're going to loop.

2032
02:58:35,090 --> 02:58:38,070
And when we loop, we want to of course loop in the exact same way.

2033
02:58:39,038 --> 02:58:41,020
So we sort of like reset back.

2034
02:58:42,040 --> 02:58:48,050
So this is the simplest change that I can find for kind of a very simple distributed DataLoaderLite.

2035
02:58:48,098 --> 02:58:56,062
And you can notice that if process rank is zero, and numProcesses is one, then the whole thing will be identical to what we had before.

2036
02:58:56,066 --> 02:59:00,086
But now we can have actually multiple processes running and they should work fine.

2037
02:59:03,018 --> 02:59:04,068
So that's the data loader.

2038
02:59:05,000 --> 02:59:11,030
Okay, so next up, once they've all initialized the data loader, they come here and they all create a GPT model.

2039
02:59:11,084 --> 02:59:15,057
So we create eight GPT models on eight processes.

2040
02:59:15,057 --> 02:59:20,024
But because the seeds are fixed here, they all create the same identical model.

2041
02:59:20,024 --> 02:59:24,067
They all move it to the device of their rank, and they all compile the model.

2042
02:59:24,067 --> 02:59:31,022
And because the models are identical, there are eight identical compilations happening in parallel, but that's okay.

2043
02:59:31,022 --> 02:59:34,047
Now, none of this changes because that is on a per step basis.

2044
02:59:34,055 --> 02:59:43,024
And we're currently working kind of within step, because we need to just... All the changes we're making are kind of like within-step changes.

2045
02:59:43,024 --> 02:59:48,042
Now, the important thing here is when we construct the model, we actually have a bit of work to do here.

2046
02:59:49,024 --> 02:59:51,020
GetLogits is deprecated, so create model.

2047
02:59:51,044 --> 02:59:58,006
We need to actually wrap the model into the distributed data parallel container.

2048
02:59:59,070 --> 03:00:03,070
So this is how we wrap the model into the DDP container.

2049
03:00:04,048 --> 03:00:07,053
And these are the docs for DDP and they're quite extensive.

2050
03:00:07,053 --> 03:00:14,037
And there's a lot of caveats, and a lot of things to be careful with. Because everything complexifies times 10 when multiple processes are involved.

2051
03:00:14,037 --> 03:00:18,072
But roughly speaking, this device IDs I believe has to be passed in.

2052
03:00:18,072 --> 03:00:24,008
Now, unfortunately, the docs for what device IDs is extremely unclear.

2053
03:00:24,022 --> 03:00:32,026
So when you actually like come here, this comment for what device IDs is roughly nonsensical.

2054
03:00:32,026 --> 03:00:35,014
But I'm pretty sure it's supposed to be the DDP local rank.

2055
03:00:35,014 --> 03:00:37,038
So not the DDP rank, the local rank.

2056
03:00:37,090 --> 03:00:40,020
So this is what you pass in here.

2057
03:00:40,060 --> 03:00:41,060
This wraps the model.

2058
03:00:42,014 --> 03:00:46,056
And in particular, what DDP does for you is in a forward pass, it actually behaves identically.

2059
03:00:46,094 --> 03:00:50,078
So my understanding of it is nothing should be changed in the forward pass.

2060
03:00:51,010 --> 03:00:59,092
But in the backward pass, as you are doing the backward pass, in the simplest setting, once the backward pass is over on each independent GPU,

2061
03:01:00,036 --> 03:01:04,014
each independent GPU has the gradient for all the parameters.

2062
03:01:04,092 --> 03:01:10,020
And what DDP does for you is once the backward pass is over, it will call what's called all reduce.

2063
03:01:10,082 --> 03:01:16,080
And it basically does an average across all the ranks of their gradients.

2064
03:01:17,070 --> 03:01:21,008
And then it will deposit that average on every single rank.

2065
03:01:21,046 --> 03:01:24,054
So every single rank will end up with the average on it.

2066
03:01:25,032 --> 03:01:29,004
And so basically, that's the communication, it just synchronizes and averages the gradients.

2067
03:01:29,038 --> 03:01:30,062
And that's what DDP offers you.

2068
03:01:30,096 --> 03:01:39,032
Now, DDP actually is a little bit more, is a little bit more involved than that, because as you are doing the backward pass through the layers in the transformer,

2069
03:01:39,060 --> 03:01:44,078
it actually can dispatch communications for the gradient while the backward pass is still happening.

2070
03:01:45,018 --> 03:01:51,012
So there's overlap of the communication of the gradients and the synchronization of them and the backward pass.

2071
03:01:51,046 --> 03:01:55,084
And this is just more efficient and to do it that way.

2072
03:01:56,038 --> 03:01:57,096
So that's what DDP does for you.

2073
03:01:59,008 --> 03:02:01,096
Forward is unchanged, and backward is mostly unchanged.

2074
03:02:02,018 --> 03:02:05,024
And we're tacking on this average, as we'll see in a bit.

2075
03:02:06,014 --> 03:02:08,064
Okay, so now let's go to the optimization.

2076
03:02:09,000 --> 03:02:10,014
Nothing here changes.

2077
03:02:10,084 --> 03:02:16,040
Let's go to the optimization here, the inner loop, and think through the synchronization of these gradients in the DDP.

2078
03:02:17,026 --> 03:02:23,000
So basically, by default, what happens, as I mentioned, is when you do loss.backward here, it will do the backward pass,

2079
03:02:23,010 --> 03:02:24,090
and then it will synchronize the gradients.

2080
03:02:25,072 --> 03:02:36,022
The problem here is because of the gradient accumulation steps loop here, we don't actually want to do the synchronization after every single loss.backward,

2081
03:02:36,032 --> 03:02:41,044
because we are just depositing gradients, and we're doing that serially, and we just want them adding up.

2082
03:02:41,092 --> 03:02:43,086
And we don't want to synchronize every single time.

2083
03:02:43,094 --> 03:02:45,010
That would be extremely wasteful.

2084
03:02:45,068 --> 03:02:54,032
So basically, we want to add them up, and then on the very last, it's only on the very last step, when microstep becomes grad_accum_steps minus one,

2085
03:02:54,082 --> 03:03:01,006
only at that last step do we want to actually do the all-reduce to average up the gradients.

2086
03:03:01,054 --> 03:03:09,048
So to do that, we come here, and the official sanctioned way, is to do this no-sync context manager.

2087
03:03:10,032 --> 03:03:15,052
So PyTorch says, this is a context manager to disable gradient synchronization across DDP processes.

2088
03:03:16,022 --> 03:03:23,046
So within this context, gradients will be accumulated, and basically, when you do no-sync, there will be no communication.

2089
03:03:23,090 --> 03:03:30,054
So they are telling us to do, with DDP no-sync, do the gradient accumulation, accumulate grads, and then

2090
03:03:30,054 --> 03:03:33,096
they are asking us to do DDP again with another input and that backward.

2091
03:03:34,052 --> 03:03:36,042
And I just really don't love this.

2092
03:03:36,057 --> 03:03:38,020
I just really don't like it.

2093
03:03:38,042 --> 03:03:43,000
The fact that you have to copy-paste your code here and use a context manager, and this is just super ugly.

2094
03:03:43,044 --> 03:03:50,048
So when I went to the source code here, you can see that when you enter, you simply toggle this variable,

2095
03:03:51,006 --> 03:03:57,048
this require backward grad-sync, and this is being toggled around and changed.

2096
03:03:57,094 --> 03:04:06,006
And this is the variable that basically, if you step through it, is being toggled to determine if the gradient is going to be synchronized.

2097
03:04:06,048 --> 03:04:09,004
So I actually just kind of like to use that directly.

2098
03:04:09,094 --> 03:04:12,028
So instead, what I like to do is the following.

2099
03:04:13,096 --> 03:04:23,036
Right here, before the last dot backward, if we are using DDP, then basically, we only want to synchronize,

2100
03:04:23,098 --> 03:04:29,038
we only want this variable to be true when it is the final iteration.

2101
03:04:29,080 --> 03:04:33,057
In all the other iterations inside the microsteps, we want it to be false.

2102
03:04:34,006 --> 03:04:35,086
So I just toggle it like this.

2103
03:04:36,044 --> 03:04:40,084
So require backward grad-sync should only turn on when the microstep is the last step.

2104
03:04:42,004 --> 03:04:47,080
And so I'm toggling this variable directly, and I hope that that impacts loss.backward.

2105
03:04:48,020 --> 03:04:53,026
And this is a naughty thing to do because, you know, they could probably change the DDP and this variable will go away.

2106
03:04:53,060 --> 03:04:59,048
But for now, I believe this works, and it allows me to avoid the use of context managers and code duplication.

2107
03:05:00,000 --> 03:05:04,016
I'm just toggling the variable, and then loss.backward will not synchronize most of the steps, and

2108
03:05:04,016 --> 03:05:05,052
it will synchronize the very last step.

2109
03:05:05,094 --> 03:05:21,040
And so once this is over, and we come out, every single rank will suddenly, magically have the average of all the gradients that were stored on all the ranks.

2110
03:05:21,040 --> 03:05:30,010
So now we have to think through whether that is what we want, and also if this suffices, and whether how it works with the loss,

2111
03:05:30,024 --> 03:05:31,042
and what is loss-accum.

2112
03:05:31,057 --> 03:05:33,010
So let's think through that now.

2113
03:05:33,064 --> 03:05:41,000
And the problem I'm getting at is that we've averaged the gradients, which is great, but the loss-accum has not been impacted yet.

2114
03:05:41,028 --> 03:05:45,098
And this is outside of the DDP container, so that is not being averaged.

2115
03:05:46,038 --> 03:05:52,022
And so here, when we are printing loss-accum, well, presumably we're only going to be printing on the master process, rank 0, 

2116
03:05:53,010 --> 03:05:57,026
and it's just going to be printing the losses that it saw on its process.

2117
03:05:57,026 --> 03:06:03,014
But instead, we want it to print the loss over all the processes, and the average of that loss, because

2118
03:06:03,014 --> 03:06:05,078
we did average of gradients, so we want the average of loss as well.

2119
03:06:06,036 --> 03:06:11,028
So simply here, after this, this is the code that I've used in the past.

2120
03:06:12,080 --> 03:06:15,002
And instead of loss-eff, we want loss-accum.

2121
03:06:15,068 --> 03:06:22,028
So if DDP, again, then dist is a PyTorch distributed.

2122
03:06:22,094 --> 03:06:24,024
I imported...

2123
03:06:24,024 --> 03:06:25,036
where do I import it?

2124
03:06:26,056 --> 03:06:28,020
Oh gosh.

2125
03:06:29,004 --> 03:06:31,044
So this file is starting to get out of control, huh?

2126
03:06:32,020 --> 03:06:35,010
So import torch.distributed as dist.

2127
03:06:35,070 --> 03:06:41,057
So dist.all_reduce, and we're doing the average on loss-accum.

2128
03:06:42,002 --> 03:06:44,090
And so this loss-accum tensor exists on all the ranks.

2129
03:06:45,036 --> 03:06:51,066
When we call allReduce of average, it creates the average of those numbers, and it deposits that average on all the ranks.

2130
03:06:52,018 --> 03:06:58,060
So all the ranks after this call will now contain loss-accum averaged up.

2131
03:06:58,098 --> 03:07:03,040
And so when we print here on the master process, the loss-accum is identical in all the other ranks as well.

2132
03:07:04,004 --> 03:07:09,052
So here, if master process, we want to print like this.

2133
03:07:10,048 --> 03:07:14,060
Okay, and finally, we have to be careful because we're not processing even more tokens.

2134
03:07:15,008 --> 03:07:20,072
So times DDP world size, that's the number of tokens that we've processed up above.

2135
03:07:22,026 --> 03:07:27,004
And everything else should be fine.

2136
03:07:27,060 --> 03:07:32,062
The only other thing to be careful with is, as I mentioned, you want to destroy the process group so that

2137
03:07:32,062 --> 03:07:39,066
we are nice to nickel and it's not going to DDP and it's not going to complain to us when we exit here.

2138
03:07:40,010 --> 03:07:42,062
So that should be it.

2139
03:07:42,072 --> 03:07:43,074
Let's try to take it for a spin.

2140
03:07:44,006 --> 03:07:47,050
Okay, so I launched the script and it should be printing here imminently.

2141
03:07:47,092 --> 03:07:53,084
We're now training with eight GPUs at the same time, so the gradient accumulation steps is now 32, it is now divide 8,

2142
03:07:54,012 --> 03:07:54,098
and it's just 4.

2143
03:07:56,030 --> 03:08:00,050
So otherwise, this is what the optimization now looks like.

2144
03:08:01,022 --> 03:08:02,046
And wow, we're going really fast.

2145
03:08:02,060 --> 03:08:08,034
So we're processing 1.5 million tokens per second now.

2146
03:08:08,066 --> 03:08:10,048
So these are some serious numbers.

2147
03:08:10,090 --> 03:08:15,098
And the tiny Shakespeare dataset is so tiny that we're just doing like so many epochs over it, most likely.

2148
03:08:16,062 --> 03:08:17,090
But this is roughly what it looks like.

2149
03:08:18,036 --> 03:08:25,074
One thing that I had to fix, by the way, is that this was model.configure_optimizers, which now doesn't work because

2150
03:08:25,074 --> 03:08:27,010
model now is a DDP model.

2151
03:08:27,064 --> 03:08:32,003
So instead, this has to become rawModel.configure_optimizers,

2152
03:08:32,003 --> 03:08:34,078
 where rawModel is something I create here.

2153
03:08:35,024 --> 03:08:43,028
So right after I wrap the model into DDP, I have to create the rawModel, which in the case of DDP is a model.module,

2154
03:08:43,036 --> 03:08:51,095
is where it stores the raw module of GPT-2 as we have it, which contains the configureOptimizers function that we want to call.

2155
03:08:52,024 --> 03:08:53,072
So that's one thing that I had to fix.

2156
03:08:54,044 --> 03:08:55,064
Otherwise, this seems to run.

2157
03:08:56,004 --> 03:09:01,096
Now, one thing you'll notice is that when you actually compare this run and the numbers in it to the just running a single GPU,

2158
03:09:02,044 --> 03:09:08,070
you'll notice that this is a single GPU run with 32 radicum, the numbers won't exactly match up.

2159
03:09:09,090 --> 03:09:12,048
And it's kind of a boring reason for why that happens.

2160
03:09:12,088 --> 03:09:18,018
The reason for that is that in a data loader, we're basically just iterating through batches in a slightly different way,

2161
03:09:18,050 --> 03:09:20,082
because now we're looking for an entire page of data.

2162
03:09:21,018 --> 03:09:27,057
And if that page for all the GPUs, if that chunk exceeds the number of tokens, we just loop.

2163
03:09:28,008 --> 03:09:35,002
And so actually, the single GPU and the GPU process will end up resetting in a slightly different manner.

2164
03:09:35,022 --> 03:09:36,088
And so our batches are slightly different.

2165
03:09:37,040 --> 03:09:38,068
And so we get slightly different numbers.

2166
03:09:39,026 --> 03:09:45,078
But one way to convince yourself that this is okay, is just make the total batch size much smaller, and the B and  T.

2167
03:09:46,030 --> 03:09:51,080
And then, so I think I used 4 times 124 times 8.

2168
03:09:52,016 --> 03:09:54,072
So I used 32768 as a total batch size.

2169
03:09:54,094 --> 03:10:01,056
And then, so I made sure that the single GPU will do eight gradient accumulation steps, and then the multi GPU.

2170
03:10:02,002 --> 03:10:06,090
And then you're reducing the boundary effects of the data loader, and you'll see that the numbers match up.

2171
03:10:07,032 --> 03:10:10,018
So long story short, we're now going really, really fast.

2172
03:10:10,070 --> 03:10:14,026
The optimization is mostly consistent with GPT-2 and 3 hybrid parameters.

2173
03:10:15,006 --> 03:10:20,016
And we have outgrown our tiny Shakespeare file, and we want to upgrade it.

2174
03:10:20,030 --> 03:10:21,086
So let's move to next to that next.

2175
03:10:21,098 --> 03:10:25,004
So let's now take a look at what data sets were used by GPT-2 and GPT-3.

2176
03:10:25,090 --> 03:10:29,064
So GPT-2 used this web text data set that was never released.

2177
03:10:30,060 --> 03:10:33,044
There's an attempt at reproducing it called open web text.

2178
03:10:34,010 --> 03:10:39,004
So basically, roughly speaking, what they say here in the paper is that is create all outbound links from Reddit,

2179
03:10:39,076 --> 03:10:42,012
and then with at least three karma.

2180
03:10:42,052 --> 03:10:44,024
And that was kind of like their starting point.

2181
03:10:44,038 --> 03:10:47,020
And they collected all the web page, all the web pages and all the text in them.

2182
03:10:47,070 --> 03:10:49,048
And so this was 45 million links.

2183
03:10:49,070 --> 03:10:51,066
And this ended up being 40 gigabytes of text.

2184
03:10:51,082 --> 03:10:57,030
So that's roughly what GPT-2 says about its data set.

2185
03:10:57,036 --> 03:10:59,000
So it's basically outbound links from Reddit.

2186
03:10:59,074 --> 03:11:03,008
Now, when we go over to GPT-3, there's a training data set section.

2187
03:11:03,028 --> 03:11:08,084
And that's where they start to talk about Common Crawl, which is a lot more used.

2188
03:11:09,076 --> 03:11:12,060
Actually, I think even GPT-2 talked about Common Crawl.

2189
03:11:13,064 --> 03:11:17,096
But basically, it's not a very high quality data set all by itself, because it's extremely noisy.

2190
03:11:18,014 --> 03:11:20,016
This is a completely random subset of the internet.

2191
03:11:20,057 --> 03:11:22,010
And it's much worse than you think.

2192
03:11:22,082 --> 03:11:26,038
So people go into great lengths to filter Common Crawl, because there's good stuff in it.

2193
03:11:26,060 --> 03:11:31,010
But most of it is just like ad spam, random tables and numbers and stock tickers.

2194
03:11:31,030 --> 03:11:34,040
And it's just a total mess.

2195
03:11:35,040 --> 03:11:42,052
So that's why people like to train on these data mixtures that they curate and are careful with.

2196
03:11:42,094 --> 03:11:49,012
So a large chunk of these data mixtures typically will be Common Crawl, like, for example, 50% of the tokens will be Common Crawl.

2197
03:11:49,028 --> 03:11:52,094
But then here in GPT-3, they're also using Webtext 2 from before.

2198
03:11:53,010 --> 03:11:54,010
So that's Reddit outbound.

2199
03:11:54,052 --> 03:11:59,094
But they're also adding, for example, books, and they're adding Wikipedia, there's many other things you can decide to add.

2200
03:12:00,068 --> 03:12:03,022
Now, this data set for GPT-3 was also never released.

2201
03:12:03,052 --> 03:12:09,046
So today, some of the data sets that I'm familiar with, there are quite good and would be representative of something along these lines,

2202
03:12:09,090 --> 03:12:17,012
are number one, the red pajama data set, or more specifically, for example, the slim pajama subset of the red pajama data set,

2203
03:12:17,066 --> 03:12:20,052
which is a cleaned and deduplicated version of it.

2204
03:12:20,090 --> 03:12:27,012
And just to give you a sense, again, it's a bunch of Common Crawl, C4, which is also, as far as I know,

2205
03:12:27,024 --> 03:12:29,022
more Common Crawl, but processed differently.

2206
03:12:29,084 --> 03:12:33,044
And then we have GitHub, books, archive, Wikipedia, StackExchange.

2207
03:12:33,078 --> 03:12:36,024
These are the kinds of data sets that would go into these data mixtures.

2208
03:12:36,070 --> 03:12:41,066
Now, specifically, the one that I like that came out recently is called FindWebDataSet.

2209
03:12:42,024 --> 03:12:49,010
So this is an attempt to basically collect really high quality Common Crawl data and filter it, in this case,

2210
03:12:49,018 --> 03:12:50,040
to 15 trillion tokens.

2211
03:12:51,022 --> 03:12:59,048
And then in addition to that, more recently, Hugging Face released this FindWebEDU subset, which is 1.3 trillion of educational and

2212
03:12:59,048 --> 03:13:02,016
5.4 trillion of high educational content.

2213
03:13:02,060 --> 03:13:08,008
So basically, they're trying to filter Common Crawl to very high quality educational subsets.

2214
03:13:08,062 --> 03:13:10,064
And this is the one that we will use.

2215
03:13:11,012 --> 03:13:17,012
There's a long web page here on FindWeb, and they go into a ton of detail about how they process the data,

2216
03:13:17,026 --> 03:13:18,094
which is really fascinating reading, by the way.

2217
03:13:19,020 --> 03:13:24,060
And I would definitely recommend, if you're interested into data mixtures and so on, and how data gets processed at these scales,

2218
03:13:24,074 --> 03:13:25,092
look at this page.

2219
03:13:26,048 --> 03:13:30,004
And more specifically, we'll be working with the FindWebEDU, I think.

2220
03:13:30,098 --> 03:13:33,056
And it's basically educational content from the internet.

2221
03:13:35,014 --> 03:13:42,008
They show that training on educational content in their metrics works really, really well.

2222
03:13:43,016 --> 03:13:50,066
And we're going to use this sample 10 billion tokens subsample of it, because we're not going to be training on trillions of tokens.

2223
03:13:51,008 --> 03:13:55,092
We're just going to train on 10 billion sample of the FindWebEDU.

2224
03:13:56,024 --> 03:14:02,008
Because empirically, in my previous few experiments, this actually suffices to really get close to GPT-2 performance.

2225
03:14:02,060 --> 03:14:04,052
And it's simple enough to work with.

2226
03:14:04,068 --> 03:14:07,016
And so let's work with the sample 10 BT.

2227
03:14:07,057 --> 03:14:13,076
So our goal will be to download it, process it, and make sure that our data loader can work with it.

2228
03:14:14,010 --> 03:14:15,024
So let's get to that.

2229
03:14:15,068 --> 03:14:23,050
Okay, so I introduced another file here that will basically download FindWebEDU from Hugging Face datasets.

2230
03:14:23,096 --> 03:14:27,004
It will pre-process and pre-tokenize all of the data.

2231
03:14:27,052 --> 03:14:33,032
And it will save data shards to a folder on a local disk.

2232
03:14:34,034 --> 03:14:43,016
And so while this is running, just wanted to briefly mention that you can kind of look through the dataset viewer here just to get a sense of what's in here.

2233
03:14:43,066 --> 03:14:44,046
And it's kind of interesting.

2234
03:14:44,057 --> 03:14:48,002
I mean, it basically looks like it's working fairly well.

2235
03:14:48,034 --> 03:14:50,030
Like it's talking about nuclear energy in France.

2236
03:14:51,006 --> 03:14:57,006
It's talking about Mexican America, some Mac Pi Js, etc.

2237
03:14:57,030 --> 03:14:59,098
So actually, it seems like their filters are working pretty well.

2238
03:15:00,064 --> 03:15:05,098
The filters here, by the way, were applied automatically using LLAMA370B, I believe.

2239
03:15:06,020 --> 03:15:10,072
And so basically, LLMs are judging which content is educational.

2240
03:15:10,098 --> 03:15:12,052
And that ends up making it through the filter.

2241
03:15:13,034 --> 03:15:14,012
So that's pretty cool.

2242
03:15:14,052 --> 03:15:21,034
Now, in terms of the script itself, I'm not going to go through the full script because it's not as interesting and not as LLM-centric.

2243
03:15:21,034 --> 03:15:27,097
But when you run this, basically, number one, we going to load the dataset, which this is all Hugging Face code running this.

2244
03:15:28,011 --> 03:15:31,010
You're going to need to pip install datasets.

2245
03:15:33,012 --> 03:15:38,095
So, it's downloading the dataset, then it is tokenizing all of the documents inside this dataset.

2246
03:15:39,016 --> 03:15:42,080
Now, when we tokenize the documents, you'll notice that 

2247
03:15:42,080 --> 03:15:50,002
to tokenize a single document, we first start the tokens with the end of text token.

2248
03:15:50,054 --> 03:15:53,008
And this is a special token in the GPT-2 tokenizer, as you know.

2249
03:15:53,046 --> 03:15:57,000
So 50,256 is the ID of the end of text.

2250
03:15:57,038 --> 03:16:00,042
And this is what begins a document, even though it's called end of text.

2251
03:16:00,078 --> 03:16:03,046
But this is the first token that begins a document.

2252
03:16:04,024 --> 03:16:07,016
Then we extend with all of the tokens of that document.

2253
03:16:07,052 --> 03:16:09,078
Then we create a NumPy array out of that.

2254
03:16:10,044 --> 03:16:13,094
We make sure that all the tokens are between...

2255
03:16:13,094 --> 03:16:16,086
Okay, let me debug this.

2256
03:16:17,056 --> 03:16:18,068
Okay, so apologies for that.

2257
03:16:19,002 --> 03:16:25,092
It just had to do with me using a float division in Python and must be integer division so that this is an int and everything is nice.

2258
03:16:26,078 --> 03:16:30,032
Okay, but basically, the tokenization here is relatively straightforward.

2259
03:16:30,074 --> 03:16:33,020
Returns tokens in np.uint16.

2260
03:16:33,080 --> 03:16:40,042
We're using uint16 to save a little bit of space because 2 to the 16 minus 1 is 65,000.

2261
03:16:40,080 --> 03:16:43,054
So the GPT-2 max token ID is well below that.

2262
03:16:44,050 --> 03:16:46,062
And then here, there's a bunch of multiprocessing code.

2263
03:16:46,088 --> 03:16:48,004
And it's honestly not that exciting.

2264
03:16:48,020 --> 03:16:49,062
So I'm not going to step through it.

2265
03:16:49,078 --> 03:16:54,076
But we're loading the dataset, we're tokenizing it, and we're saving everything to shards.

2266
03:16:55,026 --> 03:16:56,086
And the shards are NumPy files.

2267
03:16:57,068 --> 03:17:03,040
So just storing a NumPy array, which is very, very similar to Torch tensors.

2268
03:17:04,010 --> 03:17:08,046
And the first shard, 000, is a validation shard.

2269
03:17:08,082 --> 03:17:11,016
And all the other shards are training shards.

2270
03:17:11,082 --> 03:17:15,006
And as I mentioned, they all have 100 million tokens in them exactly.

2271
03:17:16,046 --> 03:17:21,054
And that just makes it easier to work with, to shard the files.

2272
03:17:21,088 --> 03:17:25,056
Because if we just have a single massive file, sometimes they can be hard to work with on the disk.

2273
03:17:26,006 --> 03:17:29,050
And so sharding it is just kind of nicer from that perspective.

2274
03:17:31,012 --> 03:17:32,098
And yeah, so we'll just let this run.

2275
03:17:33,016 --> 03:17:37,062
This will be probably 30-ish minutes or so.

2276
03:17:37,080 --> 03:17:39,088
And then we're going to come back to actually train on this data.

2277
03:17:40,038 --> 03:17:43,004
And we're going to be actually doing some legit pre-training in this case.

2278
03:17:43,014 --> 03:17:44,042
This is a good dataset.

2279
03:17:44,096 --> 03:17:46,064
We're doing lots of tokens per second.

2280
03:17:47,018 --> 03:17:48,028
We have eight GPUs.

2281
03:17:48,040 --> 03:17:49,002
The code is ready.

2282
03:17:49,052 --> 03:17:51,078
And so we're actually going to be doing a serious training run.

2283
03:17:52,002 --> 03:17:53,044
So let's get back in a bit.

2284
03:17:54,032 --> 03:17:55,020
Okay, so we're back.

2285
03:17:56,010 --> 03:18:01,016
So if we ls edu fineweb, we see that there's now 100 shards in it.

2286
03:18:02,066 --> 03:18:05,042
And that makes sense because each shard is 100 million tokens.

2287
03:18:05,076 --> 03:18:08,046
So 100 shards of that is 10 billion tokens in total.

2288
03:18:09,022 --> 03:18:13,032
Now, swinging over to the main file, I made some adjustments to our data loader again.

2289
03:18:13,088 --> 03:18:16,084
And that's because we're not running with Shakespeare anymore.

2290
03:18:17,022 --> 03:18:19,062
We want to use the fineweb shards.

2291
03:18:20,028 --> 03:18:23,078
And so you'll see some code here that additionally basically can load these shards.

2292
03:18:24,040 --> 03:18:27,076
We load the np.uint16 numpy file.

2293
03:18:28,018 --> 03:18:33,088
We convert it to a torch.long tensor, which is what a lot of the layers up top expect by default.

2294
03:18:34,050 --> 03:18:36,036
And then here we're just enumerating all the shards.

2295
03:18:37,020 --> 03:18:39,096
I also added a split to data loader light.

2296
03:18:40,022 --> 03:18:44,042
So we can load the split train, but also the split val, the zero split.

2297
03:18:45,048 --> 03:18:47,024
And then we can load the shards.

2298
03:18:47,054 --> 03:18:52,012
And then here we also have not just a current position now, but also the current shard.

2299
03:18:52,054 --> 03:18:54,028
So we have a position inside a shard.

2300
03:18:54,070 --> 03:19:01,052
And then when we run out of tokens in a single shard, we first advance the shard and loop if we need to.

2301
03:19:01,057 --> 03:19:04,026
And then we get the tokens and readjust the position.

2302
03:19:04,094 --> 03:19:08,054
So this data loader will now iterate all the shards as well.

2303
03:19:09,048 --> 03:19:10,057
So I changed that.

2304
03:19:10,072 --> 03:19:17,016
And then the other thing that I did while the data was processing is our train loader now has split train,

2305
03:19:17,024 --> 03:19:17,056
of course.

2306
03:19:18,036 --> 03:19:21,012
And down here I set up some numbers.

2307
03:19:21,040 --> 03:19:29,098
So we are doing 2 to the 19 tokens per step.

2308
03:19:30,064 --> 03:19:37,008
And we want to do roughly 10 billion tokens, because that's how many unique tokens we have.

2309
03:19:37,052 --> 03:19:43,064
So if we did 10 billion tokens, then divide that by 2 to the 19, we see that this is 19,073 steps.

2310
03:19:43,088 --> 03:19:44,072
So that's where that's from.

2311
03:19:45,024 --> 03:19:50,092
And then the GPT-3 paper says that they warm up the learning rate over 375 million tokens.

2312
03:19:51,060 --> 03:20:00,020
So I came here and 375E6 tokens divide 2 to the 19 is 715 steps.

2313
03:20:00,056 --> 03:20:02,090
So that's why warm up steps is set to 715.

2314
03:20:03,024 --> 03:20:07,078
So this will exactly match the warm up schedule that GPT-3 used.

2315
03:20:08,064 --> 03:20:11,048
And I think 715, by the way, is very mild.

2316
03:20:11,084 --> 03:20:13,054
And this could be made significantly more aggressive.

2317
03:20:13,074 --> 03:20:15,042
Probably even like 100 is good enough.

2318
03:20:16,034 --> 03:20:17,068
But it's okay.

2319
03:20:17,078 --> 03:20:20,090
Let's leave it for now so that we have the exact hyperparameters of GPT-3.

2320
03:20:21,046 --> 03:20:23,008
So I fixed that.

2321
03:20:23,012 --> 03:20:26,036
And then that's pretty much it.

2322
03:20:26,050 --> 03:20:27,084
We can run.

2323
03:20:28,012 --> 03:20:29,030
So we have our script here.

2324
03:20:30,080 --> 03:20:32,022
And we can launch.

2325
03:20:33,060 --> 03:20:34,076
And actually, sorry, let me do one more thing.

2326
03:20:38,054 --> 03:20:39,048
Excuse me.

2327
03:20:40,044 --> 03:20:44,038
For my GPU, I can actually fit more batch size.

2328
03:20:44,048 --> 03:20:49,088
And I believe I can fit 64 on my GPU as a micro batch size.

2329
03:20:50,008 --> 03:20:51,018
So let me try that.

2330
03:20:55,000 --> 03:20:56,054
I could be misremembering.

2331
03:20:57,010 --> 03:20:59,060
But that means 64 times 124 per GPU.

2332
03:20:59,088 --> 03:21:01,002
And then we have eight GPUs.

2333
03:21:01,042 --> 03:21:04,052
So that means we would not even be doing gradient accumulation if this fits.

2334
03:21:04,080 --> 03:21:09,096
Because this just multiplies out to the full total batch size.

2335
03:21:10,060 --> 03:21:12,004
So no gradient accumulation.

2336
03:21:12,044 --> 03:21:14,092
And that would run pretty quickly if that fits.

2337
03:21:27,042 --> 03:21:27,074
Let's go.

2338
03:21:28,082 --> 03:21:31,092
I mean, if this works, then this is basically a serious pre-training run.

2339
03:21:32,052 --> 03:21:33,072
We're not logging.

2340
03:21:34,006 --> 03:21:35,072
We're not evaluating the validation split.

2341
03:21:35,088 --> 03:21:37,052
We're not running any evaluations yet.

2342
03:21:37,092 --> 03:21:40,070
So it's not we haven't crossed our T's and dotted our I's.

2343
03:21:41,016 --> 03:21:46,016
But if we let this run for a while, we're going to actually get a pretty good model.

2344
03:21:46,028 --> 03:21:50,098
And the model that might even be on par with or better than GPT-2-124m.

2345
03:21:52,020 --> 03:21:55,036
So it looks like everything is going great.

2346
03:21:55,056 --> 03:21:58,016
We're processing 1.5 million tokens per second.

2347
03:21:59,032 --> 03:22:02,030
Everything here looks good.

2348
03:22:03,026 --> 03:22:06,054
We're doing 330 milliseconds per iteration.

2349
03:22:07,010 --> 03:22:11,048
And we have to do a total of where are we printing that.

2350
03:22:11,060 --> 03:22:12,022
19073.

2351
03:22:12,084 --> 03:22:18,064
So 19073 times 0.33 is this many seconds.

2352
03:22:18,090 --> 03:22:20,012
This many minutes.

2353
03:22:20,072 --> 03:22:23,046
So this will run for 1.7 hours.

2354
03:22:25,010 --> 03:22:28,056
So one and a half hour run like this.

2355
03:22:29,000 --> 03:22:31,080
And we don't even have to use gradient accumulation, which is nice.

2356
03:22:32,006 --> 03:22:33,088
And you might not have that luxury in your GPU.

2357
03:22:34,016 --> 03:22:37,028
In that case, just start decreasing the batch size until things fit.

2358
03:22:37,072 --> 03:22:38,080
But keep it to nice numbers.

2359
03:22:41,018 --> 03:22:42,002
So that's pretty exciting.

2360
03:22:42,048 --> 03:22:43,092
We're currently warming up the learning rate.

2361
03:22:43,098 --> 03:22:45,066
So you see that it's still very low.

2362
03:22:45,084 --> 03:22:46,046
1e-4.

2363
03:22:46,084 --> 03:22:53,024
So this will ramp up over the next few steps all the way to 6e-4 here.

2364
03:22:54,026 --> 03:22:55,004
Very cool.

2365
03:22:55,048 --> 03:22:58,060
So now what I'd like to do is let's cross the T's and dot our I's.

2366
03:22:58,082 --> 03:23:00,060
Let's evaluate on the validation split.

2367
03:23:01,020 --> 03:23:06,020
And let's try to figure out how we can run evals, how we can do logging, how we can visualize our losses,

2368
03:23:06,068 --> 03:23:07,062
and all the good stuff.

2369
03:23:08,002 --> 03:23:10,070
So let's get to that before we actually do the run.

2370
03:23:11,016 --> 03:23:11,036
Okay.

2371
03:23:11,044 --> 03:23:14,044
So I've adjusted the code so that we're evaluating on the validation split.

2372
03:23:14,094 --> 03:23:22,006
So creating the val loader just by passing in split equals val, that will basically create a data loader just for the validation shard.

2373
03:23:23,018 --> 03:23:29,010
The other thing I did is in the data loader, I introduced a new function reset, which is called at init.

2374
03:23:29,064 --> 03:23:31,054
And it basically resets the data loader.

2375
03:23:31,082 --> 03:23:38,006
And that is very useful because when we come to the main training loop now, so this is the code I've added.

2376
03:23:38,060 --> 03:23:44,086
And basically every hundredth iteration, including the zeroth iteration, we put the model into evaluation mode,

2377
03:23:45,030 --> 03:23:50,000
we reset the val loader, and then no gradients involved.

2378
03:23:50,048 --> 03:23:58,020
We're going to basically accumulate the gradients over, say, 20 steps, and then average it all up and

2379
03:23:58,020 --> 03:23:59,028
print out the validation loss.

2380
03:23:59,098 --> 03:24:07,012
And so that basically is the exact same logic as the training loop, roughly, but there's no loss that

2381
03:24:07,012 --> 03:24:07,042
backward.

2382
03:24:07,057 --> 03:24:08,030
It's only inference.

2383
03:24:08,048 --> 03:24:09,048
We're just measuring the loss.

2384
03:24:09,062 --> 03:24:10,030
We're adding it up.

2385
03:24:10,057 --> 03:24:13,094
Everything else otherwise applies and is exactly as we've seen it before.

2386
03:24:14,054 --> 03:24:20,014
And so this will print the validation loss every hundredth iteration, including the very first iteration.

2387
03:24:21,056 --> 03:24:22,036
So that's nice.

2388
03:24:22,094 --> 03:24:26,032
That will tell us a little bit about how much we're overfitting.

2389
03:24:27,010 --> 03:24:32,096
That said, we have roughly infinity data, so we're mostly expecting our train and val loss to be about the same.

2390
03:24:33,068 --> 03:24:39,008
But the other reason I'm kind of interested in this is because we can take the GPT-2-124M as OpenAI released it,

2391
03:24:39,040 --> 03:24:44,030
we can initialize from it, and we can basically see what kind of loss it achieves on the validation loss as well.

2392
03:24:44,064 --> 03:24:50,072
And that gives us kind of an indication as to how much that model would generalize to 124M, but it's not an...

2393
03:24:50,072 --> 03:24:53,072
Sorry, to fine web EDU validation split.

2394
03:24:54,006 --> 03:24:58,070
That said, it's not a super fair comparison to GPT-2 because it was trained on a very different data distribution,

2395
03:24:59,002 --> 03:25:00,098
but it's still kind of like an interesting data point.

2396
03:25:01,078 --> 03:25:07,028
And in any case, you would always want to have a validation split in a training run like this so that

2397
03:25:07,028 --> 03:25:10,062
you can make sure that you are not overfitting.

2398
03:25:11,012 --> 03:25:15,046
And this is especially a concern if we were to make more epochs in our training data.

2399
03:25:16,072 --> 03:25:22,060
So for example, right now we're just doing a single epoch, but if we get to a point where we want to train on 10 epochs or something like that,

2400
03:25:22,084 --> 03:25:28,060
we would be really careful with maybe we are memorizing that data too much if we have a big enough model.

2401
03:25:29,042 --> 03:25:32,050
And our validation split would be one way to tell whether that is happening.

2402
03:25:33,004 --> 03:25:33,014
Okay.

2403
03:25:33,020 --> 03:25:39,018
And in addition to that, if you remember, at the bottom of our script, we had all of this orphaned code for sampling from way back when.

2404
03:25:39,057 --> 03:25:43,044
So I deleted that code and I moved it up to here.

2405
03:25:43,080 --> 03:25:46,016
So once in a while we sample a validation.

2406
03:25:46,062 --> 03:25:49,078
Once in a while we sample, we generate samples.

2407
03:25:50,040 --> 03:25:55,060
And then we do that only every 100 steps and we train on every single step.

2408
03:25:56,010 --> 03:25:57,044
So that's how I have a structure right now.

2409
03:25:57,057 --> 03:25:59,066
And I've been running this for 1000 iterations.

2410
03:26:00,022 --> 03:26:01,080
So here are some samples on iteration 1000.

2411
03:26:06,002 --> 03:26:08,038
Hello, I'm a language model and I'm not able to get more creative.

2412
03:26:10,018 --> 03:26:14,054
I'm a language model and languages file you're learning about here is, or is the beginning of a computer.

2413
03:26:16,016 --> 03:26:16,044
Okay.

2414
03:26:16,056 --> 03:26:25,057
So this is all like pretty, there's still a garble, but we're only at iteration 1000 and we've only just barely reached the maximum learning rate.

2415
03:26:26,006 --> 03:26:27,032
So this is still learning.

2416
03:26:27,088 --> 03:26:30,092
We're about to get some more samples coming up in 1000.

2417
03:26:31,048 --> 03:26:33,034
Okay.

2418
03:26:34,002 --> 03:26:36,062
Okay.

2419
03:26:37,000 --> 03:26:39,076
This is, you know, the model is still, still a young baby.

2420
03:26:40,074 --> 03:26:40,094
Okay.

2421
03:26:41,006 --> 03:26:47,028
So basically all of this sampling code that I've put here, everything should be familiar with to you and

2422
03:26:47,028 --> 03:26:47,094
came from before.

2423
03:26:48,026 --> 03:26:56,006
The only thing that I did is I created a generator object in PyTorch so that I have a direct control over the sampling of the random numbers.

2424
03:26:56,026 --> 03:27:00,042
We don't, because I don't want to impact the RNG state of the random number generator.

2425
03:27:00,057 --> 03:27:02,072
That is the global one used for training.

2426
03:27:02,098 --> 03:27:05,014
I want this to be completely outside of the training loop.

2427
03:27:05,046 --> 03:27:13,002
And so I'm using a special sampling RNG, and then I make sure to seed it, that every single rank has a different seed.

2428
03:27:13,012 --> 03:27:19,042
And then I pass in here where we sort of consumer random numbers in multinomial where the sampling happens.

2429
03:27:19,066 --> 03:27:21,092
I make sure to pass in the generator object there.

2430
03:27:22,052 --> 03:27:23,048
Otherwise, this is identical.

2431
03:27:24,048 --> 03:27:27,092
Now, the other thing is you'll notice that we're running a bit slower.

2432
03:27:28,036 --> 03:27:32,026
That's because I actually had to disable torch.compile to get this to sample.

2433
03:27:32,092 --> 03:27:34,082
And so we're running a bit slower.

2434
03:27:35,030 --> 03:27:37,012
So for some reason, it works with no torch compile.

2435
03:27:37,026 --> 03:27:42,052
But when I torch compile my model, I get a really scary error from PyTorch, and I have no idea how to resolve it right now.

2436
03:27:42,096 --> 03:27:46,090
So probably by the time you see this code released or something like that, maybe it's fixed.

2437
03:27:47,038 --> 03:27:53,086
But for now, I'm just going to do n false, and I'm going to bring back torch compile, and you're not going to get samples.

2438
03:27:55,034 --> 03:27:56,076
And I think I'll fix this later.

2439
03:27:57,036 --> 03:28:00,030
By the way, I will be releasing all this code.

2440
03:28:00,078 --> 03:28:04,086
And actually, I've been very careful about making git commits every time we add something.

2441
03:28:05,024 --> 03:28:11,038
And so I'm going to release the entire repo that starts completely from scratch all the way to now and

2442
03:28:11,038 --> 03:28:12,034
after this as well.

2443
03:28:12,074 --> 03:28:15,048
And so everything should be exactly documented in the git commit history.

2444
03:28:16,078 --> 03:28:18,062
And so I think that will be nice.

2445
03:28:18,096 --> 03:28:23,040
So hopefully by the time you go to GitHub, this is removed and it's working, and I will have fixed the bug.

2446
03:28:23,082 --> 03:28:25,068
Okay, so I have the optimization running here.

2447
03:28:26,050 --> 03:28:28,096
And it's stepping and we're on step 6000 or so.

2448
03:28:29,000 --> 03:28:30,052
So we're about 30% through training.

2449
03:28:31,010 --> 03:28:36,062
Now while this is training, I would like to introduce one evaluation that we're going to use to supplement the validation set.

2450
03:28:37,012 --> 03:28:39,016
And that is the Hellaswag eval.

2451
03:28:39,068 --> 03:28:43,044
So Hellaswag comes from this paper back in 2019.

2452
03:28:43,062 --> 03:28:44,084
So it's a five-year-old eval now.

2453
03:28:45,034 --> 03:28:49,076
And the way Hellaswag works is there's basically a sentence completion data set.

2454
03:28:50,016 --> 03:28:51,062
So it's a multiple choice.

2455
03:28:51,096 --> 03:28:58,026
For every one of these questions, we have basically a shared context, like a woman is outside with a bucket and

2456
03:28:58,026 --> 03:28:58,064
a dog.

2457
03:28:59,014 --> 03:29:01,026
The dog is running around trying to avoid bath.

2458
03:29:01,080 --> 03:29:06,092
She, A, rinses the bucket off with soap and blow-dried the dog's head.

2459
03:29:07,048 --> 03:29:09,086
B, uses a hose to keep it from getting soapy.

2460
03:29:10,044 --> 03:29:12,096
C, gets the dog wet and it runs away again.

2461
03:29:13,054 --> 03:29:16,002
Or D, gets into a bathtub with the dog.

2462
03:29:16,082 --> 03:29:28,044
And so basically the idea is that these multiple choice are constructed so that one of them is a natural continuation of the sentence and

2463
03:29:28,044 --> 03:29:29,032
the others are not.

2464
03:29:30,070 --> 03:29:32,074
And the others might not make sense.

2465
03:29:32,086 --> 03:29:34,076
Like, uses the hose to keep it from getting soapy.

2466
03:29:34,090 --> 03:29:35,057
That makes no sense.

2467
03:29:36,012 --> 03:29:41,040
And so what happens is that models that are not trained very well are not able to tell these apart.

2468
03:29:41,076 --> 03:29:50,062
But models that have a lot of world knowledge and can tell a lot about the world will be able to create these completions.

2469
03:29:51,000 --> 03:29:55,080
And these sentences are sourced from ActivityNet and from Wikihow.

2470
03:29:56,052 --> 03:30:05,032
And at the bottom of the paper, there's kind of like a cool chart of the kinds of domains in Wikihow.

2471
03:30:05,042 --> 03:30:09,060
So there's a lot of sentences from computers and electronics and homes and garden.

2472
03:30:10,000 --> 03:30:16,080
And it has kind of a broad coverage of the kinds of things you need to know about the world in order to find the most likely completion and

2473
03:30:18,080 --> 03:30:21,002
the identity of that completion.

2474
03:30:21,057 --> 03:30:34,000
One more thing that's kind of interesting about HellaSwag is the way it was constructed is that the incorrect options are deliberately adversarially sourced.

2475
03:30:34,038 --> 03:30:36,016
So they're not just random sentences.

2476
03:30:36,068 --> 03:30:38,076
They're actually sentences generated by language models.

2477
03:30:39,028 --> 03:30:44,057
And they're generated in such a way that language models basically find them difficult, but humans find them easy.

2478
03:30:45,026 --> 03:30:48,066
And so they mentioned that humans have a 95% accuracy on this set.

2479
03:30:48,088 --> 03:30:52,066
But at the time, the state of the art language models had only 48%.

2480
03:30:52,066 --> 03:30:54,084
And so at the time, this was a good benchmark.

2481
03:30:55,076 --> 03:30:59,028
Now, you can read the details of this paper to learn more.

2482
03:31:00,066 --> 03:31:02,076
The thing to point out, though, is that this is five years ago.

2483
03:31:03,020 --> 03:31:09,086
And since then, what happened to HellaSwag is that it's been totally just solved.

2484
03:31:10,032 --> 03:31:13,022
And so now the language models here are 96%.

2485
03:31:13,022 --> 03:31:19,006
So basically, the 4% last 4% is probably errors in the dataset, or the questions are really, really hard.

2486
03:31:19,062 --> 03:31:22,052
And so basically, this dataset is kind of crushed with respect to language models.

2487
03:31:22,088 --> 03:31:25,062
But back then, the best language model was only at about 50%.

2488
03:31:25,062 --> 03:31:28,096
But this is how far things got.

2489
03:31:29,068 --> 03:31:38,048
But still, the reason people like HellaSwag, and it's not used, by the way, in GPT-2, but in GPT-3, there is HellaSwag eval.

2490
03:31:38,088 --> 03:31:40,062
And lots of people use HellaSwag.

2491
03:31:42,036 --> 03:31:47,012
And so for GPT-3, we have results here that are cited.

2492
03:31:47,038 --> 03:31:54,042
So we know what percent accuracies GPT-3 attains at all these different model checkpoints for HellaSwag eval.

2493
03:31:54,088 --> 03:31:58,082
And the reason people like it is because HellaSwag is a smooth eval.

2494
03:31:59,026 --> 03:32:02,070
And it is an eval that offers, quote unquote, early signal.

2495
03:32:03,050 --> 03:32:09,066
So early signal means that even small language models are going to start at the random chance of 25%,

2496
03:32:09,066 --> 03:32:11,022
but they're going to slowly improve.

2497
03:32:11,028 --> 03:32:13,092
And you're going to see 25, 26, 27, et cetera.

2498
03:32:14,072 --> 03:32:19,074
And you can see slow improvement, even when the models are very small, and it's very early.

2499
03:32:20,026 --> 03:32:21,032
So it's smooth.

2500
03:32:21,080 --> 03:32:23,064
It has early signal.

2501
03:32:24,046 --> 03:32:26,094
And it's been around for a long time.

2502
03:32:27,004 --> 03:32:29,057
So that's why people kind of like this eval.

2503
03:32:30,066 --> 03:32:34,036
Now, the way that we're going to evaluate this is as follows.

2504
03:32:35,046 --> 03:32:37,090
As I mentioned, we have a shared context.

2505
03:32:38,048 --> 03:32:40,038
And this is kind of like a multiple choice task.

2506
03:32:40,078 --> 03:32:47,006
But instead of giving the model a multiple choice question and asking it for A, B, C, or D, we can't do that

2507
03:32:47,006 --> 03:32:52,016
because these models, when they are so small, as we are seeing here, the models can't actually do multiple choice.

2508
03:32:52,048 --> 03:32:57,044
They don't understand the concept of associating a label to one of the options of multiple choice.

2509
03:32:57,096 --> 03:32:58,098
They don't understand that.

2510
03:32:59,026 --> 03:33:01,026
So we have to give it to them in a native form.

2511
03:33:01,056 --> 03:33:03,092
And the native form is a token completion.

2512
03:33:04,092 --> 03:33:05,060
So here's what we do.

2513
03:33:05,070 --> 03:33:11,062
We construct a batch of four rows and T tokens, whatever that T happens to be.

2514
03:33:12,002 --> 03:33:20,018
Then the shared context, that is basically the context for the four choices, the tokens of that are shared across all of the rows.

2515
03:33:20,080 --> 03:33:21,096
And then we have the four options.

2516
03:33:22,038 --> 03:33:23,078
So we kind of like lay them out.

2517
03:33:24,024 --> 03:33:26,006
And then only one of the options is correct.

2518
03:33:26,024 --> 03:33:28,002
In this case, label three, option three.

2519
03:33:28,057 --> 03:33:31,040
And so this is the correct option.

2520
03:33:31,062 --> 03:33:33,000
And option one, two, and four are incorrect.

2521
03:33:34,010 --> 03:33:36,081
Now, these options might be of different lengths.

2522
03:33:36,081 --> 03:33:42,012
So what we do is we sort of like take the longest length, and that's the size of the batch, B by T.

2523
03:33:42,052 --> 03:33:46,057
And then some of these here are going to be padded dimensions.

2524
03:33:47,000 --> 03:33:48,036
So they're going to be unused.

2525
03:33:49,008 --> 03:33:56,012
And so we need the tokens, we need the correct label, and we need a mask that tells us which tokens are active.

2526
03:33:56,076 --> 03:33:59,098
And the mask is then zero for these padded areas.

2527
03:34:00,086 --> 03:34:03,010
So that's how we construct these batches.

2528
03:34:03,068 --> 03:34:10,074
And then in order to get the language model to predict A, B, C, or D, the way this works is basically we're just going to look at the tokens,

2529
03:34:11,004 --> 03:34:20,030
their probabilities, and we're going to pick the option that gets the lowest or the highest average probability for the token,

2530
03:34:21,024 --> 03:34:27,022
for the tokens, because that is the most likely completion according to the language model.

2531
03:34:27,090 --> 03:34:36,086
So we're just going to look at the probabilities here and average them up across the options and pick the one with the highest probability,

2532
03:34:37,024 --> 03:34:37,098
roughly speaking.

2533
03:34:39,000 --> 03:34:41,004
So this is how we're going to do HellaSwag.

2534
03:34:41,048 --> 03:34:47,022
And this is, I believe, also how GPT-3 did it.

2535
03:34:49,034 --> 03:34:51,036
This is how GPT-3 did it, as far as I know.

2536
03:34:51,072 --> 03:34:56,062
But you should note that some of the other evals where you might see HellaSwag may not do it this way.

2537
03:34:57,010 --> 03:35:03,054
They may do it in a multiple choice format where you sort of give the context a single time and then the four completions.

2538
03:35:03,096 --> 03:35:08,024
And so the model is able to see all the four options before it picks the best possible option.

2539
03:35:08,074 --> 03:35:14,030
And that's actually an easier task for a model because you get to see the other options when you're picking your choice.

2540
03:35:15,032 --> 03:35:17,088
But unfortunately, models that are our size can't do that.

2541
03:35:18,004 --> 03:35:20,078
Only models at a bigger size are able to do that.

2542
03:35:21,044 --> 03:35:26,020
And so our models are actually slightly handicapped in this way that they are not going to see the other options.

2543
03:35:26,038 --> 03:35:33,020
They're only going to see one option at a time, and they just have to assign probabilities, and the correct option has to win out in this metric.

2544
03:35:34,000 --> 03:35:38,052
All right, so let's now implement this very briefly and incorporate it into our script.

2545
03:35:39,020 --> 03:35:44,048
Okay, so what I've done here is I've introduced a new file called hella-swag.py that you can take a look into.

2546
03:35:44,098 --> 03:35:50,016
And I'm not going to step through all of it because this is not exactly like deep code.

2547
03:35:50,054 --> 03:35:52,088
Deep code, it's kind of like a little bit tedious, honestly.

2548
03:35:53,068 --> 03:35:58,020
Because what's happening is I'm downloading HellaSwag from GitHub, and I'm rendering all of its examples,

2549
03:35:58,034 --> 03:35:59,078
and there are a total of 10,000 examples.

2550
03:36:00,024 --> 03:36:02,068
I am rendering them into this format.

2551
03:36:04,026 --> 03:36:17,054
And so here at the end of this render example function, you can see that I'm returning the tokens, the tokens of this 4 by T array of tokens,

2552
03:36:17,098 --> 03:36:25,038
the mask, which tells us which parts are the options and everything else is zero, and the label that is the correct label.

2553
03:36:26,028 --> 03:36:28,098
And so that allows us to then iterate the examples and render them.

2554
03:36:29,030 --> 03:36:36,052
And I have an evaluate function here, which can load a GPT-2 from HuggingFace, and it runs the eval here.

2555
03:36:38,068 --> 03:36:45,078
And basically just calculates, just as I described, it predicts the option that has the lowest or the highest probability.

2556
03:36:46,054 --> 03:36:50,064
And the way to do that actually is we can basically evaluate the cross-entropy loss.

2557
03:36:51,042 --> 03:36:54,094
So we're basically evaluating the loss of predicting the next token in a sequence.

2558
03:36:55,036 --> 03:36:59,038
And then we're looking at the row that has the lowest average loss.

2559
03:36:59,054 --> 03:37:04,076
And that's option that we pick as the prediction.

2560
03:37:05,018 --> 03:37:07,064
And then we do some stats and prints and stuff like that.

2561
03:37:08,028 --> 03:37:09,088
So that is a way to evaluate Hellaswag.

2562
03:37:09,098 --> 03:37:16,062
Now, if you go up here, I'm showing that for GPT-2124M, if you run this script, you're going to see that

2563
03:37:16,062 --> 03:37:18,096
Hellaswag gets 29.55%.

2564
03:37:21,004 --> 03:37:22,048
So that's the performance we get here.

2565
03:37:22,072 --> 03:37:24,066
Now, remember that random chance is 25%.

2566
03:37:24,066 --> 03:37:26,060
So we haven't gone too far.

2567
03:37:27,030 --> 03:37:33,064
And GPT-2XL, which is the biggest, the GPT-2, gets all the way up to 49% roughly.

2568
03:37:34,032 --> 03:37:39,002
So these are pretty low values, considering that today's state of the art is more like 95%.

2569
03:37:39,002 --> 03:37:41,002
So these are definitely older models by now.

2570
03:37:42,000 --> 03:37:47,082
And then there's one more thing called Eleuther Harness, which is a very common piece of infrastructure for running evals for language models.

2571
03:37:48,032 --> 03:37:49,098
And they get slightly different numbers.

2572
03:37:50,042 --> 03:37:52,088
And I'm not 100% sure what the discrepancy is for these.

2573
03:37:53,062 --> 03:37:58,066
It could be that they actually do the multiple choice instead of just the completions.

2574
03:37:58,098 --> 03:38:01,088
And then that could be the discrepancy.

2575
03:38:02,018 --> 03:38:03,080
But I'm not 100% sure about that.

2576
03:38:03,080 --> 03:38:04,068
I'd have to take a look.

2577
03:38:05,006 --> 03:38:07,068
But for now, our script reports 29.55.

2578
03:38:08,008 --> 03:38:13,060
And so that is the number that we'd like to beat if we were training GPT-2 124M from scratch in ourselves.

2579
03:38:14,054 --> 03:38:23,070
So now I'm going to go into actually incorporating this eval into our main training script.

2580
03:38:24,060 --> 03:38:31,060
And basically, because we want to evaluate it in a periodic manner, so that we can track how it evolves over time,

2581
03:38:32,012 --> 03:38:39,080
and see when and if we cross this 29.55 region.

2582
03:38:40,016 --> 03:38:43,034
So let's now walk through some of the changes to train GPT-2.py.

2583
03:38:43,082 --> 03:38:49,096
The first thing I did here is I actually made use compile optional, kind of, and I disabled it by default.

2584
03:38:51,000 --> 03:38:56,028
And the problem with that is, the problem with compile is that unfortunately, it does make our code faster,

2585
03:38:56,057 --> 03:38:59,026
but it actually breaks the evaluation code and the sampling code.

2586
03:38:59,046 --> 03:39:01,056
It gives me a very gnarly message, and I don't know why.

2587
03:39:02,004 --> 03:39:07,057
So hopefully, by the time you get to the code base, when I put it up on GitHub, we're going to fix that by then.

2588
03:39:07,057 --> 03:39:11,030
But for now, I'm running without Torch Compile, which is why you see this be a bit slower.

2589
03:39:11,076 --> 03:39:13,082
So we're running without Torch Compile.

2590
03:39:14,076 --> 03:39:21,068
I also created a log directory, log, where we can place our log.txt, which will record the train loss,

2591
03:39:22,006 --> 03:39:24,002
validation loss, and the hellaswag accuracies.

2592
03:39:24,064 --> 03:39:30,060
So a very simple text file, and we're going to open for writing, so that it sort of starts empty, and

2593
03:39:30,060 --> 03:39:31,068
then we're going to append to it.

2594
03:39:33,012 --> 03:39:38,002
I created a simple variable that helps tell us when we have a last step.

2595
03:39:39,008 --> 03:39:47,012
And then basically, periodically, inside this loop, every 250th iteration, or at the last step, we're going to evaluate the validation loss.

2596
03:39:48,028 --> 03:39:57,020
And then every 250th iteration, we are going to evaluate hellaswag, but only if we are not using compile,

2597
03:39:57,048 --> 03:39:58,068
because compile breaks it.

2598
03:39:59,057 --> 03:40:02,068
So I'm going to come back to this code for evaluating hellaswag in a second.

2599
03:40:03,080 --> 03:40:07,074
And then every 250th iteration as well, we're also going to sample from the model.

2600
03:40:08,008 --> 03:40:14,020
And so you should recognize this as our ancient code from way back when we started the video, and we're just sampling from the model.

2601
03:40:15,050 --> 03:40:24,060
And then finally here, these are, if we're not, after we validate, sample, and evaluate hellaswag, we actually do a training step here.

2602
03:40:25,002 --> 03:40:30,048
And so this is one step of training, and you should be pretty familiar with all of what this does.

2603
03:40:31,026 --> 03:40:34,056
And at the end here, once we get our training loss, we write it to the file.

2604
03:40:35,006 --> 03:40:39,040
So the only thing that changed that I really added is this entire section for hellaswag eval.

2605
03:40:39,088 --> 03:40:44,048
And the way this works is I'm trying to get all the GPUs to collaborate on the hellaswag.

2606
03:40:44,090 --> 03:40:46,068
And so we're iterating all the examples.

2607
03:40:47,032 --> 03:40:52,096
And then each process only picks the examples that assigned to it.

2608
03:40:53,030 --> 03:40:57,050
So we sort of take i and mod it by the world size, and we have to make it equal to rank.

2609
03:40:57,070 --> 03:40:58,054
Otherwise, we continue.

2610
03:40:59,054 --> 03:41:03,062
And then we render an example, put it on a GPU, we get the logits.

2611
03:41:04,004 --> 03:41:08,096
Then I create a helper function that helps us basically predict the option with the lowest loss.

2612
03:41:09,066 --> 03:41:11,018
So this comes here, the prediction.

2613
03:41:11,072 --> 03:41:13,096
And then if it's correct, we sort of keep count.

2614
03:41:14,048 --> 03:41:19,050
And then if multiple processes were collaborating on all of this, then we need to synchronize their stats.

2615
03:41:20,014 --> 03:41:29,008
And so the one way to do that is to package up our statistics here into tensors, which we can then call this.all_reduce and sum.

2616
03:41:29,048 --> 03:41:35,022
And then here we sort of unwrap them from tensors so that we just have ints.

2617
03:41:35,086 --> 03:41:39,040
And then here, the master process will print and log the hellaswag accuracy.

2618
03:41:40,088 --> 03:41:44,092
So that's kind of it.

2619
03:41:45,012 --> 03:41:46,096
And that's what I'm running right here.

2620
03:41:47,020 --> 03:41:48,076
So you see this optimization here.

2621
03:41:49,092 --> 03:41:51,084
And we just had a generation.

2622
03:41:52,024 --> 03:41:55,028
And this is step 10,000 out of about 20,000, right?

2623
03:41:55,038 --> 03:41:56,096
So we are halfway done.

2624
03:41:57,060 --> 03:42:00,038
And these are the kinds of samples that we are getting at this stage.

2625
03:42:00,056 --> 03:42:01,014
So let's take a look.

2626
03:42:02,016 --> 03:42:03,036
Hello, I'm a language model.

2627
03:42:03,080 --> 03:42:06,022
So I'd like to use it to generate some kinds of output.

2628
03:42:06,080 --> 03:42:09,096
Hello, I'm a language model, and I'm a developer for a lot of companies.

2629
03:42:11,028 --> 03:42:12,022
Hello, I'm a language model.

2630
03:42:13,062 --> 03:42:15,068
Let's see if I can find any fun one.

2631
03:42:29,014 --> 03:42:29,062
I don't know.

2632
03:42:29,072 --> 03:42:30,074
You can go through this yourself.

2633
03:42:31,002 --> 03:42:33,057
But certainly, the predictions are getting less and less random.

2634
03:42:34,040 --> 03:42:42,090
It seems like the model is a little bit more self-aware and using language that is a bit more specific to it being a language model.

2635
03:42:43,040 --> 03:42:48,088
Hello, I'm a language model, and like how the language is used to communicate, I'm a language model and

2636
03:42:48,088 --> 03:42:51,056
I'm going to be speaking English and German.

2637
03:42:52,018 --> 03:42:52,076
Okay, I don't know.

2638
03:42:53,012 --> 03:42:57,054
So let's just wait until this optimization finishes, and we'll see what kind of samples we get.

2639
03:42:57,096 --> 03:43:05,004
And we're also going to look at the train, val, and the hellaswag accuracy and see how we're doing with respect to GPT-2.

2640
03:43:07,038 --> 03:43:07,088
Okay, good morning.

2641
03:43:08,070 --> 03:43:15,054
So focusing for a moment on the Jupyter Notebook here on the right, I created a new cell that basically allows us to visualize the train,

2642
03:43:15,064 --> 03:43:16,042
val, and the hellaswags score.

2643
03:43:19,042 --> 03:43:20,096
And you can step through this.

2644
03:43:21,018 --> 03:43:24,032
It basically like parses the log file that we are writing.

2645
03:43:24,032 --> 03:43:27,054
And a lot of this is just like boring matplotlib code.

2646
03:43:27,068 --> 03:43:30,054
But basically, this is what our optimization looks like.

2647
03:43:31,018 --> 03:43:45,020
So we ran for 19,073 steps, which is roughly 10 billion tokens, which is, whoops, oh my gosh, which is one epoch of the sample 10B of FindWebBTU.

2648
03:43:46,010 --> 03:43:48,030
On the left, we have the loss.

2649
03:43:48,030 --> 03:43:53,033
And in blue, we have the training loss. In orange, we have the validation loss.

2650
03:43:53,033 --> 03:44:04,070
And in red, as a horizontal line, we have the opening our GPT-2 124M model checkpoint, when it's just evaluated on the validation set of this FindWebBTU.

2651
03:44:05,020 --> 03:44:09,030
So you can see that we are surpassing this orange is below the red.

2652
03:44:09,046 --> 03:44:12,020
So we're surpassing the validation set of this data set.

2653
03:44:12,070 --> 03:44:16,060
And like I mentioned, the data set distribution is very different from what GPT-2 trained on.

2654
03:44:16,072 --> 03:44:22,084
So this is not an exactly fair comparison, but it's a good cross check to look at.

2655
03:44:23,016 --> 03:44:29,002
Now, we would ideally like something that is withheld and comparable and somewhat standard.

2656
03:44:30,006 --> 03:44:31,096
And so for us, that is Hellaswag.

2657
03:44:32,038 --> 03:44:37,042
And so on here, we see the Hellaswag progress we made from 25% all the way here.

2658
03:44:38,000 --> 03:44:43,028
In red, we see the opening our GPT-2 124M model in red.

2659
03:44:43,044 --> 03:44:45,040
So it achieves this Hellaswag here.

2660
03:44:46,028 --> 03:44:53,004
And the GPT-3 model 124M, which was trained on 300 billion tokens, achieves green.

2661
03:44:53,088 --> 03:44:55,012
So that's over here.

2662
03:44:55,072 --> 03:45:03,082
So you see that we basically surpassed the GPT-2 124M model right here, which is really nice.

2663
03:45:04,064 --> 03:45:12,026
Now, interestingly, we were able to do so with only training on 10 billion tokens, while GPT-2 was trained on 100 billion tokens.

2664
03:45:13,028 --> 03:45:17,042
So for some reason, we were able to get away with significantly fewer tokens for training.

2665
03:45:17,090 --> 03:45:25,050
There are many possibilities as to why we could match or surpass this accuracy with only 10 billion training.

2666
03:45:25,096 --> 03:45:32,068
So number one, it could be that OpenAI GPT-2 was trained on a much wider data distribution.

2667
03:45:33,006 --> 03:45:36,018
So in particular, FindWebEDU is all English.

2668
03:45:36,050 --> 03:45:37,056
It's not multilingual.

2669
03:45:38,008 --> 03:45:40,006
And there's not that much math and code.

2670
03:45:41,046 --> 03:45:47,072
And so math and code and multilingual could have been stealing capacity from the original GPT-2 model.

2671
03:45:48,046 --> 03:45:53,080
And basically, that could be partially the reason why this is not working out.

2672
03:45:54,014 --> 03:45:55,030
There's many other reasons.

2673
03:45:56,016 --> 03:45:59,072
So for example, the Hellaswag eval is fairly old, maybe five years or so.

2674
03:46:00,002 --> 03:46:07,088
It is possible that aspects of Hellaswag in some way or even identically have made it into the training set of FindWeb.

2675
03:46:08,014 --> 03:46:09,044
We don't know for sure.

2676
03:46:09,082 --> 03:46:13,052
But if that was the case, then we are basically looking at the training curve instead of the validation curve.

2677
03:46:14,030 --> 03:46:16,036
So long story short, this is not a perfect eval.

2678
03:46:16,042 --> 03:46:17,048
And there's some caveats here.

2679
03:46:18,008 --> 03:46:21,098
But at least we have some confidence that we're not doing something completely wrong.

2680
03:46:22,078 --> 03:46:29,000
And it's probably the case that when people try to create these data sets, they try to make sure that

2681
03:46:29,000 --> 03:46:32,048
test sets that are very common are not part of the training set.

2682
03:46:32,082 --> 03:46:37,010
For example, when HuggingFace created the FindWebEDU, they use Hellaswag as an eval.

2683
03:46:37,032 --> 03:46:42,034
So I would hope that they make sure that they deduplicate and that there's no Hellaswag in the training set.

2684
03:46:42,074 --> 03:46:43,078
But we can't be sure.

2685
03:46:45,004 --> 03:46:48,012
The other thing I wanted to address briefly is, look at this loss curve.

2686
03:46:48,012 --> 03:46:50,057
This looks really wrong here.

2687
03:46:51,002 --> 03:46:52,084
I don't actually know 100% what this is.

2688
03:46:53,012 --> 03:46:59,082
And I suspect it's because the 10 billion sample of FindWebEDU was not properly shuffled.

2689
03:47:00,090 --> 03:47:05,070
And there's some issue here with the data that I don't fully understand yet.

2690
03:47:05,092 --> 03:47:07,062
And there's some weird periodicity to it.

2691
03:47:08,050 --> 03:47:17,010
And because we are in a very lazy way, sort of serializing all the tokens and just iterating on them from scratch without doing any permutations or any random sampling ourselves,

2692
03:47:17,074 --> 03:47:22,004
I think we're inheriting some of the ordering that they have in the data set.

2693
03:47:22,042 --> 03:47:24,084
So this is not ideal.

2694
03:47:25,050 --> 03:47:30,048
But hopefully by the time you get to this repo, some of these things, by the way, will hopefully be fixed.

2695
03:47:31,036 --> 03:47:34,082
And I will release this build.nanoGPT repo.

2696
03:47:35,062 --> 03:47:37,080
And right now it looks a little ugly and preliminary.

2697
03:47:38,060 --> 03:47:40,026
So hopefully by the time you get here, it's nicer.

2698
03:47:40,072 --> 03:47:42,098
But down here, I'm going to show Errata.

2699
03:47:43,048 --> 03:47:46,084
And I'm going to talk about some of the things that happened after the video.

2700
03:47:47,034 --> 03:47:50,020
And I expect that we will have fixed the small issue.

2701
03:47:51,038 --> 03:47:55,092
But for now, basically, this shows that our training is not completely wrong.

2702
03:47:56,020 --> 03:48:01,048
And it shows that we're able to surpass the accuracy with only 10x the token budget.

2703
03:48:03,006 --> 03:48:06,086
And possibly it could be also that the data set may have improved.

2704
03:48:07,024 --> 03:48:10,060
So the original GPT-2 data set was WebText.

2705
03:48:11,002 --> 03:48:14,022
It's possible that not a lot of care and attention went into the data set.

2706
03:48:14,054 --> 03:48:16,014
This was very early in LLMs.

2707
03:48:16,057 --> 03:48:22,078
Whereas now there's a lot more scrutiny on good practices around deduplication, filtering, quality filtering,

2708
03:48:22,090 --> 03:48:23,030
and so on.

2709
03:48:23,057 --> 03:48:26,086
And it's possible that the data set we're training on is just of higher quality per token.

2710
03:48:27,032 --> 03:48:28,096
And that could be giving us a boost as well.

2711
03:48:29,076 --> 03:48:31,026
So a number of caveats to think about.

2712
03:48:31,040 --> 03:48:33,040
But for now, we're pretty happy with this.

2713
03:48:34,066 --> 03:48:35,042
And yeah.

2714
03:48:36,008 --> 03:48:39,004
Now, the next thing I was interested in is, as you see, it's a morning now.

2715
03:48:39,038 --> 03:48:40,038
So there was an overnight.

2716
03:48:40,082 --> 03:48:43,054
And I wanted to basically see how far I could push the result.

2717
03:48:43,086 --> 03:48:51,020
So to do an overnight run, I basically did instead of one epoch, which took roughly two hours, I just did it times four.

2718
03:48:51,044 --> 03:48:53,032
So that that would take eight hours while I was sleeping.

2719
03:48:53,088 --> 03:48:57,070
And so we did four epochs, or roughly 40 billion tokens of training.

2720
03:48:58,040 --> 03:49:00,036
And I was trying to see how far we could get.

2721
03:49:01,034 --> 03:49:02,070
And so this was the only change.

2722
03:49:02,080 --> 03:49:03,062
And I re-ran the script.

2723
03:49:04,014 --> 03:49:09,062
And when I point and read the log file at the 40B, this is what the curve looked like.

2724
03:49:10,046 --> 03:49:11,057
Okay.

2725
03:49:12,014 --> 03:49:17,098
So to narrate this, number one, we are seeing this issue here with the periodicity through the different epochs and

2726
03:49:17,098 --> 03:49:20,098
something really weird with the FindWebEDU data set.

2727
03:49:21,028 --> 03:49:22,074
And that is to be determined.

2728
03:49:23,088 --> 03:49:28,046
But otherwise, we are seeing that the HellaSwag actually went up by a lot.

2729
03:49:28,090 --> 03:49:36,028
And we almost made it to the GPT-3 124M accuracy up here, but not quite.

2730
03:49:36,050 --> 03:49:39,006
So it's too bad that I didn't sleep slightly longer.

2731
03:49:40,054 --> 03:49:45,066
And I think if this was a five epoch run, we may have gotten here.

2732
03:49:46,020 --> 03:49:52,077
Now, one thing to point out is that if you're doing multi epoch runs, we're not actually being very careful in our data loader.

2733
03:49:52,077 --> 03:49:53,056
And we're not.

2734
03:49:55,042 --> 03:50:01,052
This data loader goes through the data in exactly the same format, and exactly the same order.

2735
03:50:01,084 --> 03:50:02,090
And this is kind of suboptimal.

2736
03:50:03,022 --> 03:50:07,054
And you would want to look into extensions where you actually permute the data randomly.

2737
03:50:07,092 --> 03:50:15,090
You permute the documents around in every single shard on every single new epoch, and potentially even permute the shards.

2738
03:50:17,014 --> 03:50:19,046
And that would go a long way into decreasing the periodicity.

2739
03:50:19,060 --> 03:50:24,066
And it's also better for the optimization, so that you're not seeing things in the identical format.

2740
03:50:25,002 --> 03:50:29,022
And you're introducing some of the randomness in how the documents follow each other.

2741
03:50:29,070 --> 03:50:33,048
Because you have to remember that in every single row, these documents follow each other.

2742
03:50:33,064 --> 03:50:35,090
And then there's the end of text token, and then the next document.

2743
03:50:36,036 --> 03:50:40,096
So the documents are currently glued together in the exact same identical manner.

2744
03:50:41,050 --> 03:50:47,004
But we actually want to break up the documents and shuffle them around, because the order of the documents shouldn't matter.

2745
03:50:47,054 --> 03:50:48,062
And they shouldn't...

2746
03:50:48,062 --> 03:50:52,060
Basically, we want to break up that dependence, because it's kind of a spurious correlation.

2747
03:50:53,048 --> 03:50:55,054
And so our data letter is not currently doing that.

2748
03:50:55,060 --> 03:50:57,096
And that's one improvement you could think of making.

2749
03:50:59,028 --> 03:51:04,052
The other thing to point out is we're almost matching GPT-3 accuracy with only 40 billion tokens.

2750
03:51:05,004 --> 03:51:07,060
GPT-3 trained on 300 billion tokens.

2751
03:51:08,000 --> 03:51:13,040
So again, we're seeing about a 10x improvement here with respect to learning efficiency.

2752
03:51:13,096 --> 03:51:15,090
The other thing I wanted to...

2753
03:51:16,008 --> 03:51:22,004
And I don't actually know exactly what to attribute this to, other than some of the things that I already mentioned previously for the previous run.

2754
03:51:22,054 --> 03:51:27,038
The other thing I wanted to briefly mention is the max LR here.

2755
03:51:27,070 --> 03:51:31,094
I saw some people already play with this a little bit in a previous related repository.

2756
03:51:33,002 --> 03:51:35,066
And it turns out that you can actually almost 3x this.

2757
03:51:36,006 --> 03:51:38,054
So it's possible that the maximum learning rate can be a lot higher.

2758
03:51:38,080 --> 03:51:43,057
And for some reason, the GPT-3 hyperparameters that we are inheriting are actually extremely conservative.

2759
03:51:44,000 --> 03:51:46,086
And you can actually get away with a higher learning rate and it would train faster.

2760
03:51:47,052 --> 03:51:51,026
So a lot of these hyperparameters are quite tunable.

2761
03:51:51,064 --> 03:51:52,082
And feel free to play with them.

2762
03:51:53,004 --> 03:51:55,086
And they're probably not set precisely correctly.

2763
03:51:56,068 --> 03:52:00,082
And it's possible that you can get away with doing this, basically.

2764
03:52:01,030 --> 03:52:09,046
And if you wanted to exactly be faithful to GPT-3, you would also want to make the following difference.

2765
03:52:09,096 --> 03:52:10,096
You'd want to come here.

2766
03:52:11,032 --> 03:52:13,068
And the sequence length of GPT-3 is 2x.

2767
03:52:13,088 --> 03:52:15,088
It's 2048 instead of 1024.

2768
03:52:16,034 --> 03:52:19,026
So you would come here, change this to 2048 for T.

2769
03:52:19,070 --> 03:52:26,018
And then if you want the exact same number of tokens, half a million per iteration or per step, you want to then

2770
03:52:26,018 --> 03:52:27,022
decrease this to 32.

2771
03:52:27,088 --> 03:52:29,046
So they still multiply to half a mil.

2772
03:52:30,034 --> 03:52:35,004
So that would give your model sequence length equal to that of GPT-3.

2773
03:52:35,036 --> 03:52:42,042
And in that case, the models would be roughly identical as far as I'm aware.

2774
03:52:42,080 --> 03:52:45,036
Because again, GPT-2 and GPT-3 are very, very similar models.

2775
03:52:46,018 --> 03:52:50,028
Now we can also look at some of the samples here from the model that was trained overnight.

2776
03:52:50,086 --> 03:52:54,024
So this is the optimization.

2777
03:52:54,084 --> 03:52:59,050
And you see that here we stepped all the way to 76,290 or so.

2778
03:53:00,032 --> 03:53:04,042
And the hellaswags was 33.24.

2779
03:53:04,090 --> 03:53:07,076
And these are some of the samples from the model.

2780
03:53:08,062 --> 03:53:13,052
And you can see that if you read through this and pause the video briefly, you can see that they are a lot more coherent.

2781
03:53:14,056 --> 03:53:19,042
And they're actually addressing the fact that it's a language model, almost.

2782
03:53:20,016 --> 03:53:25,092
So hello, I'm a language model, and I try to be as accurate as possible.

2783
03:53:27,084 --> 03:53:30,052
I'm a language model, not a programming language.

2784
03:53:32,032 --> 03:53:33,074
I know how to communicate.

2785
03:53:34,042 --> 03:53:35,026
I use Python.

2786
03:53:38,096 --> 03:53:39,028
I don't know.

2787
03:53:39,038 --> 03:53:44,016
If you pause this and look at it and then compare it to the one, to the model that was only trained for 10 billion,

2788
03:53:44,070 --> 03:53:46,052
you will see that these are a lot more coherent.

2789
03:53:46,092 --> 03:53:48,022
And you can play with this yourself.

2790
03:53:49,018 --> 03:53:52,074
One more thing I added to the code, by the way, is this chunk of code here.

2791
03:53:53,038 --> 03:54:00,022
So basically, right after we evaluate the validation loss, if we are the master process, in addition to logging the validation loss,

2792
03:54:00,070 --> 03:54:06,016
every 5,000 steps, we're also going to save the checkpoint, which is really just the state dictionary of the model.

2793
03:54:06,090 --> 03:54:12,042
And so checkpointing is nice just because you can save the model, and later you can use it in some way.

2794
03:54:12,078 --> 03:54:19,098
If you wanted to resume the optimization, then in addition to saving the model, we have to also save the optimizer state dict.

2795
03:54:20,042 --> 03:54:23,094
Because remember that the optimizer has a few additional buffers because of Adam.

2796
03:54:24,057 --> 03:54:26,056
So it's got the M and V.

2797
03:54:27,036 --> 03:54:30,014
And you need to also resume the optimizer properly.

2798
03:54:30,044 --> 03:54:33,088
You have to be careful with the RNG seeds, random number generators, and so on.

2799
03:54:34,028 --> 03:54:40,052
So if you wanted to exactly be able to resume optimization, you have to think through the state of the training process.

2800
03:54:40,094 --> 03:54:43,008
But if you just want to save the model, this is how you would do it.

2801
03:54:43,056 --> 03:54:49,038
And one nice reason why you might want to do this is because you may want to evaluate the model a lot more carefully.

2802
03:54:50,012 --> 03:54:57,084
So here we are only kind of like winging the LSY eval, but you may want to use something nicer, like for example,

2803
03:54:57,092 --> 03:55:01,030
the Luther evaluation hardness.

2804
03:55:04,076 --> 03:55:09,004
So this is a way to also evaluate language models.

2805
03:55:10,024 --> 03:55:20,092
So it's possible that you may want to use basically different infrastructure to more thoroughly evaluate the models on different evaluations and

2806
03:55:20,092 --> 03:55:29,004
compare it to the OpenAI GPT-2 model on many other tasks, like for example, that involve math, code, or different languages, and so on.

2807
03:55:29,004 --> 03:55:30,072
So this is a nice functionality to have as well.

2808
03:55:32,026 --> 03:55:38,092
And then the other thing I wanted to mention is that everything we've built here, this is only the pre-training step.

2809
03:55:39,066 --> 03:55:44,048
So the GPT here is a, it dreams documents, it just predicts the next token.

2810
03:55:44,084 --> 03:55:47,012
You can't talk to it like you can talk to chat GPT.

2811
03:55:48,026 --> 03:55:53,008
If you wanted to talk to the model, we have to fine tune it into the chat format.

2812
03:55:53,057 --> 03:55:55,014
And that's not actually like that complicated.

2813
03:55:55,036 --> 03:56:01,070
If you're looking at supervised fine tuning or SFT, really what that means is we're just swapping out a dataset into a dataset that

2814
03:56:01,070 --> 03:56:07,022
is a lot more conversational, and there's a user assistant, user assistant kind of structure, and we just fine tune on it.

2815
03:56:07,042 --> 03:56:12,060
And then we basically fill in the user tokens and we sample the assistant tokens.

2816
03:56:12,094 --> 03:56:17,034
It's not a lot more deeper than that, but basically we swap out the dataset and continue training.

2817
03:56:18,052 --> 03:56:20,078
But for now, we're going to stop it at pre-training.

2818
03:56:21,030 --> 03:56:27,060
One more thing that I wanted to briefly show you is that, of course, what we've built up today was building towards NanoGPT,

2819
03:56:27,074 --> 03:56:29,020
which is this repository from earlier.

2820
03:56:30,020 --> 03:56:35,044
But also there's actually another NanoGPT implementation, and it's hiding in a more recent project that

2821
03:56:35,044 --> 03:56:37,040
I've been working on called LLM.C.

2822
03:56:38,048 --> 03:56:47,057
And LLM.C is a pure C CUDA implementation of GPT-2 or GPT-3 training, and it just directly uses CUDA and

2823
03:56:47,057 --> 03:56:49,004
is written as C CUDA.

2824
03:56:49,064 --> 03:56:54,040
Now, the NanoGPT here acts as reference code in PyTorch to the C implementation.

2825
03:56:54,082 --> 03:56:59,028
So we're trying to exactly match up the two, but we're hoping that the C CUDA is faster.

2826
03:56:59,066 --> 03:57:04,038
And of course, currently that seems to be the case, because it is a direct optimized implementation.

2827
03:57:05,022 --> 03:57:09,024
So train GPT-2.py in LLM.C is basically the NanoGPT.

2828
03:57:09,076 --> 03:57:17,066
And when you scroll through this file, you'll find a lot of things that very much look like things that

2829
03:57:17,066 --> 03:57:19,040
we've built up in this lecture.

2830
03:57:19,078 --> 03:57:25,036
And then when you look at train GPT-2.cu, this is the C CUDA implementation.

2831
03:57:26,018 --> 03:57:32,076
So there's a lot of MPI, NICL, GPU, CUDA, C, C++, and you have to be familiar with that.

2832
03:57:33,046 --> 03:57:41,040
But when this is built up, we can actually run the two side by side, and they're going to produce the exact same results,

2833
03:57:41,040 --> 03:57:43,084
but LLM.C actually runs faster.

2834
03:57:43,098 --> 03:57:44,064
So let's see that.

2835
03:57:45,014 --> 03:57:49,054
So on the left, I have PyTorch, NanoGPT looking thing.

2836
03:57:49,072 --> 03:57:51,048
On the right, I have the LLM.C call.

2837
03:57:51,084 --> 03:57:53,070
And here I'm going to launch the two.

2838
03:57:55,026 --> 03:57:56,094
Both of these are going to be running on a single GPU.

2839
03:57:57,026 --> 03:58:02,086
And here I'm putting the LLM.C on GPU1, and this one will grab GPU0 by default.

2840
03:58:03,078 --> 03:58:12,024
And then we can see here that LLM.C compiled, and then allocate space, and it's stepping.

2841
03:58:13,024 --> 03:58:24,082
So basically, meanwhile, PyTorch is still compiling, because Torch compile is a bit slower here than the LLM.C NVCC C CUDA compile.

2842
03:58:25,024 --> 03:58:29,092
And so this program has already started running, and we're still waiting here for Torch compile.

2843
03:58:30,042 --> 03:58:34,062
Now, of course, this is a very specific implementation to GPT-2 and 3.

2844
03:58:35,014 --> 03:58:38,080
PyTorch is a very general neural network framework, so they're not exactly comparable.

2845
03:58:39,014 --> 03:58:43,040
But if you're only interested in training GPT-2 and 3, LLM.C is very fast.

2846
03:58:43,098 --> 03:58:45,034
It takes less space.

2847
03:58:45,066 --> 03:58:49,042
It's faster to start, and it's faster per step.

2848
03:58:50,094 --> 03:58:52,062
And so PyTorch started stepping here.

2849
03:58:53,040 --> 03:59:00,052
And as you can see, we're running at about 223,000 tokens per second here, and about 185,000 tokens per second here.

2850
03:59:00,096 --> 03:59:04,002
So quite a bit slower.

2851
03:59:04,048 --> 03:59:09,088
But I don't have full confidence that I exactly squeezed out all the juice from the PyTorch implementation.

2852
03:59:10,054 --> 03:59:16,034
But the important thing here is notice that if I align up the steps, you will see that the losses and

2853
03:59:16,034 --> 03:59:19,018
norms that are printed between these two are identical.

2854
03:59:20,026 --> 03:59:25,060
So on the left, we have the PyTorch, and on the right, this C CUDA implementation, and they're the same,

2855
03:59:25,092 --> 03:59:27,062
except this one runs faster.

2856
03:59:28,040 --> 03:59:29,056
So that's kind of...

2857
03:59:29,056 --> 03:59:35,018
I wanted to show you also briefly LLM.C, and this is a parallel implementation, and it's also something that

2858
03:59:35,018 --> 03:59:39,012
you may want to play with or look at, and it's kind of interesting.

2859
03:59:39,068 --> 03:59:44,080
Okay, so at this point, I should probably start wrapping up the video, because I think it's getting way longer than anticipated.

2860
03:59:45,036 --> 03:59:48,042
But we did cover a lot of ground, and we built everything from scratch.

2861
03:59:48,096 --> 03:59:54,060
So as a brief summary, we were looking at the GPT-2 and GPT-3 papers.

2862
03:59:55,054 --> 04:00:00,042
We were looking at how you set up these training runs, and all the considerations involved.

2863
04:00:00,074 --> 04:00:07,020
We wrote everything from scratch, and then we saw that over the duration of either a two-hour training run or an overnight run,

2864
04:00:07,046 --> 04:00:13,080
we can actually match the 124 million parameter checkpoints of GPT-2 and GPT-3 to a very large extent.

2865
04:00:14,028 --> 04:00:20,042
In principle, the code that we wrote would be able to train even bigger models if you have the patience or the computing resources,

2866
04:00:21,008 --> 04:00:24,060
and so you could potentially think about training some of the bigger checkpoints as well.

2867
04:00:25,064 --> 04:00:28,018
There are a few remaining issues to address.

2868
04:00:28,064 --> 04:00:33,046
What's happening with the loss here, which I suspect has to do with the fine web EDU data sampling.

2869
04:00:34,014 --> 04:00:35,098
Why can't we turn on Torch Compile?

2870
04:00:36,036 --> 04:00:38,082
It currently breaks generation and Hellaswag.

2871
04:00:39,008 --> 04:00:39,088
What's up with that?

2872
04:00:40,038 --> 04:00:44,056
In the data loader, we should probably be permuting our data when we reach EPUG boundaries.

2873
04:00:44,096 --> 04:00:51,098
There's a few more issues like that, and I expect to be documenting some of those over time in the build-nano-gpt repository here,

2874
04:00:52,062 --> 04:00:55,012
which I'm going to be releasing with this video.

2875
04:00:56,002 --> 04:01:02,002
If you have any questions or would like to talk about anything that we covered, please go to the discussions tab so

2876
04:01:02,002 --> 04:01:08,078
we can talk here, or please go to issues or pull requests, depending on what you'd like to contribute,

2877
04:01:09,036 --> 04:01:15,054
or also have a look at the Zero2Hero Discord, and I'm going to be hanging out here on nano-gpt.

2878
04:01:18,012 --> 04:01:25,018
Otherwise, for now, I'm pretty happy about where we got, and I hope you enjoyed the video, and I will see you later.

